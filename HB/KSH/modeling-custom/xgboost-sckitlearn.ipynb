{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db09392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# XGBoost CPU íŒŒì´í”„ë¼ì¸ (pandas + sklearn)\n",
    "# =============================================================================\n",
    "# ìš©ë„: 50pct ì´ìƒ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë˜ëŠ” GPU ë¯¸ì‚¬ìš© í™˜ê²½\n",
    "# ì…€ êµ¬ë¶„: #%% ì£¼ì„ìœ¼ë¡œ ë…¸íŠ¸ë¶ ì…€ ë¶„ë¦¬\n",
    "# =============================================================================\n",
    "#\n",
    "# âš ï¸ TODO: ê³¼ì í•© í•´ê²° (Train ROC-AUC 0.89 vs Valid 0.73)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# [1] CFG.XGB_PARAMSì— ì •ê·œí™” ê°•í™” (Ridge/Lasso)\n",
    "#     CFG.XGB_PARAMS['reg_lambda'] = 5.0    # Ridge (L2), ê¸°ë³¸=1 â†’ 5~10 ì‹œë„\n",
    "#     CFG.XGB_PARAMS['reg_alpha'] = 1.0     # Lasso (L1), ê¸°ë³¸=0 â†’ 0.5~2 ì‹œë„\n",
    "#\n",
    "# [2] CFG.XGB_PARAMSì— scale_pos_weight ì¶”ê°€ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •)\n",
    "#     # ë‹¤ìš´ìƒ˜í”Œë§ í›„ 1:1ì´ë©´ ë¶ˆí•„ìš”í•˜ì§€ë§Œ, ì›ë³¸ ë¹„ìœ¨ ë°˜ì˜ ì‹œ:\n",
    "#     CFG.XGB_PARAMS['scale_pos_weight'] = len(train_df[train_df['clicked']==0]) / len(train_df[train_df['clicked']==1])\n",
    "#\n",
    "# [3] CFG.XGB_PARAMS íŠ¸ë¦¬ ë³µì¡ë„ ì¤„ì´ê¸°\n",
    "#     CFG.XGB_PARAMS['max_depth'] = 6       # í˜„ì¬ 8 â†’ 5~6ìœ¼ë¡œ ì¤„ì´ê¸°\n",
    "#     CFG.XGB_PARAMS['min_child_weight'] = 5 # ê¸°ë³¸=1 â†’ 5~10 (leaf ìµœì†Œ ìƒ˜í”Œ)\n",
    "#     CFG.XGB_PARAMS['gamma'] = 0.3         # ê¸°ë³¸=0 â†’ 0.1~0.5 (split ìµœì†Œ gain)\n",
    "#\n",
    "# [4] í”¼ì²˜ ìˆ˜ ì¤„ì´ê¸° (í˜„ì¬ 1437ê°œ â†’ OHE ë•Œë¬¸ì— í¼)\n",
    "#     # interaction í”¼ì²˜ ì œê±° í…ŒìŠ¤íŠ¸:\n",
    "#     # add_interaction_features() í˜¸ì¶œ ì£¼ì„ ì²˜ë¦¬ í›„ ì¬ì‹¤í–‰\n",
    "#     # ë˜ëŠ” groupby í”¼ì²˜ë§Œìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11646855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# # ì €ì¥ëœ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "# with open('XGB_CPU_20240216_0105_config.json', 'r', encoding='utf-8') as f:\n",
    "#     config = json.load(f)\n",
    "# print(config['learning_rate'])  # 0.08 ì¶œë ¥\n",
    "# print(config['max_depth'])      # 6 ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f48df7",
   "metadata": {},
   "source": [
    "[PART 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76aa5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import gc\n",
    "import datetime\n",
    "from itertools import combinations\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    log_loss,\n",
    "    accuracy_score\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# seq ì„ë² ë”© ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append('/home/konkukstat/python3/HB/KSH')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e677e",
   "metadata": {},
   "source": [
    "[PART 2] ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f46eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = '/home/konkukstat/python3/workspace/data'\n",
    "    TRAIN_FILE = 'train.parquet'   # ë³€ê²½ ê°€ëŠ¥\n",
    "    TEST_FILE = 'test.parquet'\n",
    "\n",
    "    # [ìˆ˜ì •] í˜„ì¬ ì‹œê°„ì„ êµ¬í•´ì„œ íŒŒì¼ëª… ì ‘ë‘ì‚¬ì— ë¶™ì…ë‹ˆë‹¤.\n",
    "    # ì˜ˆ: XGB_CPU_20240214_2040\n",
    "    NOW = datetime.datetime.now().strftime('%Y%m%d_%H%M') \n",
    "    OUTPUT_PREFIX = f'XGB_CPU_{NOW}'\n",
    "    OUTPUT_DIR = os.path.join('results', OUTPUT_PREFIX)\n",
    "\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    NEG_RATIO = 1\n",
    "    VALID_RATIO = 0.2\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'aucpr',\n",
    "        'learning_rate': 0.08,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_estimators': 500,\n",
    "        'early_stopping_rounds': 30,\n",
    "        'tree_method': 'hist',\n",
    "        'min_child_weight' : 3,\n",
    "        'reg_lambda' : 5,\n",
    "        'reg_alpha' : 1,\n",
    "        'device': 'cuda',\n",
    "    }\n",
    "\n",
    "    # Feature Engineering Flags (True: ì‚¬ìš© / False: ë¯¸ì‚¬ìš©)\n",
    "    USE_SEQ_FEATURES = False\n",
    "    USE_SEQ_FEATURES_V2 = True    # seq ì‹¬ì¸µ íŒŒìƒë³€ìˆ˜ (nunique, click_score, repeat_ratio)\n",
    "    USE_INTERACTION_FEATURES = True\n",
    "    USE_GROUPBY_FEATURES = True\n",
    "    USE_COUNT_FEATURES = True\n",
    "\n",
    "    # ë²”ì£¼í˜• ë³€ìˆ˜ ëª©ë¡ (unique.db ê¸°ë°˜)\n",
    "    CATEGORICAL_COLS = [\n",
    "        'gender', 'age_group', 'inventory_id', 'day_of_week', 'hour',\n",
    "        'l_feat_1', 'l_feat_2', 'l_feat_3', 'l_feat_4', 'l_feat_8',\n",
    "        'l_feat_13', 'l_feat_16', 'l_feat_18', 'l_feat_19', 'l_feat_20',\n",
    "        'l_feat_21', 'l_feat_22', 'l_feat_23', 'l_feat_24', 'l_feat_26',\n",
    "        'l_feat_27', 'feat_e_4', 'feat_d_1', 'feat_d_2', 'feat_d_3',\n",
    "        'feat_d_5', 'feat_d_6', 'feat_a_1', 'feat_a_2', 'feat_a_3',\n",
    "        'feat_a_4', 'feat_a_6', 'feat_a_7', 'feat_a_8', 'feat_a_9',\n",
    "        'feat_a_10', 'feat_a_11', 'feat_a_12', 'feat_a_13', 'feat_a_15',\n",
    "        'feat_a_16', 'feat_a_17', 'feat_a_18'\n",
    "    ]\n",
    "\n",
    "     # ì „ì²´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ëª©ë¡ (119ê°œ - ë²”ì£¼í˜• - seq - clicked)\n",
    "    ALL_NUMERIC_COLS = [\n",
    "        'l_feat_5', 'l_feat_6', 'l_feat_7', 'l_feat_9', 'l_feat_10',\n",
    "        'l_feat_11', 'l_feat_12', 'l_feat_14', 'l_feat_15', 'l_feat_17',\n",
    "        'l_feat_25',\n",
    "        'feat_e_1', 'feat_e_2', 'feat_e_3', 'feat_e_5', 'feat_e_6',\n",
    "        'feat_e_7', 'feat_e_8', 'feat_e_9', 'feat_e_10',\n",
    "        'feat_d_4',\n",
    "        'feat_c_1', 'feat_c_2', 'feat_c_3', 'feat_c_4', 'feat_c_5',\n",
    "        'feat_c_6', 'feat_c_7', 'feat_c_8',\n",
    "        'feat_b_1', 'feat_b_2', 'feat_b_3', 'feat_b_4', 'feat_b_5',\n",
    "        'feat_b_6',\n",
    "        'feat_a_5', 'feat_a_14',\n",
    "        'history_a_1', 'history_a_2', 'history_a_3', 'history_a_4',\n",
    "        'history_a_5', 'history_a_6', 'history_a_7',\n",
    "        'history_b_1', 'history_b_2', 'history_b_3', 'history_b_4',\n",
    "        'history_b_5', 'history_b_6', 'history_b_7', 'history_b_8',\n",
    "        'history_b_9', 'history_b_10', 'history_b_11', 'history_b_12',\n",
    "        'history_b_13', 'history_b_14', 'history_b_15', 'history_b_16',\n",
    "        'history_b_17', 'history_b_18', 'history_b_19', 'history_b_20',\n",
    "        'history_b_21', 'history_b_22', 'history_b_23', 'history_b_24',\n",
    "        'history_b_25', 'history_b_26', 'history_b_27', 'history_b_28',\n",
    "        'history_b_29', 'history_b_30',\n",
    "    ]\n",
    "    \n",
    "    DEFAULT_AGG_DICT = {col: ['mean', 'std'] for col in ALL_NUMERIC_COLS}\n",
    "\n",
    "    # EXCLUDE = {*CATEGORICAL_COLS, 'seq', 'clicked'}\n",
    "\n",
    "    # # Groupby í†µê³„ì— ì‚¬ìš©í•  agg_dict (ê¸°ë³¸ê°’)\n",
    "\n",
    "    # DEFAULT_AGG_DICT = {\n",
    "    # col: ['mean', 'std'] \n",
    "    # for col in train_df.columns \n",
    "    # if col not in EXCLUDE\n",
    "    #}\n",
    "\n",
    "    # DEFAULT_AGG_DICT = {\n",
    "    #     'history_a_1': ['mean', 'std'], 'history_a_2': ['mean', 'std'],\n",
    "    #     'history_a_3': ['mean', 'std'], 'history_a_6': ['mean', 'std'],\n",
    "    #     'feat_e_2': ['mean', 'std'], 'feat_e_3': ['mean', 'std'],\n",
    "    #     'feat_c_8': ['mean', 'std'], 'feat_e_9': ['mean', 'std'],\n",
    "    #     'feat_d_4': ['mean', 'std'],\n",
    "    #     'l_feat_1': ['mean', 'std'], 'l_feat_2': ['mean', 'std'],\n",
    "    #     'l_feat_5': ['mean', 'std'], 'l_feat_7': ['mean', 'std'],\n",
    "    #     'l_feat_10': ['mean', 'std'], 'l_feat_15': ['mean', 'std'],\n",
    "    # }\n",
    "\n",
    "    \n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    \"\"\"ëª¨ë“  ë‚œìˆ˜ ìƒì„±ê¸°ë¥¼ ê³ ì •í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥\"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def save_experiment_config(cfg):\n",
    "    \"\"\"ì„¤ì •ê°’(íŒŒë¼ë¯¸í„°, í”Œë˜ê·¸ ë“±)ì„ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    import json\n",
    "    config_data = {}\n",
    "    \n",
    "    # XGB_PARAMS ì €ì¥\n",
    "    if hasattr(cfg, 'XGB_PARAMS'):\n",
    "        config_data.update(cfg.XGB_PARAMS)\n",
    "        \n",
    "    # Feature Flags ë° ê¸°íƒ€ ì„¤ì • ì €ì¥\n",
    "    for attr in dir(cfg):\n",
    "        if attr.startswith('USE_') or attr in ['VALID_RATIO', 'NEG_RATIO', 'SEEDS', 'OUTPUT_PREFIX']:\n",
    "            val = getattr(cfg, attr)\n",
    "            # serialize numpy types\n",
    "            if hasattr(val, 'item'): val = val.item()\n",
    "            config_data[attr] = val\n",
    "            \n",
    "    def default(o):\n",
    "        if hasattr(o, 'item'): return o.item()\n",
    "        return str(o)\n",
    "    \n",
    "    filename = os.path.join(cfg.OUTPUT_DIR, f\"{cfg.OUTPUT_PREFIX}_config.json\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_data, f, indent=4, ensure_ascii=False, default=default)\n",
    "        print(f\"ğŸ“„ Configuration saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to save configuration: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b05cd",
   "metadata": {},
   "source": [
    "[PART 3] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4308a1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- í´ë¦­ í™•ë¥  ë§µ ë¡œë“œ ---\n",
    "try:\n",
    "    _df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    CLICK_PROB_MAP = dict(zip(_df_click_prob['number'], _df_click_prob['click_prob']))\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ 'high_click_numbers.xlsx' not found. click_prob features will be 0.\")\n",
    "    CLICK_PROB_MAP = {}\n",
    "\n",
    "POS_LIST = frozenset({370, 528, 68, 561, 144, 227, 417, 442, 186, 395})\n",
    "NEG_LIST = frozenset({154, 222, 84, 498, 434, 511, 216, 497, 309, 446})\n",
    "\n",
    "\n",
    "\n",
    "def add_seq_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    seq ì»¬ëŸ¼ì—ì„œ íŒŒìƒ í”¼ì²˜ 4ê°œ ìƒì„±:\n",
    "      seq_len, avg_click_prob, seq_neglogcount, seq_poslogcount\n",
    "    \"\"\"\n",
    "    seq_col = df[\"seq\"]\n",
    "\n",
    "    seq_len = np.zeros(len(df), dtype=np.int32)\n",
    "    avg_prob = np.zeros(len(df), dtype=np.float32)\n",
    "    seq_neg = np.zeros(len(df), dtype=np.int32)\n",
    "    seq_pos = np.zeros(len(df), dtype=np.int32)\n",
    "\n",
    "    for i, s in enumerate(seq_col):\n",
    "        if isinstance(s, str) and s:\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "            seq_len[i] = len(arr)\n",
    "            if CLICK_PROB_MAP:\n",
    "                probs = [CLICK_PROB_MAP.get(num, 0.0) for num in arr]\n",
    "                avg_prob[i] = np.mean(probs) if probs else 0.0\n",
    "            seq_neg[i] = sum(1 for x in arr if x in NEG_LIST)\n",
    "            seq_pos[i] = sum(1 for x in arr if x in POS_LIST)\n",
    "\n",
    "    df[\"seq_len\"] = seq_len\n",
    "    df[\"avg_click_prob\"] = avg_prob\n",
    "\n",
    "    df[\"seq_neglogcount\"] = seq_neg\n",
    "    df[\"seq_poslogcount\"] = seq_pos\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def build_item_ctr_from_train(train_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    trainì˜ seq + clicked ì»¬ëŸ¼ìœ¼ë¡œ ì•„ì´í…œë³„ CTR ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±.\n",
    "    CTR = clicked=1ì—ì„œì˜ ë“±ì¥ ë¹ˆë„ / ì „ì²´ ë“±ì¥ ë¹ˆë„\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "    pos_counter = Counter()\n",
    "    neg_counter = Counter()\n",
    "\n",
    "    for clicked_val, counter in [(1, pos_counter), (0, neg_counter)]:\n",
    "        subset = train_df[train_df['clicked'] == clicked_val]['seq']\n",
    "        for s in subset:\n",
    "            if isinstance(s, str) and s:\n",
    "                items = s.split(',')\n",
    "                counter.update(items)\n",
    "\n",
    "    all_items = set(pos_counter.keys()) | set(neg_counter.keys())\n",
    "    item_ctr = {}\n",
    "    for item in all_items:\n",
    "        freq_1 = pos_counter.get(item, 0)\n",
    "        freq_0 = neg_counter.get(item, 0)\n",
    "        total = freq_1 + freq_0\n",
    "        item_ctr[item] = freq_1 / total if total > 0 else 0.0\n",
    "\n",
    "    print(f\"  âœ… Item CTR map built: {len(item_ctr):,} items\")\n",
    "    return item_ctr\n",
    "\n",
    "\n",
    "def add_seq_features_v2(df: pd.DataFrame, item_ctr_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    seq ì»¬ëŸ¼ì—ì„œ ì‹¬ì¸µ íŒŒìƒ í”¼ì²˜ 3ê°œ ìƒì„± (Cohen's d ìƒìœ„ 3ê°œ):\n",
    "      seq_nunique      : ê³ ìœ  ì•„ì´í…œ ìˆ˜ (íƒìƒ‰ ë‹¤ì–‘ì„±)\n",
    "      seq_click_score  : ì•„ì´í…œë³„ CTR í‰ê·  (í´ë¦­ ê²½í–¥ ì ìˆ˜)\n",
    "      seq_repeat_ratio : ë°˜ë³µ ë¹„ìœ¨ (ê´€ì‹¬ ì§‘ì¤‘ë„)\n",
    "    \"\"\"\n",
    "    seq_col = df[\"seq\"]\n",
    "\n",
    "    nunique = np.zeros(len(df), dtype=np.int32)\n",
    "    click_score = np.zeros(len(df), dtype=np.float32)\n",
    "    repeat_ratio = np.zeros(len(df), dtype=np.float32)\n",
    "\n",
    "    for i, s in enumerate(seq_col):\n",
    "        if isinstance(s, str) and s:\n",
    "            items = s.split(\",\")\n",
    "            seq_len = len(items)\n",
    "            unique_count = len(set(items))\n",
    "\n",
    "            nunique[i] = unique_count\n",
    "            repeat_ratio[i] = (seq_len - unique_count) / seq_len if seq_len > 0 else 0.0\n",
    "\n",
    "            if item_ctr_map:\n",
    "                ctrs = [item_ctr_map.get(item, 0.0) for item in items]\n",
    "                click_score[i] = np.mean(ctrs) if ctrs else 0.0\n",
    "\n",
    "    df[\"seq_nunique\"] = nunique\n",
    "    df[\"seq_click_score\"] = click_score\n",
    "    df[\"seq_repeat_ratio\"] = repeat_ratio\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_interaction_features(df: pd.DataFrame, cols: list = None) -> pd.DataFrame:\n",
    "    \"\"\"ë²”ì£¼í˜• í”¼ì²˜ ìŒì˜ ì¡°í•©ìœ¼ë¡œ interaction í”¼ì²˜ ìƒì„±\"\"\"\n",
    "    if cols is None:\n",
    "        cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    for col1, col2 in combinations(cols, 2):\n",
    "        new_col = f'{col1}_{col2}'\n",
    "        df[new_col] = df[col1].astype(str) + '_' + df[col2].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_groupby_features(train_df, val_df, test_df, feature_name, agg_dict=None):\n",
    "    \"\"\"\n",
    "    train_df ê¸°ì¤€ìœ¼ë¡œ groupby í†µê³„ ê³„ì‚° â†’ val, testì— merge.\n",
    "    agg_dict ë¯¸ì§€ì • ì‹œ CFG.DEFAULT_AGG_DICT ì‚¬ìš©.\n",
    "    ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì»¬ëŸ¼ ë° ë¹„ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì€ ìë™ ìŠ¤í‚µ.\n",
    "    \"\"\"\n",
    "    if agg_dict is None:\n",
    "        agg_dict = CFG.DEFAULT_AGG_DICT\n",
    "\n",
    "    if feature_name not in train_df.columns:\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "    filtered_agg = {\n",
    "        k: v for k, v in agg_dict.items()\n",
    "        if k in train_df.columns and pd.api.types.is_numeric_dtype(train_df[k])\n",
    "    }\n",
    "    if not filtered_agg:\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    agg_stats = train_df.groupby(feature_name).agg(filtered_agg)\n",
    "    new_cols = [f\"{feature_name}_{col[0]}_{col[1]}\" for col in agg_stats.columns]\n",
    "    agg_stats.columns = new_cols\n",
    "    agg_stats = agg_stats.reset_index()\n",
    "\n",
    "    train_df = train_df.merge(agg_stats, on=feature_name, how='left')\n",
    "    val_df = val_df.merge(agg_stats, on=feature_name, how='left')\n",
    "    test_df = test_df.merge(agg_stats, on=feature_name, how='left')\n",
    "\n",
    "    for col in new_cols:\n",
    "        fill_val = train_df[col].mean()\n",
    "        val_df[col] = val_df[col].fillna(fill_val)\n",
    "        test_df[col] = test_df[col].fillna(fill_val)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def add_count_features(train_df, val_df, test_df, count_cols):\n",
    "    \"\"\"Count Encoding: trainì—ì„œë§Œ ë¹ˆë„ ê³„ì‚° â†’ val, testì— ì ìš©.\"\"\"\n",
    "    for col in count_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "        count_map = train_df[col].value_counts().to_dict()\n",
    "        train_df[f\"{col}_count\"] = train_df[col].map(count_map).fillna(0).astype(int)\n",
    "        val_df[f\"{col}_count\"] = val_df[col].map(count_map).fillna(0).astype(int)\n",
    "        test_df[f\"{col}_count\"] = test_df[col].map(count_map).fillna(0).astype(int)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def pandas_hstack(matrices):\n",
    "    \"\"\"\n",
    "    CPUìš© ìë™ hstack í•¨ìˆ˜ (Pandas/Scipy ìŠ¤íƒ€ì¼).\n",
    "    Dense Array, Pandas DataFrame, CSR Matrix ë¬´ì—‡ì´ë“  ì„ì—¬ ìˆì–´ë„ í•©ì³ì¤Œ.\n",
    "    \"\"\"\n",
    "    converted_list = []\n",
    "    for mat in matrices:\n",
    "        if hasattr(mat, 'tocsr'):           # scipy sparse\n",
    "            converted_list.append(mat.tocsr())\n",
    "        elif hasattr(mat, 'values'):        # pandas DataFrame/Series\n",
    "            arr = mat.values\n",
    "            if arr.ndim == 1:\n",
    "                arr = arr.reshape(-1, 1)\n",
    "            converted_list.append(csr_matrix(arr))\n",
    "        elif isinstance(mat, np.ndarray):   # numpy array\n",
    "            if mat.ndim == 1:\n",
    "                mat = mat.reshape(-1, 1)\n",
    "            converted_list.append(csr_matrix(mat))\n",
    "        else:\n",
    "            converted_list.append(csr_matrix(mat))\n",
    "    return hstack(converted_list).tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be665395",
   "metadata": {},
   "source": [
    "[PART 4] í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76a9221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_toss_score(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Toss ëŒ€íšŒ ìŠ¤ì½”ì–´ ê³„ì‚°:\n",
    "    Score = 0.5 * AP + 0.5 * (1 / (1 + WeightedLogLoss))\n",
    "    \"\"\"\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "\n",
    "    N_1 = np.sum(y_true == 1)\n",
    "    N_0 = np.sum(y_true == 0)\n",
    "    if N_1 == 0 or N_0 == 0:\n",
    "        return 0.5, 0.0, 0.0\n",
    "\n",
    "    weights = np.where(y_true == 1, 0.5 / N_1, 0.5 / N_0)\n",
    "    wll = log_loss(y_true, y_pred_clipped, sample_weight=weights)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, threshold=0.5):\n",
    "    \"\"\"\n",
    "    XGBoost ëª¨ë¸ì˜ í‰ê°€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜.\n",
    "    ROC-AUC, PR-AUC, Avg Precision, Precision, Recall, F1-Score, Accuracy, Log Loss, Toss Score\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for name, X, y_true in [('Train', X_train, y_train), ('Valid', X_val, y_val)]:\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        y_pred_class = (y_prob >= threshold).astype(int)\n",
    "\n",
    "        y_true_np = np.asarray(y_true)\n",
    "\n",
    "        roc = roc_auc_score(y_true_np, y_prob)\n",
    "        prec_curve, rec_curve, _ = precision_recall_curve(y_true_np, y_prob)\n",
    "        pr_auc = auc(rec_curve, prec_curve)\n",
    "        avg_prec = average_precision_score(y_true_np, y_prob)\n",
    "        ll = log_loss(y_true_np, y_prob)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_np, y_pred_class, average='binary', zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(y_true_np, y_pred_class)\n",
    "        toss, _, _ = compute_toss_score(y_true_np, y_prob)\n",
    "\n",
    "        results[name] = {\n",
    "            'ROC-AUC': roc, 'PR-AUC': pr_auc, 'Avg Precision': avg_prec,\n",
    "            'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'Accuracy': acc,\n",
    "            'Log Loss': ll, 'Toss Score': toss\n",
    "        }\n",
    "\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    return df_results\n",
    "\n",
    "\n",
    "def soft_voting_ensemble(prediction_files, output_file):\n",
    "    \"\"\"ì—¬ëŸ¬ ì‹œë“œì˜ ì˜ˆì¸¡ íŒŒì¼ì„ í‰ê· í•˜ì—¬ ì•™ìƒë¸”\"\"\"\n",
    "    if not prediction_files:\n",
    "        print(\"âŒ No prediction files found.\")\n",
    "        return\n",
    "\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={'clicked': 'click_0'})\n",
    "    for i, f in enumerate(prediction_files[1:], 1):\n",
    "        d = pd.read_csv(f).rename(columns={'clicked': f'click_{i}'})\n",
    "        merged = merged.merge(d, on='ID')\n",
    "\n",
    "    click_cols = [c for c in merged.columns if c.startswith('click_')]\n",
    "    merged['clicked'] = merged[click_cols].mean(axis=1)\n",
    "\n",
    "    merged[['ID', 'clicked']].to_csv(output_file, index=False)\n",
    "    print(f\"ğŸ‰ Ensemble saved: {output_file}\")\n",
    "    print(merged.head())\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6168c4",
   "metadata": {},
   "source": [
    "[PART 5] ë©”ì¸ í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be4040c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Configuration saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_config.json\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Processing Seed: 42\n",
      "============================================================\n",
      "ğŸ“‚ Loading: /home/konkukstat/python3/workspace/data/train.parquet\n",
      "âœ… Train loaded: (10704179, 119)\n",
      "âœ… Test loaded: (1527298, 119)\n",
      "ğŸ”„ Converting categorical columns to str...\n",
      "âœ… Categorical dtype conversion done\n",
      "â­ï¸ Skipping seq features...\n",
      "ğŸ› ï¸ Building item CTR map from train seq...\n",
      "  âœ… Item CTR map built: 594 items\n",
      "ğŸ› ï¸ Adding seq features v2 (nunique, click_score, repeat_ratio)...\n",
      "âœ… seq íŒŒìƒ í”¼ì²˜ ìƒì„± + seq ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ“Š Splitting train/validation...\n",
      "ğŸ”» Down-sampling train only...\n",
      "  After downsampling: (326686, 121)\n",
      "ğŸ› ï¸ Feature Engineering...\n",
      "  -> Adding interaction features...\n",
      "  -> Adding groupby features...\n",
      "  -> Adding count features...\n",
      "  ìˆ˜ì¹˜í˜• í”¼ì²˜: 230ê°œ, ë²”ì£¼í˜• í”¼ì²˜: 53ê°œ\n",
      "ğŸ“‹ Preprocessing numerical features...\n",
      "ğŸ“‹ OneHot Encoding categorical features...\n",
      "  OneHot shape: Train=(326686, 1753), Val=(2140836, 1753), Test=(1527298, 1753)\n",
      "ğŸ”— Combining features with hstack...\n",
      "  X_train: (326686, 1983), X_val: (2140836, 1983), X_test: (1527298, 1983)\n",
      "ğŸ”¥ Training XGBoost...\n",
      "[0]\tvalidation_0-aucpr:0.05529\n",
      "[50]\tvalidation_0-aucpr:0.07327\n",
      "[100]\tvalidation_0-aucpr:0.07641\n",
      "[150]\tvalidation_0-aucpr:0.07768\n",
      "[200]\tvalidation_0-aucpr:0.07844\n",
      "[250]\tvalidation_0-aucpr:0.07885\n",
      "[300]\tvalidation_0-aucpr:0.07910\n",
      "[350]\tvalidation_0-aucpr:0.07939\n",
      "[400]\tvalidation_0-aucpr:0.07963\n",
      "[450]\tvalidation_0-aucpr:0.07982\n",
      "[499]\tvalidation_0-aucpr:0.07998\n",
      "ğŸ“Š Evaluating...\n",
      "       ROC-AUC  PR-AUC  Avg Precision  Precision  Recall  F1-Score  Accuracy  Log Loss  Toss Score\n",
      "Train   0.7854  0.7916         0.7916     0.7227  0.6878    0.7048    0.7120    0.5607      0.7162\n",
      "Valid   0.7426  0.0800         0.0800     0.0409  0.6546    0.0770    0.7005    0.5922      0.3532\n",
      "ğŸ“Š Metrics saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_metrics_42.csv\n",
      "ğŸ’¾ Saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_42.csv\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Processing Seed: 106\n",
      "============================================================\n",
      "ğŸ“‚ Loading: /home/konkukstat/python3/workspace/data/train.parquet\n",
      "âœ… Train loaded: (10704179, 119)\n",
      "âœ… Test loaded: (1527298, 119)\n",
      "ğŸ”„ Converting categorical columns to str...\n",
      "âœ… Categorical dtype conversion done\n",
      "â­ï¸ Skipping seq features...\n",
      "ğŸ› ï¸ Building item CTR map from train seq...\n",
      "  âœ… Item CTR map built: 594 items\n",
      "ğŸ› ï¸ Adding seq features v2 (nunique, click_score, repeat_ratio)...\n",
      "âœ… seq íŒŒìƒ í”¼ì²˜ ìƒì„± + seq ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ“Š Splitting train/validation...\n",
      "ğŸ”» Down-sampling train only...\n",
      "  After downsampling: (326686, 121)\n",
      "ğŸ› ï¸ Feature Engineering...\n",
      "  -> Adding interaction features...\n",
      "  -> Adding groupby features...\n",
      "  -> Adding count features...\n",
      "  ìˆ˜ì¹˜í˜• í”¼ì²˜: 230ê°œ, ë²”ì£¼í˜• í”¼ì²˜: 53ê°œ\n",
      "ğŸ“‹ Preprocessing numerical features...\n",
      "ğŸ“‹ OneHot Encoding categorical features...\n",
      "  OneHot shape: Train=(326686, 1751), Val=(2140836, 1751), Test=(1527298, 1751)\n",
      "ğŸ”— Combining features with hstack...\n",
      "  X_train: (326686, 1981), X_val: (2140836, 1981), X_test: (1527298, 1981)\n",
      "ğŸ”¥ Training XGBoost...\n",
      "[0]\tvalidation_0-aucpr:0.05434\n",
      "[50]\tvalidation_0-aucpr:0.07268\n",
      "[100]\tvalidation_0-aucpr:0.07582\n",
      "[150]\tvalidation_0-aucpr:0.07721\n",
      "[200]\tvalidation_0-aucpr:0.07781\n",
      "[250]\tvalidation_0-aucpr:0.07826\n",
      "[300]\tvalidation_0-aucpr:0.07862\n",
      "[350]\tvalidation_0-aucpr:0.07887\n",
      "[400]\tvalidation_0-aucpr:0.07893\n",
      "[403]\tvalidation_0-aucpr:0.07896\n",
      "ğŸ“Š Evaluating...\n",
      "       ROC-AUC  PR-AUC  Avg Precision  Precision  Recall  F1-Score  Accuracy  Log Loss  Toss Score\n",
      "Train   0.7777  0.7836         0.7836     0.7149  0.6829    0.6985    0.7053    0.5670      0.7109\n",
      "Valid   0.7415  0.0790         0.0790     0.0406  0.6569    0.0764    0.6970    0.5934      0.3525\n",
      "ğŸ“Š Metrics saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_metrics_106.csv\n",
      "ğŸ’¾ Saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_106.csv\n",
      "\n",
      "============================================================\n",
      "ğŸš€ Processing Seed: 1031\n",
      "============================================================\n",
      "ğŸ“‚ Loading: /home/konkukstat/python3/workspace/data/train.parquet\n",
      "âœ… Train loaded: (10704179, 119)\n",
      "âœ… Test loaded: (1527298, 119)\n",
      "ğŸ”„ Converting categorical columns to str...\n",
      "âœ… Categorical dtype conversion done\n",
      "â­ï¸ Skipping seq features...\n",
      "ğŸ› ï¸ Building item CTR map from train seq...\n",
      "  âœ… Item CTR map built: 594 items\n",
      "ğŸ› ï¸ Adding seq features v2 (nunique, click_score, repeat_ratio)...\n",
      "âœ… seq íŒŒìƒ í”¼ì²˜ ìƒì„± + seq ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\n",
      "ğŸ“Š Splitting train/validation...\n",
      "ğŸ”» Down-sampling train only...\n",
      "  After downsampling: (326686, 121)\n",
      "ğŸ› ï¸ Feature Engineering...\n",
      "  -> Adding interaction features...\n",
      "  -> Adding groupby features...\n",
      "  -> Adding count features...\n",
      "  ìˆ˜ì¹˜í˜• í”¼ì²˜: 230ê°œ, ë²”ì£¼í˜• í”¼ì²˜: 53ê°œ\n",
      "ğŸ“‹ Preprocessing numerical features...\n",
      "ğŸ“‹ OneHot Encoding categorical features...\n",
      "  OneHot shape: Train=(326686, 1756), Val=(2140836, 1756), Test=(1527298, 1756)\n",
      "ğŸ”— Combining features with hstack...\n",
      "  X_train: (326686, 1986), X_val: (2140836, 1986), X_test: (1527298, 1986)\n",
      "ğŸ”¥ Training XGBoost...\n",
      "[0]\tvalidation_0-aucpr:0.05090\n",
      "[50]\tvalidation_0-aucpr:0.07300\n",
      "[100]\tvalidation_0-aucpr:0.07610\n",
      "[150]\tvalidation_0-aucpr:0.07734\n",
      "[200]\tvalidation_0-aucpr:0.07803\n",
      "[250]\tvalidation_0-aucpr:0.07843\n",
      "[300]\tvalidation_0-aucpr:0.07854\n",
      "[350]\tvalidation_0-aucpr:0.07870\n",
      "[400]\tvalidation_0-aucpr:0.07898\n",
      "[450]\tvalidation_0-aucpr:0.07914\n",
      "[499]\tvalidation_0-aucpr:0.07912\n",
      "ğŸ“Š Evaluating...\n",
      "       ROC-AUC  PR-AUC  Avg Precision  Precision  Recall  F1-Score  Accuracy  Log Loss  Toss Score\n",
      "Train   0.7841  0.7895         0.7895     0.7218  0.6881    0.7046    0.7115    0.5619      0.7149\n",
      "Valid   0.7403  0.0792         0.0792     0.0404  0.6517    0.0760    0.6978    0.5930      0.3525\n",
      "ğŸ“Š Metrics saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_metrics_1031.csv\n",
      "ğŸ’¾ Saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_1031.csv\n",
      "\n",
      "âœ… All seeds processed: ['results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_42.csv', 'results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_106.csv', 'results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_1031.csv']\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 0. ì„¤ì • ì €ì¥ â”€â”€\n",
    "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
    "save_experiment_config(CFG)\n",
    "\n",
    "prediction_files = []\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸš€ Processing Seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # â”€â”€ 1. ë°ì´í„° ë¡œë“œ â”€â”€\n",
    "    train_path = os.path.join(CFG.DATA_PATH, CFG.TRAIN_FILE)\n",
    "    print(f\"ğŸ“‚ Loading: {train_path}\")\n",
    "    full_train_df = pd.read_parquet(train_path)\n",
    "    print(f\"âœ… Train loaded: {full_train_df.shape}\")\n",
    "\n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, CFG.TEST_FILE))\n",
    "        print(f\"âœ… Test loaded: {test_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ Test not found. Creating dummy.\")\n",
    "        test_df = full_train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(100)]\n",
    "\n",
    "    # â”€â”€ 1-1. ë²”ì£¼í˜• ë³€ìˆ˜ dtype ë³€í™˜ â”€â”€\n",
    "    print(\"ğŸ”„ Converting categorical columns to str...\")\n",
    "    for col in CFG.CATEGORICAL_COLS:\n",
    "        if col in full_train_df.columns:\n",
    "            full_train_df[col] = full_train_df[col].astype(str)\n",
    "        if col in test_df.columns:\n",
    "            test_df[col] = test_df[col].astype(str)\n",
    "    print(\"âœ… Categorical dtype conversion done\")\n",
    "\n",
    "    # â”€â”€ 2. seq íŒŒìƒ í”¼ì²˜ ìƒì„± â”€â”€\n",
    "    if CFG.USE_SEQ_FEATURES:\n",
    "        print(\"ğŸ› ï¸ Adding seq features...\")\n",
    "        full_train_df = add_seq_features(full_train_df)\n",
    "        test_df = add_seq_features(test_df)\n",
    "    else:\n",
    "        print(\"â­ï¸ Skipping seq features...\")\n",
    "\n",
    "    # â”€â”€ 2-1. seq ì‹¬ì¸µ íŒŒìƒ í”¼ì²˜ ìƒì„± (v2) â”€â”€\n",
    "    if CFG.USE_SEQ_FEATURES_V2:\n",
    "        print(\"ğŸ› ï¸ Building item CTR map from train seq...\")\n",
    "        item_ctr_map = build_item_ctr_from_train(full_train_df)\n",
    "        print(\"ğŸ› ï¸ Adding seq features v2 (nunique, click_score, repeat_ratio)...\")\n",
    "        full_train_df = add_seq_features_v2(full_train_df, item_ctr_map)\n",
    "        test_df = add_seq_features_v2(test_df, item_ctr_map)\n",
    "    else:\n",
    "        print(\"â­ï¸ Skipping seq features v2...\")\n",
    "\n",
    "    # â”€â”€ 3. seq ì»¬ëŸ¼ ë³´ê´€ í›„ ì œê±° (ì„ë² ë”©ìš©ìœ¼ë¡œ ë”°ë¡œ ë³´ê´€) â”€â”€\n",
    "    train_seq_raw = full_train_df['seq'].copy() if 'seq' in full_train_df.columns else None\n",
    "    test_seq_raw = test_df['seq'].copy() if 'seq' in test_df.columns else None\n",
    "    if 'seq' in full_train_df.columns:\n",
    "        full_train_df.drop(columns=['seq'], inplace=True)\n",
    "    if 'seq' in test_df.columns:\n",
    "        test_df.drop(columns=['seq'], inplace=True)\n",
    "    print(\"âœ… seq íŒŒìƒ í”¼ì²˜ ìƒì„± + seq ì»¬ëŸ¼ ì œê±° ì™„ë£Œ\")\n",
    "\n",
    "    # â”€â”€ 4. Train / Validation Split â”€â”€\n",
    "    print(\"ğŸ“Š Splitting train/validation...\")\n",
    "    train_df, val_df = train_test_split(\n",
    "        full_train_df, test_size=CFG.VALID_RATIO,\n",
    "        random_state=seed, stratify=full_train_df['clicked']\n",
    "    )\n",
    "    # seqë„ ê°™ì´ split\n",
    "    if train_seq_raw is not None:\n",
    "        train_seq = train_seq_raw.loc[train_df.index]\n",
    "        val_seq = train_seq_raw.loc[val_df.index]\n",
    "    del full_train_df; gc.collect()\n",
    "\n",
    "    # â”€â”€ 5. Train setë§Œ ë‹¤ìš´ìƒ˜í”Œë§ â”€â”€\n",
    "    print(\"ğŸ”» Down-sampling train only...\")\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    n_neg = min(int(len(clicked_1) * CFG.NEG_RATIO),\n",
    "                len(train_df[train_df['clicked'] == 0]))\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=n_neg, random_state=seed)\n",
    "    \n",
    "    # [FIX] ë¨¼ì € ê²°í•© ë° ì…”í”Œ (ì¸ë±ìŠ¤ ìœ ì§€)\n",
    "    downsampled_df = pd.concat([clicked_1, clicked_0]).sample(frac=1, random_state=seed)\n",
    "    \n",
    "    # seq ë°ì´í„° ë™ê¸°í™” (ê°™ì€ ì¸ë±ìŠ¤ ì‚¬ìš©) ë° Series ìœ ì§€\n",
    "    if train_seq_raw is not None:\n",
    "        train_seq = train_seq_raw.loc[downsampled_df.index].reset_index(drop=True)\n",
    "        \n",
    "    # train_df ìµœì¢… í™•ì •\n",
    "    train_df = downsampled_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"  After downsampling: {train_df.shape}\")\n",
    "\n",
    "    # â”€â”€ 6. Feature Engineering â”€â”€\n",
    "    print(\"ğŸ› ï¸ Feature Engineering...\")\n",
    "    \n",
    "    if CFG.USE_INTERACTION_FEATURES:\n",
    "        print(\"  -> Adding interaction features...\")\n",
    "        train_df = add_interaction_features(train_df)\n",
    "        val_df = add_interaction_features(val_df)\n",
    "        test_df = add_interaction_features(test_df)\n",
    "    else:\n",
    "        print(\"  -> Skipping interaction features...\")\n",
    "\n",
    "    if CFG.USE_GROUPBY_FEATURES:\n",
    "        print(\"  -> Adding groupby features...\")\n",
    "        train_df, val_df, test_df = add_groupby_features(\n",
    "            train_df, val_df, test_df, 'inventory_id'\n",
    "        )\n",
    "    else:\n",
    "        print(\"  -> Skipping groupby features...\")\n",
    "\n",
    "    if CFG.USE_COUNT_FEATURES:\n",
    "        print(\"  -> Adding count features...\")\n",
    "        count_target_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "        count_target_cols = [c for c in count_target_cols if c in train_df.columns]\n",
    "        train_df, val_df, test_df = add_count_features(train_df, val_df, test_df, count_target_cols)\n",
    "    else:\n",
    "        print(\"  -> Skipping count features...\")\n",
    "\n",
    "    # â”€â”€ 7. ì»¬ëŸ¼ ë¶„ë¦¬: ìˆ˜ì¹˜í˜• / ë²”ì£¼í˜• â”€â”€\n",
    "    exclude_cols = {'clicked', 'ID'}\n",
    "    all_cols = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "    # ë²”ì£¼í˜• = CFG.CATEGORICAL_COLS + object/string dtype (interaction í”¼ì²˜ í¬í•¨)\n",
    "    cat_cols = [\n",
    "        c for c in all_cols\n",
    "        if (c in CFG.CATEGORICAL_COLS or not pd.api.types.is_numeric_dtype(train_df[c]))\n",
    "        and c in val_df.columns and c in test_df.columns\n",
    "    ]\n",
    "    # ìˆ˜ì¹˜í˜• = ë‚˜ë¨¸ì§€ (ë²”ì£¼í˜• ì œì™¸)\n",
    "    num_cols = [c for c in all_cols if c not in cat_cols\n",
    "                and c in val_df.columns and c in test_df.columns]\n",
    "\n",
    "    print(f\"  ìˆ˜ì¹˜í˜• í”¼ì²˜: {len(num_cols)}ê°œ, ë²”ì£¼í˜• í”¼ì²˜: {len(cat_cols)}ê°œ\")\n",
    "\n",
    "    # â”€â”€ 8. ì „ì²˜ë¦¬: ìˆ˜ì¹˜í˜• â”€â”€\n",
    "    print(\"ğŸ“‹ Preprocessing numerical features...\")\n",
    "    # Scaling\n",
    "    scaler = StandardScaler()\n",
    "    train_df[num_cols] = scaler.fit_transform(train_df[num_cols])\n",
    "    val_df[num_cols] = scaler.transform(val_df[num_cols])\n",
    "    test_df[num_cols] = scaler.transform(test_df[num_cols])\n",
    "\n",
    "    # NaN ì²˜ë¦¬ (train ê¸°ì¤€ í‰ê· )\n",
    "    num_imputer = SimpleImputer(strategy='mean')\n",
    "    train_df[num_cols] = num_imputer.fit_transform(train_df[num_cols])\n",
    "    val_df[num_cols] = num_imputer.transform(val_df[num_cols])\n",
    "    test_df[num_cols] = num_imputer.transform(test_df[num_cols])\n",
    "\n",
    "    # â”€â”€ 9. ì „ì²˜ë¦¬: ë²”ì£¼í˜• (OneHotEncoder â†’ CSR) â”€â”€\n",
    "    print(\"ğŸ“‹ OneHot Encoding categorical features...\")\n",
    "    # ë²”ì£¼í˜• NaN â†’ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    train_df[cat_cols] = cat_imputer.fit_transform(train_df[cat_cols])\n",
    "    val_df[cat_cols] = cat_imputer.transform(val_df[cat_cols])\n",
    "    test_df[cat_cols] = cat_imputer.transform(test_df[cat_cols])\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    train_cat_sparse = ohe.fit_transform(train_df[cat_cols])\n",
    "    val_cat_sparse = ohe.transform(val_df[cat_cols])\n",
    "    test_cat_sparse = ohe.transform(test_df[cat_cols])\n",
    "    print(f\"  OneHot shape: Train={train_cat_sparse.shape}, Val={val_cat_sparse.shape}, Test={test_cat_sparse.shape}\")\n",
    "\n",
    "    # â”€â”€ 10. seq ì„ë² ë”© (ì„ íƒ ì‚¬í•­ â€” ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ) â”€â”€\n",
    "    ### seq ì„ë² ë”© ì‹œì‘ ###\n",
    "    # from src.models.seq_embedding import SeqEncoder\n",
    "    # seq_encoder = SeqEncoder(embedding_dim=16)\n",
    "    \n",
    "    # # Train: í•™ìŠµ + ë²¡í„° ì¶”ì¶œ\n",
    "    # seq_train = seq_encoder.get_features(\n",
    "    #     train_seq,\n",
    "    #     target_series=train_df['clicked'],\n",
    "    #     train_mode=True, epochs=5\n",
    "    # )\n",
    "    # # Valid/Test: ë²¡í„°ë§Œ ì¶”ì¶œ\n",
    "    # seq_val = seq_encoder.get_features(val_seq, train_mode=False)\n",
    "    # seq_test = seq_encoder.get_features(test_seq_raw, train_mode=False)\n",
    "    # ## seq ì„ë² ë”© ë â†’ ì•„ë˜ Noneì„ ì‚­ì œí•˜ê³  ìœ„ ì£¼ì„ í•´ì œ ###\n",
    "    seq_train = None\n",
    "    seq_val = None\n",
    "    seq_test = None\n",
    "\n",
    "    # â”€â”€ 11. Feature ê²°í•© (hstack) â”€â”€\n",
    "    print(\"ğŸ”— Combining features with hstack...\")\n",
    "    hstack_list_train = [train_df[num_cols], train_cat_sparse]\n",
    "    hstack_list_val = [val_df[num_cols], val_cat_sparse]\n",
    "    hstack_list_test = [test_df[num_cols], test_cat_sparse]\n",
    "\n",
    "    if seq_train is not None:\n",
    "        hstack_list_train.append(seq_train)\n",
    "        hstack_list_val.append(seq_val)\n",
    "        hstack_list_test.append(seq_test)\n",
    "\n",
    "    X_train = pandas_hstack(hstack_list_train)\n",
    "    y_train = train_df['clicked'].values\n",
    "    X_val = pandas_hstack(hstack_list_val)\n",
    "    y_val = val_df['clicked'].values\n",
    "    X_test = pandas_hstack(hstack_list_test)\n",
    "\n",
    "    print(f\"  X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # â”€â”€ 12. ëª¨ë¸ í•™ìŠµ â”€â”€\n",
    "    print(\"ğŸ”¥ Training XGBoost...\")\n",
    "    model = xgb.XGBClassifier(**CFG.XGB_PARAMS, random_state=seed)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=50\n",
    "    )\n",
    "\n",
    "    # â”€â”€ 13. í‰ê°€ â”€â”€\n",
    "    print(\"ğŸ“Š Evaluating...\")\n",
    "    eval_df = evaluate_model(model, X_train, y_train, X_val, y_val, threshold=0.5)\n",
    "    print(eval_df.round(4).to_string())\n",
    "\n",
    "    # â”€â”€ 13-1. í‰ê°€ ì§€í‘œ CSV ì €ì¥ â”€â”€\n",
    "    metrics_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_metrics_{seed}.csv\")\n",
    "    eval_df.to_csv(metrics_file)\n",
    "    print(f\"ğŸ“Š Metrics saved: {metrics_file}\")\n",
    "\n",
    "    # â”€â”€ 14. ì˜ˆì¸¡ ì €ì¥ â”€â”€\n",
    "    test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    out_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_{seed}.csv\")\n",
    "    sub = pd.DataFrame({\n",
    "        'ID': test_df['ID'].values if 'ID' in test_df.columns else range(len(test_pred)),\n",
    "        'clicked': test_pred\n",
    "    })\n",
    "    sub.to_csv(out_file, index=False)\n",
    "    prediction_files.append(out_file)\n",
    "    print(f\"ğŸ’¾ Saved: {out_file}\")\n",
    "\n",
    "    del train_df, val_df, model, X_train, X_val, X_test\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… All seeds processed: {prediction_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86ad6c1",
   "metadata": {},
   "source": [
    "[PART 6] Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2298daf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Creating metrics summary...\n",
      "ğŸ“Š Summary metrics saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_metrics_summary.csv\n",
      "\n",
      "       ROC-AUC  PR-AUC  Avg Precision  Precision  Recall  F1-Score  Accuracy  Log Loss  Toss Score  Seed\n",
      "Train   0.7854  0.7916         0.7916     0.7227  0.6878    0.7048    0.7120    0.5607      0.7162    42\n",
      "Valid   0.7426  0.0800         0.0800     0.0409  0.6546    0.0770    0.7005    0.5922      0.3532    42\n",
      "Train   0.7777  0.7836         0.7836     0.7149  0.6829    0.6985    0.7053    0.5670      0.7109   106\n",
      "Valid   0.7415  0.0790         0.0790     0.0406  0.6569    0.0764    0.6970    0.5934      0.3525   106\n",
      "Train   0.7841  0.7895         0.7895     0.7218  0.6881    0.7046    0.7115    0.5619      0.7149  1031\n",
      "Valid   0.7403  0.0792         0.0792     0.0404  0.6517    0.0760    0.6978    0.5930      0.3525  1031\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Creating metrics summary...\")\n",
    "all_metrics = []\n",
    "for seed in CFG.SEEDS:\n",
    "    metrics_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_metrics_{seed}.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file, index_col=0)\n",
    "        df['Seed'] = seed\n",
    "        all_metrics.append(df)\n",
    "\n",
    "if all_metrics:\n",
    "    summary_metrics = pd.concat(all_metrics, ignore_index=False)\n",
    "    summary_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_metrics_summary.csv\")\n",
    "    summary_metrics.to_csv(summary_file)\n",
    "    print(f\"ğŸ“Š Summary metrics saved: {summary_file}\")\n",
    "    print(\"\\n\" + summary_metrics.round(4).to_string())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d352e669",
   "metadata": {},
   "source": [
    "[PART 7] Soft Voting ì•™ìƒë¸”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0b1fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ Ensemble saved: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_SoftVoting.csv\n",
      "             ID   click_0   click_1   click_2   clicked\n",
      "0  TEST_0000000  0.270375  0.353513  0.327055  0.316981\n",
      "1  TEST_0000001  0.307034  0.276096  0.314368  0.299166\n",
      "2  TEST_0000002  0.389529  0.451339  0.457040  0.432636\n",
      "3  TEST_0000003  0.527451  0.540300  0.432835  0.500195\n",
      "4  TEST_0000004  0.347424  0.426988  0.319743  0.364718\n",
      "\n",
      "============================================================\n",
      "ğŸ ëª¨ë“  í•™ìŠµ ë° ì•™ìƒë¸” ì™„ë£Œ!\n",
      "  ì‹œë“œ: [42, 106, 1031]\n",
      "  ê°œë³„ ì˜ˆì¸¡: ['results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_42.csv', 'results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_106.csv', 'results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_1031.csv']\n",
      "  ì•™ìƒë¸” ê²°ê³¼: results/XGB_CPU_20260226_0729/XGB_CPU_20260226_0729_SoftVoting.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "final_output = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_SoftVoting.csv\")\n",
    "soft_voting_ensemble(prediction_files, final_output)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ ëª¨ë“  í•™ìŠµ ë° ì•™ìƒë¸” ì™„ë£Œ!\")\n",
    "print(f\"  ì‹œë“œ: {CFG.SEEDS}\")\n",
    "print(f\"  ê°œë³„ ì˜ˆì¸¡: {prediction_files}\")\n",
    "print(f\"  ì•™ìƒë¸” ê²°ê³¼: {final_output}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
