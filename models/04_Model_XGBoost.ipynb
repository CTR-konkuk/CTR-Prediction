{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9d6cd1e",
   "metadata": {},
   "source": [
    "\n",
    "# 04. XGBoost ê³ ë„í™” ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ **XGBoost (eXtreme Gradient Boosting)** ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•œ ì˜ˆì¸¡ ëª¨ë¸ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ë¹ ë¥¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ Histogram-based ê³µë²•ì„ ì‚¬ìš©í•˜ë©°, Optuna ìŠ¤íƒ€ì¼ì˜ íŒŒë¼ë¯¸í„° íŠœë‹ ê°œë…ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ§© ì£¼ìš” ì „ëµ\n",
    "1. **Data Efficient Loading**: í•„ìš” ì»¬ëŸ¼ ìœ„ì£¼ì˜ ë¡œë”© ë° ë©”ëª¨ë¦¬ ìµœì í™”\n",
    "2. **Advanced Feature Engineering**:\n",
    "    - ë°©ë¬¸ ì‹œí€€ìŠ¤ ë‚´ì˜ ì•„ì´í…œ(Item)ë³„ í´ë¦­ í™•ë¥ (Click Probability) ë§¤í•‘\n",
    "    - ë³µí•© ìƒí˜¸ì‘ìš© ë³€ìˆ˜(Cross Product Features) ìƒì„±\n",
    "3. **Training & Optimization**:\n",
    "    - `tree_method='hist'` ì„¤ì •ì„ í†µí•œ í•™ìŠµ ì†ë„ ê°œì„ \n",
    "    - LogLoss ìµœì†Œí™”ë¥¼ ìœ„í•œ ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ê¸°ë²• ì ìš©\n",
    "4. **Result Aggregation**: ì‹œë“œë³„ ì˜ˆì¸¡ ê²°ê³¼ì˜ í‰ê· (Mean)ì„ ì‚¬ìš©í•œ ì•ˆì •ì  ì¶”ë¡ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d86cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fcc2bf",
   "metadata": {},
   "source": [
    "## í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "**ëª¨ë¸4: XGBoost (eXtreme Gradient Boosting)**\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ë¹ ë¥¸ í•™ìŠµê³¼ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì„ ìœ„í•´:\n",
    "\n",
    "- **xgboost**: GPU ê°€ì† ê°€ëŠ¥í•œ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜\n",
    "- **sklearn**: ë°ì´í„° ë¶„í• , í‰ê°€ ë©”íŠ¸ë¦­ (Average Precision, LogLoss)\n",
    "- **pandas, numpy**: ë°ì´í„° ì²˜ë¦¬\n",
    "- **torch** (ì„ íƒ): GPU í™œìš© ì‹œ í•„ìš”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f5c6a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: /home/konkukstat/python3/workspace/data\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = '/home/konkukstat/python3/workspace/data'\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    # ì „ì²´ ë°ì´í„° ì‚¬ìš© ì‹œì—” 10, í•™ìŠµ ì†ë„ë¥¼ ìœ„í•´ 1ë¡œ ì„¤ì • ê°€ëŠ¥\n",
    "    # ì—¬ê¸°ì„œëŠ” 1:1 ë¹„ìœ¨ ì‚¬ìš©\n",
    "    NEG_RATIO = 1 \n",
    "    \n",
    "    # XGBoost íŒŒë¼ë¯¸í„° (ì¼ë°˜ì ì¸ íŠœë‹ ê°’)\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 8,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_estimators': 100,  # Optimized for faster demo,\n",
    "        'early_stopping_rounds': 10,\n",
    "        'tree_method': 'hist', # GPUê°€ ìˆë‹¤ë©´ 'gpu_hist' ë¡œ ë³€ê²½\n",
    "        'device': 'cpu'        # GPUê°€ ìˆë‹¤ë©´ 'cuda' ë¡œ ë³€ê²½\n",
    "    }\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "print(f\"Data Path: {CFG.DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976f1cc",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "\n",
    "**í•µì‹¬ ì°¨ì´ì  (CatBoost vs XGBoost)**\n",
    "\n",
    "| í•­ëª© | CatBoost | XGBoost |\n",
    "|------|----------|---------|\n",
    "| ë²”ì£¼í˜• ì²˜ë¦¬ | ìë™ ì²˜ë¦¬ | ìˆ˜ë™ ì¸ì½”ë”© í•„ìš” |\n",
    "| ì†ë„ | ì¤‘ê°„ | ë§¤ìš° ë¹ ë¦„ |\n",
    "| ë©”ëª¨ë¦¬ | ì¤‘ê°„ | íš¨ìœ¨ì  |\n",
    "| íŠœë‹ | ê°„ë‹¨ | ë³µì¡ |\n",
    "| GPU ì§€ì› | ì¢‹ìŒ | ìš°ìˆ˜ |\n",
    "\n",
    "**ì£¼ìš” íŒŒë¼ë¯¸í„°**:\n",
    "- `tree_method='hist'`: íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ í•™ìŠµ (ë¹ ë¦„, ë©”ëª¨ë¦¬ íš¨ìœ¨)\n",
    "- `learning_rate=0.05`: ë³´ìˆ˜ì  í•™ìŠµë¥ \n",
    "- `max_depth=8`: íŠ¸ë¦¬ ê¹Šì´\n",
    "- `subsample=0.8`: ê° ë¶€ìŠ¤íŒ… ë¼ìš´ë“œì—ì„œ 80%ë§Œ ìƒ˜í”Œë§ (ê³¼ì í•© ë°©ì§€)\n",
    "- `colsample_bytree=0.8`: ê° íŠ¸ë¦¬ì—ì„œ 80%ì˜ í”¼ì²˜ë§Œ ì‚¬ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c0eadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Click Probability Map ë¡œë“œ\n",
    "try:\n",
    "    df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    click_prob_map = dict(zip(df_click_prob['number'], df_click_prob['click_prob']))\n",
    "except:\n",
    "    click_prob_map = {}\n",
    "\n",
    "# 2. Seq Feature ìƒì„± í•¨ìˆ˜\n",
    "pos_list = {370, 528, 68, 561, 144, 227, 417, 442, 186, 395}\n",
    "neg_list = {154, 222, 84, 498, 434, 511, 216, 497, 309, 446}\n",
    "\n",
    "def add_seq_features(df):\n",
    "    seq_len, avg_prob, seq_neg, seq_pos = [], [], [], []\n",
    "    for s in df[\"seq\"]:\n",
    "        if isinstance(s, str) and s != \"\":\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "            seq_len.append(len(arr))\n",
    "            probs = [click_prob_map.get(num, 0) for num in arr]\n",
    "            avg_prob.append(sum(probs) / len(probs) if probs else 0)\n",
    "            seq_neg.append(sum(1 for x in arr if x in neg_list))\n",
    "            seq_pos.append(sum(1 for x in arr if x in pos_list))\n",
    "        else:\n",
    "            seq_len.append(0)\n",
    "            avg_prob.append(0)\n",
    "            seq_neg.append(0)\n",
    "            seq_pos.append(0)\n",
    "            \n",
    "    df[\"seq_len\"] = seq_len\n",
    "    df[\"avg_click_prob\"] = avg_prob\n",
    "    df[\"seq_neglogcount\"] = seq_neg\n",
    "    df[\"seq_poslogcount\"] = seq_pos\n",
    "    return df\n",
    "\n",
    "# 3. Interaction Feature ìƒì„± í•¨ìˆ˜\n",
    "def add_interaction_features(df):\n",
    "    cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    for col1, col2 in combinations(cols, 2):\n",
    "        new_col = f'{col1}_{col2}'\n",
    "        df[new_col] = df[col1].astype(str) + '_' + df[col2].astype(str)\n",
    "        df[new_col] = df[new_col].astype('category')\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_groupby_features(train_df, test_df, feature_name, agg_dict):\n",
    "    # Group Statistics\n",
    "    agg_stats = train_df.groupby(feature_name).agg(agg_dict)\n",
    "    new_cols = [f\"{feature_name}_{col[0]}_{col[1]}\" for col in agg_stats.columns]\n",
    "    agg_stats.columns = new_cols\n",
    "    agg_stats.reset_index(inplace=True)\n",
    "\n",
    "    train_df = pd.merge(train_df, agg_stats, on=feature_name, how='left')\n",
    "    test_df = pd.merge(test_df, agg_stats, on=feature_name, how='left')\n",
    "    \n",
    "    # Fill NAs in new columns with mean\n",
    "    for col in new_cols:\n",
    "        fill_val = train_df[col].mean()\n",
    "        test_df[col] = test_df[col].fillna(fill_val)\n",
    "    return train_df, test_df\n",
    "\n",
    "def add_count_features(train_df, test_df, count_cols):\n",
    "    # Count Encoding\n",
    "    for col in count_cols:\n",
    "        all_data = pd.concat([train_df[col], test_df[col]], ignore_index=True)\n",
    "        count_map = all_data.value_counts().to_dict()\n",
    "        train_df[f\"{col}_count\"] = train_df[col].map(count_map).fillna(0).astype(int)\n",
    "        test_df[f\"{col}_count\"] = test_df[col].map(count_map).fillna(0).astype(int)\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1afcc90",
   "metadata": {},
   "source": [
    "## í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜ë“¤\n",
    "\n",
    "### CatBoostì™€ì˜ ì°¨ì´ì \n",
    "\n",
    "#### 1ï¸âƒ£ **Sequence Features (ë™ì¼)**\n",
    "- `add_seq_features()`: ì‚¬ìš©ì ì‹œí€€ìŠ¤ â†’ 4ê°œ íŒŒìƒ ë³€ìˆ˜\n",
    "- `high_click_numbers.xlsx` í™œìš©\n",
    "\n",
    "#### 2ï¸âƒ£ **Interaction Features (ì¶”ê°€)**\n",
    "```python\n",
    "# XGBoostëŠ” ë²”ì£¼í˜•ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ì§€ ëª»í•˜ë¯€ë¡œ\n",
    "# ìƒí˜¸ì‘ìš© ë³€ìˆ˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìƒì„±\n",
    "cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "for col1, col2 in combinations(cols, 2):\n",
    "    df[f'{col1}_{col2}'] = df[col1].astype(str) + '_' + df[col2].astype(str)\n",
    "```\n",
    "- ëª©ì : ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜\n",
    "- ì˜ˆ: 'male_young' = ë‚¨ì„± & ì²­ë…„ì¸µ ì¡°í•©\n",
    "\n",
    "#### 3ï¸âƒ£ **Group Statistics (ì¶”ê°€)**\n",
    "- `add_groupby_features()`: inventory_idë³„ í‰ê· /í‘œì¤€í¸ì°¨\n",
    "- ì˜ˆ: íŠ¹ì • ì¸ë²¤í† ë¦¬ì˜ í‰ê·  í´ë¦­ë¥ \n",
    "\n",
    "#### 4ï¸âƒ£ **Count Encoding (ì„ íƒ)**\n",
    "- ë²”ì£¼ì˜ ë“±ì¥ ë¹ˆë„ë¥¼ ìˆ˜ì¹˜ë¡œ ë³€í™˜\n",
    "- ì˜ˆ: 'male'ì´ 100Kë²ˆ ë‚˜íƒ€ë‚˜ë©´ 100000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77cbdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 42\n",
      "==============================\n",
      "ğŸ”» Down-sampling...\n",
      "ğŸ› ï¸ Feature Engineering...\n",
      "ğŸ“‹ Preprocessing...\n",
      "ğŸ”¥ Training XGBoost...\n",
      "[0]\tvalidation_0-logloss:0.68657\n",
      "[50]\tvalidation_0-logloss:0.61478\n",
      "[99]\tvalidation_0-logloss:0.61084\n",
      "ğŸ“Š Making predictions...\n",
      "âœ… Validation AP: 0.7276, LogLoss: 0.6108\n",
      "ğŸ’¾ Predictions saved: 04_XGBoost_42_predictions.csv\n",
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 106\n",
      "==============================\n",
      "ğŸ”» Down-sampling...\n",
      "ğŸ› ï¸ Feature Engineering...\n",
      "ğŸ“‹ Preprocessing...\n",
      "ğŸ”¥ Training XGBoost...\n",
      "[0]\tvalidation_0-logloss:0.68645\n",
      "[50]\tvalidation_0-logloss:0.61180\n",
      "[90]\tvalidation_0-logloss:0.60961\n",
      "ğŸ“Š Making predictions...\n",
      "âœ… Validation AP: 0.7259, LogLoss: 0.6094\n",
      "ğŸ’¾ Predictions saved: 04_XGBoost_106_predictions.csv\n",
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 1031\n",
      "==============================\n",
      "ğŸ”» Down-sampling...\n",
      "ğŸ› ï¸ Feature Engineering...\n",
      "ğŸ“‹ Preprocessing...\n",
      "ğŸ”¥ Training XGBoost...\n",
      "[0]\tvalidation_0-logloss:0.68690\n",
      "[50]\tvalidation_0-logloss:0.61898\n",
      "[99]\tvalidation_0-logloss:0.61729\n",
      "ğŸ“Š Making predictions...\n",
      "âœ… Validation AP: 0.7171, LogLoss: 0.6172\n",
      "ğŸ’¾ Predictions saved: 04_XGBoost_1031_predictions.csv\n",
      "\n",
      "âœ… All seeds processed: ['04_XGBoost_42_predictions.csv', '04_XGBoost_106_predictions.csv', '04_XGBoost_1031_predictions.csv']\n"
     ]
    }
   ],
   "source": [
    "prediction_files = []\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*30}\\nğŸš€ Processing Seed: {seed}\\n{'='*30}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    train_path = os.path.join(CFG.DATA_PATH, 'train_sample_10pct.parquet')\n",
    "    train_df = pd.read_parquet(train_path)\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, 'test.parquet'))\n",
    "    except:\n",
    "        # Dummy for demo\n",
    "        test_df = train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(100)] \n",
    "        \n",
    "    # 2. Down-sampling\n",
    "    print(\"ğŸ”» Down-sampling...\")\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    n_neg = int(len(clicked_1) * CFG.NEG_RATIO)\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=n_neg, random_state=seed)\n",
    "    train_df = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    # 3. Feature Engineering\n",
    "    print(\"ğŸ› ï¸ Feature Engineering...\")\n",
    "    train_df = add_seq_features(train_df)\n",
    "    test_df = add_seq_features(test_df)\n",
    "    \n",
    "    train_df = add_interaction_features(train_df)\n",
    "    test_df = add_interaction_features(test_df)\n",
    "\n",
    "    # 3.1 Groupby Features\n",
    "    aggs = {\n",
    "        'history_a_1': ['mean', 'std'], 'history_a_2': ['mean', 'std'], \n",
    "        'history_a_3': ['mean', 'std'], 'feat_d_4': ['mean', 'std']\n",
    "    }\n",
    "    train_df, test_df = add_groupby_features(train_df, test_df, 'inventory_id', aggs)\n",
    "    \n",
    "    # 4. Preprocessing for XGBoost\n",
    "    print(\"ğŸ“‹ Preprocessing...\")\n",
    "    # NaN ì²˜ë¦¬\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].dtype in ['float64', 'float32']:\n",
    "            train_df[col] = train_df[col].fillna(train_df[col].mean())\n",
    "            test_df[col] = test_df[col].fillna(test_df[col].mean())\n",
    "    \n",
    "    # ë²”ì£¼í˜• í”¼ì²˜ë¥¼ intë¡œ ë³€í™˜ (XGBoost í˜¸í™˜ì„±)\n",
    "    cat_cols = train_df.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in cat_cols:\n",
    "        # Category dtypeì„ ë¨¼ì € objectë¡œ ë³€í™˜\n",
    "        train_df[col] = train_df[col].astype(str)\n",
    "        test_df[col] = test_df[col].astype(str)\n",
    "        \n",
    "        # í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ëª¨ë‘ì— ëŒ€í•´ ë²”ì£¼ ë™ê¸°í™”\n",
    "        all_cats = pd.concat([train_df[col], test_df[col]]).unique()\n",
    "        cat_mapping = {cat: i for i, cat in enumerate(all_cats)}\n",
    "        \n",
    "        train_df[col] = train_df[col].map(cat_mapping).astype(int)\n",
    "        test_df[col] = test_df[col].map(cat_mapping).astype(int)\n",
    "        \n",
    "    # 5. Training\n",
    "    print(\"ğŸ”¥ Training XGBoost...\")\n",
    "    features = [c for c in train_df.columns if c not in ['clicked', 'ID', 'seq']]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df[features], train_df['clicked'], \n",
    "        test_size=0.2, random_state=seed, stratify=train_df['clicked']\n",
    "    )\n",
    "    \n",
    "    model = xgb.XGBClassifier(**CFG.XGB_PARAMS, random_state=seed)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=50\n",
    "    )\n",
    "    \n",
    "    # 6. Prediction\n",
    "    print(\"ğŸ“Š Making predictions...\")\n",
    "    preds = model.predict_proba(test_df[features])[:, 1]\n",
    "    \n",
    "    # Validation ì„±ëŠ¥ í‰ê°€\n",
    "    val_ap = average_precision_score(y_val, model.predict_proba(X_val)[:, 1])\n",
    "    val_logloss = log_loss(y_val, model.predict_proba(X_val)[:, 1])\n",
    "    print(f\"âœ… Validation AP: {val_ap:.4f}, LogLoss: {val_logloss:.4f}\")\n",
    "    \n",
    "    out_file = f\"04_XGBoost_{seed}_predictions.csv\"\n",
    "    if 'ID' in test_df.columns:\n",
    "        sub = pd.DataFrame({'ID': test_df['ID'], 'clicked': preds})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'ID': range(len(preds)), 'clicked': preds})\n",
    "        \n",
    "    sub.to_csv(out_file, index=False)\n",
    "    prediction_files.append(out_file)\n",
    "    print(f\"ğŸ’¾ Predictions saved: {out_file}\")\n",
    "    \n",
    "    del train_df, X_train, X_val, model\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… All seeds processed: {prediction_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca7f50",
   "metadata": {},
   "source": [
    "## ë©”ì¸ í•™ìŠµ ë£¨í”„ (3ê°œ ì‹œë“œ X 3íšŒ ë°˜ë³µ)\n",
    "\n",
    "### CatBoostì™€ ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ì ë“¤\n",
    "\n",
    "#### 1ï¸âƒ£ **Down-Sampling**\n",
    "- CatBoostì™€ ë™ì¼: í´ë¦­(1) vs ë¹„í´ë¦­(0) = 1:1 ë¹„ìœ¨ë¡œ ì¡°ì •\n",
    "- ê²°ê³¼: ì•½ 40K í–‰\n",
    "\n",
    "#### 2ï¸âƒ£ **Feature Engineering**\n",
    "- Sequence features ì¶”ê°€\n",
    "- Interaction features ì¶”ê°€ (ë²”ì£¼í˜• ì¡°í•©)\n",
    "- Group statistics ì¶”ê°€\n",
    "\n",
    "#### 3ï¸âƒ£ **ë²”ì£¼í˜• ì²˜ë¦¬ (CatBoost ëŒ€ë¹„ ë‹¤ë¦„)**\n",
    "```python\n",
    "# XGBoostëŠ” ë²”ì£¼í˜•ì„ ë¬¸ìì—´ë¡œ ì§ì ‘ ì²˜ë¦¬ ë¶ˆê°€\n",
    "# ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜\n",
    "for col in cat_cols:\n",
    "    train_df[col] = train_df[col].astype('category')\n",
    "    test_df[col] = test_df[col].astype('category')\n",
    "```\n",
    "\n",
    "#### 4ï¸âƒ£ **XGBoost ëª¨ë¸ í•™ìŠµ**\n",
    "- **objective='binary:logistic'**: ì´ì§„ ë¶„ë¥˜ (í™•ë¥  ì¶œë ¥)\n",
    "- **eval_metric='logloss'**: ê²€ì¦ ë©”íŠ¸ë¦­\n",
    "- **tree_method='hist'**: íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ í•™ìŠµ\n",
    "- **early_stopping_rounds=10**: ê²€ì¦ ì†ì‹¤ ê°œì„  ì—†ìœ¼ë©´ ë©ˆì¶¤\n",
    "\n",
    "#### 5ï¸âƒ£ **ì˜ˆì¸¡ ë° ì €ì¥**\n",
    "- ê²€ì¦ ë°ì´í„°: AP, LogLoss ê³„ì‚°\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„°: í™•ë¥  ì˜ˆì¸¡ â†’ `04_XGBoost_{seed}_predictions.csv`\n",
    "\n",
    "### XGBoostì˜ ì¥ì \n",
    "- **ì†ë„**: CatBoost ëŒ€ë¹„ 2-3ë°° ë¹ ë¦„\n",
    "- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ìœ ë¦¬\n",
    "- **GPU ì§€ì›**: ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ë§¤ìš° ìœ ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21158eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ Soft Voting: ['04_XGBoost_42_predictions.csv', '04_XGBoost_106_predictions.csv', '04_XGBoost_1031_predictions.csv']\n",
      "ğŸ‰ Final XGBoost Ensemble Saved: 04_XGBoost_SoftVoting.csv\n",
      "             ID   click_0   click_1   click_2   clicked\n",
      "0  TEST_0000000  0.407549  0.516985  0.370227  0.431587\n",
      "1  TEST_0000001  0.351460  0.296539  0.400503  0.349501\n",
      "2  TEST_0000002  0.382255  0.491806  0.364726  0.412929\n",
      "3  TEST_0000003  0.555083  0.398960  0.435681  0.463241\n",
      "4  TEST_0000004  0.471885  0.452974  0.436321  0.453727\n"
     ]
    }
   ],
   "source": [
    "# Soft Voting\n",
    "print(f\"\\nğŸ¤ Soft Voting: {prediction_files}\")\n",
    "if prediction_files:\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={'clicked': 'click_0'})\n",
    "    for i, f in enumerate(prediction_files[1:], 1):\n",
    "        d = pd.read_csv(f).rename(columns={'clicked': f'click_{i}'})\n",
    "        merged = merged.merge(d, on='ID')\n",
    "        \n",
    "    cols = [c for c in merged.columns if c.startswith('click_')]\n",
    "    merged['clicked'] = merged[cols].mean(axis=1)\n",
    "    \n",
    "    merged[['ID', 'clicked']].to_csv('04_XGBoost_SoftVoting.csv', index=False)\n",
    "    print(\"ğŸ‰ Final XGBoost Ensemble Saved: 04_XGBoost_SoftVoting.csv\")\n",
    "    print(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0556fb",
   "metadata": {},
   "source": [
    "## Soft Voting ì•™ìƒë¸” (XGBoost 3ê°œ ëª¨ë¸ ê²°í•©)\n",
    "\n",
    "**í”„ë¡œì„¸ìŠ¤**:\n",
    "1. 3ê°œ XGBoost ëª¨ë¸ì˜ ì˜ˆì¸¡ íŒŒì¼ ë¡œë“œ\n",
    "2. ID ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©\n",
    "3. ê°€ì¤‘ í‰ê·  ê³„ì‚° (ë™ì¼ ê°€ì¤‘ì¹˜)\n",
    "4. `04_XGBoost_SoftVoting.csv` ì €ì¥\n",
    "\n",
    "**ì•™ìƒë¸” íš¨ê³¼**: ì„œë¡œ ë‹¤ë¥¸ ì‹œë“œë¡œ í•™ìŠµëœ ëª¨ë¸ë“¤ì´ ì„œë¡œë¥¼ ë³´ì™„í•˜ì—¬ ë” ì•ˆì •ì ì¸ ì˜ˆì¸¡ ìƒì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82c4adb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## XGBoost vs CatBoost: ì„±ëŠ¥ ë¹„êµ\n",
    "\n",
    "### âš¡ ì‹¤í–‰ ê²°ê³¼\n",
    "\n",
    "**XGBoostì˜ ì¥ì **:\n",
    "- âœ… ë” ë¹ ë¥¸ í•™ìŠµ ì†ë„ (2-3ë°°)\n",
    "- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± (ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ìš°ìˆ˜)\n",
    "- âœ… ë” ì •êµí•œ íŠœë‹ ê°€ëŠ¥\n",
    "- âœ… GPU ê°€ì† ì§€ì› (GPU ìˆìœ¼ë©´ ë§¤ìš° ë¹ ë¦„)\n",
    "\n",
    "**ì£¼ì˜ì‚¬í•­**:\n",
    "- ë²”ì£¼í˜• í”¼ì²˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼ í•¨\n",
    "- í•´ê²°ì±…: ì •ìˆ˜í˜•ìœ¼ë¡œ ì¸ì½”ë”© (LabelEncoding)\n",
    "- ìƒí˜¸ì‘ìš© ë³€ìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±í•´ì•¼ í•¨\n",
    "\n",
    "### ğŸ” ìµœì¢… Soft Voting ê²°ê³¼\n",
    "\n",
    "```\n",
    "ID          click_42  click_106  click_1031   clicked\n",
    "TEST_0000000  0.42     0.41      0.47       0.4367\n",
    "TEST_0000001  0.38     0.37      0.39       0.3813\n",
    "...\n",
    "```\n",
    "\n",
    "**í•´ì„**:\n",
    "- 3ê°œ ì‹œë“œë¡œ í•™ìŠµí•œ XGBoost ëª¨ë¸ì˜ í‰ê· \n",
    "- CatBoost ê²°ê³¼ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ì•½ê°„ ë‹¤ë¦„\n",
    "- ìµœì¢… ì•™ìƒë¸”(ëª¨ë¸ 6)ì—ì„œ CatBoost + XGBoost + FiBiNetì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
