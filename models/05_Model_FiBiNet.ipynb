{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0128e5e",
   "metadata": {},
   "source": [
    "\n",
    "# 05. ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì ‘ê·¼: FiBiNet êµ¬í˜„\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ DeepCTR ê³„ì—´ì˜ **FiBiNet (Feature Importance and Bilinear Interaction Network)** ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "ê¸°ì¡´ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸(GBDT)ì´ í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ í”¼ì²˜ ê°„ì˜ ë¯¸ì„¸í•œ ìƒí˜¸ì‘ìš©ê³¼ ì¤‘ìš”ë„ë¥¼ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì•„í‚¤í…ì²˜\n",
    "1. **SENet (Squeeze-and-Excitation Network)**: í”¼ì²˜ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "2. **Bilinear Interaction**: í”¼ì²˜ ì„ë² ë”© ê°„ì˜ ì™¸ì (Outer Product)ì„ í†µí•œ 2ì°¨ ìƒí˜¸ì‘ìš© í¬ì°©\n",
    "3. **GaussRank Transformation**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ì •ê·œ ë¶„í¬í™”ë¥¼ í†µí•œ í•™ìŠµ ì•ˆì •ì„± í™•ë³´\n",
    "4. **Deep Network**: ê³ ì°¨ì› ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ MLP(Multi-Layer Perceptron) êµ¬ì¡°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd435606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "from scipy.stats import rankdata, norm\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”¥ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026b7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFG:\n",
    "    DATA_PATH = '../rawdata'\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    batch_size = 1024\n",
    "    epochs = 1  # Optimized for faster demo # ë°ëª¨ìš©, ì‹¤ì œëŠ” ë” ë§ì´\n",
    "    learning_rate = 0.001\n",
    "    embedding_dim = 16\n",
    "    \n",
    "    # ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨\n",
    "    NEG_RATIO = 5 \n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb594716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FiBiNET(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, dense_dim, hidden_units=[256, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(f, embed_dim) for f in field_dims])\n",
    "        self.se_fc1 = nn.Linear(self.num_fields, self.num_fields // 2)\n",
    "        self.se_fc2 = nn.Linear(self.num_fields // 2, self.num_fields)\n",
    "        \n",
    "        # Bilinear\n",
    "        num_pairs = self.num_fields * (self.num_fields - 1) // 2\n",
    "        self.W = nn.Parameter(torch.randn(num_pairs, embed_dim, embed_dim) * 0.02)\n",
    "        \n",
    "        input_dim = num_pairs * embed_dim + dense_dim\n",
    "        layers = []\n",
    "        for h in hidden_units:\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            input_dim = h\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.deep = nn.Sequential(*layers)\n",
    "        \n",
    "        self.pair_idx = []\n",
    "        for i in range(self.num_fields):\n",
    "            for j in range(i+1, self.num_fields):\n",
    "                self.pair_idx.append((i, j))\n",
    "                \n",
    "    def forward(self, x_sparse, x_dense):\n",
    "        # Embedding\n",
    "        emb = [e(x_sparse[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        emb = torch.stack(emb, dim=1) # [B, F, E]\n",
    "        \n",
    "        # SENet (Simple ver)\n",
    "        z = emb.mean(dim=2)\n",
    "        w = torch.relu(self.se_fc1(z))\n",
    "        w = torch.sigmoid(self.se_fc2(w)).unsqueeze(-1)\n",
    "        emb_se = emb * w\n",
    "        \n",
    "        # Bilinear Interaction\n",
    "        p = []\n",
    "        for k, (i, j) in enumerate(self.pair_idx):\n",
    "            v_i = emb_se[:, i]\n",
    "            v_j = emb_se[:, j]\n",
    "            # Simple element-wise\n",
    "            p.append(v_i * v_j)\n",
    "            \n",
    "        p = torch.cat(p, dim=1) # [B, Pairs*E]\n",
    "        \n",
    "        # Deep\n",
    "        x = torch.cat([p, x_dense], dim=1)\n",
    "        return torch.sigmoid(self.deep(x)).squeeze()\n",
    "\n",
    "class DeepFMDataset(Dataset):\n",
    "    def __init__(self, df, sparse_cols, dense_cols, target=None):\n",
    "        self.sparse = df[sparse_cols].values.astype(np.int64)\n",
    "        self.dense = df[dense_cols].values.astype(np.float32)\n",
    "        self.target = df[target].values.astype(np.float32) if target is not None else None\n",
    "        \n",
    "    def __len__(self): return len(self.sparse)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.target is not None:\n",
    "            return self.sparse[idx], self.dense[idx], self.target[idx]\n",
    "        return self.sparse[idx], self.dense[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d5476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gauss_rank_transform(series):\n",
    "    rank = rankdata(series)\n",
    "    rank = (rank - 0.5) / len(series)\n",
    "    rank = 2 * rank - 1\n",
    "    rank = np.clip(rank, -0.999, 0.999)\n",
    "    return norm.ppf((rank + 1) / 2)\n",
    "\n",
    "sparse_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "# Simple dense list for demo\n",
    "dense_cols = ['seq_len', 'avg_click_prob'] # In real scenario, use more\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86d2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Click Map Load\n",
    "try:\n",
    "    df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    click_prob_map = dict(zip(df_click_prob['number'], df_click_prob['click_prob']))\n",
    "except:\n",
    "    click_prob_map = {}\n",
    "\n",
    "def add_simple_features(df):\n",
    "    seq_len, avg_prob = [], []\n",
    "    for s in df[\"seq\"]:\n",
    "        if isinstance(s, str) and s != \"\":\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "            seq_len.append(len(arr))\n",
    "            probs = [click_prob_map.get(num, 0) for num in arr]\n",
    "            avg_prob.append(sum(probs) / len(probs) if probs else 0)\n",
    "        else:\n",
    "            seq_len.append(0)\n",
    "            avg_prob.append(0)\n",
    "    df['seq_len'] = seq_len\n",
    "    df['avg_click_prob'] = avg_prob\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eaded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 42\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction_files = []\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*30}\\nğŸš€ Processing Seed: {seed}\\n{'='*30}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    train_path = os.path.join(CFG.DATA_PATH, 'train_sample_10pct.parquet')\n",
    "    train_df = pd.read_parquet(train_path)\n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, 'test.parquet'))\n",
    "    except:\n",
    "        test_df = train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(100)]\n",
    "        \n",
    "    # 2. Feature Engineering\n",
    "    train_df = add_simple_features(train_df)\n",
    "    test_df = add_simple_features(test_df)\n",
    "    \n",
    "    # 3. Downsampling\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    n_neg = int(len(clicked_1) * CFG.NEG_RATIO)\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=n_neg, random_state=seed)\n",
    "    train_df = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    # 4. Preprocessing (GaussRank + LabelEnc)\n",
    "    for c in dense_cols:\n",
    "        train_df[c] = gauss_rank_transform(train_df[c].fillna(0))\n",
    "        test_df[c] = gauss_rank_transform(test_df[c].fillna(0))\n",
    "        \n",
    "    field_dims = []\n",
    "    for c in sparse_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Handle unknown\n",
    "        train_df[c] = train_df[c].astype(str)\n",
    "        test_df[c] = test_df[c].astype(str)\n",
    "        le.fit(pd.concat([train_df[c], test_df[c]]))\n",
    "        train_df[c] = le.transform(train_df[c])\n",
    "        test_df[c] = le.transform(test_df[c])\n",
    "        field_dims.append(len(le.classes_))\n",
    "        \n",
    "    # 5. Dataset & DataLoader\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df, train_df['clicked'], test_size=0.2, stratify=train_df['clicked'], random_state=seed\n",
    "    )\n",
    "    \n",
    "    train_ds = DeepFMDataset(X_train, sparse_cols, dense_cols, target='clicked')\n",
    "    val_ds = DeepFMDataset(X_val, sparse_cols, dense_cols, target='clicked')\n",
    "    train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CFG.batch_size*2, shuffle=False)\n",
    "    \n",
    "    # 6. Model Train\n",
    "    model = FiBiNET(field_dims, CFG.embedding_dim, len(dense_cols)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    print(\"ğŸ”¥ Training FiBiNet...\")\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for s, d, y in train_loader:\n",
    "            s, d, y = s.to(device), d.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(s, d)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "    # 7. Predict\n",
    "    model.eval()\n",
    "    test_ds = DeepFMDataset(test_df, sparse_cols, dense_cols)\n",
    "    test_loader = DataLoader(test_ds, batch_size=CFG.batch_size*2, shuffle=False)\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for s, d in test_loader:\n",
    "            s, d = s.to(device), d.to(device)\n",
    "            p = model(s, d).cpu().numpy()\n",
    "            preds.extend(p)\n",
    "            \n",
    "    out_file = f\"05_FiBiNet_{seed}.csv\"\n",
    "    if 'ID' in test_df.columns:\n",
    "        sub = pd.DataFrame({'ID': test_df['ID'], 'clicked': preds})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'ID': range(len(preds)), 'clicked': preds})\n",
    "        \n",
    "    sub.to_csv(out_file, index=False)\n",
    "    prediction_files.append(out_file)\n",
    "    print(f\"âœ… Saved: {out_file}\")\n",
    "    \n",
    "    del model, train_df\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ All seeds processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18668b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nğŸ¤ Soft Voting: {prediction_files}\")\n",
    "if prediction_files:\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={'clicked': 'click_0'})\n",
    "    for i, f in enumerate(prediction_files[1:], 1):\n",
    "        d = pd.read_csv(f).rename(columns={'clicked': f'click_{i}'})\n",
    "        merged = merged.merge(d, on='ID')\n",
    "        \n",
    "    cols = [c for c in merged.columns if c.startswith('click_')]\n",
    "    merged['clicked'] = merged[cols].mean(axis=1)\n",
    "    \n",
    "    merged[['ID', 'clicked']].to_csv('05_FiBiNet_SoftVoting.csv', index=False)\n",
    "    print(\"ğŸ‰ Final FiBiNet Ensemble Saved: fibinet.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
