{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0128e5e",
   "metadata": {},
   "source": [
    "\n",
    "# 05. ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì ‘ê·¼: FiBiNet êµ¬í˜„\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ DeepCTR ê³„ì—´ì˜ **FiBiNet (Feature Importance and Bilinear Interaction Network)** ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "ê¸°ì¡´ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸(GBDT)ì´ í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ í”¼ì²˜ ê°„ì˜ ë¯¸ì„¸í•œ ìƒí˜¸ì‘ìš©ê³¼ ì¤‘ìš”ë„ë¥¼ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì•„í‚¤í…ì²˜\n",
    "1. **SENet (Squeeze-and-Excitation Network)**: í”¼ì²˜ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "2. **Bilinear Interaction**: í”¼ì²˜ ì„ë² ë”© ê°„ì˜ ì™¸ì (Outer Product)ì„ í†µí•œ 2ì°¨ ìƒí˜¸ì‘ìš© í¬ì°©\n",
    "3. **GaussRank Transformation**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ì •ê·œ ë¶„í¬í™”ë¥¼ í†µí•œ í•™ìŠµ ì•ˆì •ì„± í™•ë³´\n",
    "4. **Deep Network**: ê³ ì°¨ì› ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ MLP(Multi-Layer Perceptron) êµ¬ì¡°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd435606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "from scipy.stats import rankdata, norm\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”¥ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41bf21",
   "metadata": {},
   "source": [
    "## í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "**ëª¨ë¸5: FiBiNet (ë”¥ëŸ¬ë‹ ê¸°ë°˜)**\n",
    "\n",
    "GBDT(ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…) ëª¨ë¸ ëŒ€ë¹„ ì™„ì „íˆ ë‹¤ë¥¸ ì ‘ê·¼:\n",
    "\n",
    "- **torch, torch.nn**: ì‹ ê²½ë§ êµ¬ì¶• ë° í•™ìŠµ\n",
    "- **Dataset, DataLoader**: ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ ë°ì´í„° ê´€ë¦¬\n",
    "- **LabelEncoder**: ë²”ì£¼í˜• â†’ ì •ìˆ˜ ë³€í™˜\n",
    "- **rankdata, norm**: GaussRank ì •ê·œí™” (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ì •ê·œ ë¶„í¬í™”)\n",
    "\n",
    "**ì¥ì **:\n",
    "- ë¹„ì„ í˜• ìƒí˜¸ì‘ìš© ìë™ í•™ìŠµ\n",
    "- í”¼ì²˜ ì„ë² ë”©ìœ¼ë¡œ í”¼ì²˜ ê°„ ìœ ì‚¬ì„± í•™ìŠµ\n",
    "- ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "026b7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = '/home/konkukstat/python3/workspace/data'\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    batch_size = 1024\n",
    "    epochs = 1  # Optimized for faster demo # ë°ëª¨ìš©, ì‹¤ì œëŠ” ë” ë§ì´\n",
    "    learning_rate = 0.001\n",
    "    embedding_dim = 16\n",
    "    \n",
    "    # ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨\n",
    "    NEG_RATIO = 5 \n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5b6d9",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "\n",
    "**GBDT vs ë”¥ëŸ¬ë‹ ì„¤ì • ë¹„êµ**\n",
    "\n",
    "| í•­ëª© | GBDT (CatBoost/XGBoost) | FiBiNet (ë”¥ëŸ¬ë‹) |\n",
    "|------|---------|---------|\n",
    "| ì…ë ¥ | ì›ë³¸ ë°ì´í„° | ì •ê·œí™”ëœ ë°ì´í„° |\n",
    "| ë°°ì¹˜ í¬ê¸° | ì „ì²´ ë˜ëŠ” ìƒ˜í”Œ | 1,024 (ë©”ëª¨ë¦¬ íš¨ìœ¨) |\n",
    "| ì—í¬í¬ | 1-100 ë°˜ë³µ | ì—¬ëŸ¬ ì—í¬í¬ |\n",
    "| í•™ìŠµë¥  | 0.01-0.1 | 0.001 (ë³´ìˆ˜ì ) |\n",
    "| ì •ê·œí™” | íŠ¸ë¦¬ ê¸°ë°˜ | ë“œë¡­ì•„ì›ƒ (0.2) |\n",
    "\n",
    "**íŠ¹ì§•**:\n",
    "- `batch_size=1,024`: ë©”ëª¨ë¦¬/ì†ë„ ê· í˜•\n",
    "- `epochs=1`: ë°ëª¨ìš© (ì‹¤ì œ: 10-50)\n",
    "- `embedding_dim=16`: ê° ë²”ì£¼ë¥¼ 16ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„\n",
    "- `learning_rate=0.001`: ì‹ ê²½ë§ì€ ë‚®ì€ í•™ìŠµë¥  ê¶Œì¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb594716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FiBiNET(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, dense_dim, hidden_units=[256, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(f, embed_dim) for f in field_dims])\n",
    "        self.se_fc1 = nn.Linear(self.num_fields, self.num_fields // 2)\n",
    "        self.se_fc2 = nn.Linear(self.num_fields // 2, self.num_fields)\n",
    "        \n",
    "        # Bilinear\n",
    "        num_pairs = self.num_fields * (self.num_fields - 1) // 2\n",
    "        self.W = nn.Parameter(torch.randn(num_pairs, embed_dim, embed_dim) * 0.02)\n",
    "        \n",
    "        input_dim = num_pairs * embed_dim + dense_dim\n",
    "        layers = []\n",
    "        for h in hidden_units:\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            input_dim = h\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.deep = nn.Sequential(*layers)\n",
    "        \n",
    "        self.pair_idx = []\n",
    "        for i in range(self.num_fields):\n",
    "            for j in range(i+1, self.num_fields):\n",
    "                self.pair_idx.append((i, j))\n",
    "                \n",
    "    def forward(self, x_sparse, x_dense):\n",
    "        # Embedding\n",
    "        emb = [e(x_sparse[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        emb = torch.stack(emb, dim=1) # [B, F, E]\n",
    "        \n",
    "        # SENet (Simple ver)\n",
    "        z = emb.mean(dim=2)\n",
    "        w = torch.relu(self.se_fc1(z))\n",
    "        w = torch.sigmoid(self.se_fc2(w)).unsqueeze(-1)\n",
    "        emb_se = emb * w\n",
    "        \n",
    "        # Bilinear Interaction\n",
    "        p = []\n",
    "        for k, (i, j) in enumerate(self.pair_idx):\n",
    "            v_i = emb_se[:, i]\n",
    "            v_j = emb_se[:, j]\n",
    "            # Simple element-wise\n",
    "            p.append(v_i * v_j)\n",
    "            \n",
    "        p = torch.cat(p, dim=1) # [B, Pairs*E]\n",
    "        \n",
    "        # Deep\n",
    "        x = torch.cat([p, x_dense], dim=1)\n",
    "        return torch.sigmoid(self.deep(x)).squeeze()\n",
    "\n",
    "class DeepFMDataset(Dataset):\n",
    "    def __init__(self, df, sparse_cols, dense_cols, target=None):\n",
    "        self.sparse = df[sparse_cols].values.astype(np.int64)\n",
    "        self.dense = df[dense_cols].values.astype(np.float32)\n",
    "        self.target = df[target].values.astype(np.float32) if target is not None else None\n",
    "        \n",
    "    def __len__(self): return len(self.sparse)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.target is not None:\n",
    "            return self.sparse[idx], self.dense[idx], self.target[idx]\n",
    "        return self.sparse[idx], self.dense[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052b5d1",
   "metadata": {},
   "source": [
    "## FiBiNet ì‹ ê²½ë§ ì•„í‚¤í…ì²˜\n",
    "\n",
    "**êµ¬ì¡°**: Embedding â†’ SENet â†’ Bilinear Interaction â†’ MLP â†’ ì´ì§„ ë¶„ë¥˜\n",
    "\n",
    "### 1ï¸âƒ£ **Embedding Layer**\n",
    "```\n",
    "ê° ë²”ì£¼í˜• ë³€ìˆ˜ â†’ ì„ë² ë”© ì°¨ì› (16D ë²¡í„°)\n",
    "ì˜ˆ: gender='male' â†’ [0.2, -0.1, ..., 0.5]  (16ê°œ ê°’)\n",
    "```\n",
    "- ëª©ì : ë²”ì£¼ ê°„ ìœ ì‚¬ì„± í•™ìŠµ\n",
    "\n",
    "### 2ï¸âƒ£ **SENet (Squeeze-and-Excitation Network)**\n",
    "```python\n",
    "z = emb.mean(dim=2)              # í”¼ì²˜ë³„ í‰ê· \n",
    "w = sigmoid(fc1(relu(fc1(z))))   # í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "emb_se = emb * w                  # ì¤‘ìš”í•œ í”¼ì²˜ì— ë†’ì€ ê°€ì¤‘ì¹˜\n",
    "```\n",
    "- ëª©ì : í”¼ì²˜ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •\n",
    "\n",
    "### 3ï¸âƒ£ **Bilinear Interaction (2ì°¨ ìƒí˜¸ì‘ìš©)**\n",
    "```python\n",
    "# ëª¨ë“  í”¼ì²˜ ìŒ ê°„ì˜ ì™¸ì  ê³„ì‚°\n",
    "v_i * v_j  # í”¼ì²˜ iì™€ jì˜ ìƒí˜¸ì‘ìš©\n",
    "```\n",
    "- ëª©ì : í”¼ì²˜ ê°„ ë¯¸ì„¸í•œ ê´€ê³„ í¬ì°©\n",
    "- ì˜ˆ: ì„±ë³„ Ã— ì—°ë ¹ëŒ€ì˜ ìƒí˜¸ì‘ìš©\n",
    "\n",
    "### 4ï¸âƒ£ **Deep Network (MLP)**\n",
    "```\n",
    "[ìƒí˜¸ì‘ìš© ë²¡í„°] â†’ Dense(256, ReLU) \n",
    "              â†’ Dropout(0.2)\n",
    "              â†’ Dense(128, ReLU)\n",
    "              â†’ Dropout(0.2)\n",
    "              â†’ Dense(1, Sigmoid) â†’ í™•ë¥  ì¶œë ¥\n",
    "```\n",
    "- ëª©ì : ê³ ì°¨ì› ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ\n",
    "\n",
    "### FiBiNetì˜ ê°•ì \n",
    "- **ìë™ íŠ¹ì„± ìƒí˜¸ì‘ìš©**: ìˆ˜ë™ìœ¼ë¡œ ë§Œë“¤ í•„ìš” ì—†ìŒ\n",
    "- **ë™ì  ê°€ì¤‘ì¹˜**: SENetìœ¼ë¡œ ì¤‘ìš” í”¼ì²˜ ê°•ì¡°\n",
    "- **ë¹„ì„ í˜• í•™ìŠµ**: ë³µì¡í•œ íŒ¨í„´ í¬ì°© ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10d5476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gauss_rank_transform(series):\n",
    "    rank = rankdata(series)\n",
    "    rank = (rank - 0.5) / len(series)\n",
    "    rank = 2 * rank - 1\n",
    "    rank = np.clip(rank, -0.999, 0.999)\n",
    "    return norm.ppf((rank + 1) / 2)\n",
    "\n",
    "sparse_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "# Simple dense list for demo\n",
    "dense_cols = ['seq_len', 'avg_click_prob'] # In real scenario, use more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132d4af",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "### 1ï¸âƒ£ **GaussRank Transform**\n",
    "```python\n",
    "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¥¼ ì •ê·œ ë¶„í¬ë¡œ ë³€í™˜\n",
    "rank = rankdata(series)                    # ìˆœìœ„ ë³€í™˜\n",
    "rank = (rank - 0.5) / len(series)         # [0,1] ì •ê·œí™”\n",
    "rank = norm.ppf((rank + 1) / 2)           # ì—­ CDF ì ìš© â†’ ì •ê·œë¶„í¬\n",
    "```\n",
    "- ëª©ì : ì‹ ê²½ë§ í•™ìŠµ ì•ˆì •í™”\n",
    "- ì´ìœ : ì‹ ê²½ë§ì€ ì •ê·œí™”ëœ ì…ë ¥ì„ ì„ í˜¸\n",
    "\n",
    "### 2ï¸âƒ£ **ë²”ì£¼í˜• ì¸ì½”ë”©**\n",
    "```python\n",
    "# LabelEncoder: ë²”ì£¼ â†’ ì •ìˆ˜\n",
    "gender: {'male': 0, 'female': 1, 'other': 2}\n",
    "```\n",
    "\n",
    "### 3ï¸âƒ£ **Dataset í´ë˜ìŠ¤**\n",
    "```python\n",
    "class DeepFMDataset(Dataset):\n",
    "    def __getitem__(idx):\n",
    "        return sparse[idx], dense[idx], target[idx]\n",
    "```\n",
    "- ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì œê³µ\n",
    "- PyTorch DataLoaderì™€ í˜¸í™˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a86d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Click Map Load\n",
    "try:\n",
    "    df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    click_prob_map = dict(zip(df_click_prob['number'], df_click_prob['click_prob']))\n",
    "except:\n",
    "    click_prob_map = {}\n",
    "\n",
    "def add_simple_features(df):\n",
    "    seq_len, avg_prob = [], []\n",
    "    for s in df[\"seq\"]:\n",
    "        if isinstance(s, str) and s != \"\":\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "            seq_len.append(len(arr))\n",
    "            probs = [click_prob_map.get(num, 0) for num in arr]\n",
    "            avg_prob.append(sum(probs) / len(probs) if probs else 0)\n",
    "        else:\n",
    "            seq_len.append(0)\n",
    "            avg_prob.append(0)\n",
    "    df['seq_len'] = seq_len\n",
    "    df['avg_click_prob'] = avg_prob\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebf20e",
   "metadata": {},
   "source": [
    "## í´ë¦­ í™•ë¥  ë§µ ë¡œë“œ ë° ê°„ë‹¨ í”¼ì²˜ ìƒì„±\n",
    "\n",
    "**GBDT vs ë”¥ëŸ¬ë‹ì˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì°¨ì´**\n",
    "\n",
    "| ë°©ì‹ | GBDT | ë”¥ëŸ¬ë‹ |\n",
    "|------|------|--------|\n",
    "| í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ | ìˆ˜ë™, ìƒì„¸ | ìµœì†Œí™” (ì‹ ê²½ë§ì´ ìë™ í•™ìŠµ) |\n",
    "| ìƒí˜¸ì‘ìš© | ìˆ˜ë™ ìƒì„± | ìë™ í•™ìŠµ |\n",
    "| ì •ê·œí™” | ì„ íƒ | í•„ìˆ˜ |\n",
    "\n",
    "**ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„±**:\n",
    "- `seq_len`: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "- `avg_click_prob`: í‰ê·  í´ë¦­ í™•ë¥ \n",
    "\n",
    "ì‹ ê²½ë§ì€ ë³µì¡í•œ íŒŒìƒ ë³€ìˆ˜ë³´ë‹¤ **ì •ê·œí™”ëœ ì›ë³¸ ë°ì´í„°**ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7eaded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 42\n",
      "==============================\n",
      "ğŸ”¥ Training FiBiNet...\n",
      "Epoch 1 Loss: 0.4658\n",
      "âœ… Saved: 05_FiBiNet_42.csv\n",
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 106\n",
      "==============================\n",
      "ğŸ”¥ Training FiBiNet...\n",
      "Epoch 1 Loss: 0.4592\n",
      "âœ… Saved: 05_FiBiNet_106.csv\n",
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 1031\n",
      "==============================\n",
      "ğŸ”¥ Training FiBiNet...\n",
      "Epoch 1 Loss: 0.4708\n",
      "âœ… Saved: 05_FiBiNet_1031.csv\n",
      "\n",
      "ğŸ All seeds processed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction_files = []\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*30}\\nğŸš€ Processing Seed: {seed}\\n{'='*30}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    train_path = os.path.join(CFG.DATA_PATH, 'train_sample_10pct.parquet')\n",
    "    train_df = pd.read_parquet(train_path)\n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, 'test.parquet'))\n",
    "    except:\n",
    "        test_df = train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(100)]\n",
    "        \n",
    "    # 2. Feature Engineering\n",
    "    train_df = add_simple_features(train_df)\n",
    "    test_df = add_simple_features(test_df)\n",
    "    \n",
    "    # 3. Downsampling\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    n_neg = int(len(clicked_1) * CFG.NEG_RATIO)\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=n_neg, random_state=seed)\n",
    "    train_df = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    # 4. Preprocessing (GaussRank + LabelEnc)\n",
    "    for c in dense_cols:\n",
    "        train_df[c] = gauss_rank_transform(train_df[c].fillna(0))\n",
    "        test_df[c] = gauss_rank_transform(test_df[c].fillna(0))\n",
    "        \n",
    "    field_dims = []\n",
    "    for c in sparse_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Handle unknown\n",
    "        train_df[c] = train_df[c].astype(str)\n",
    "        test_df[c] = test_df[c].astype(str)\n",
    "        le.fit(pd.concat([train_df[c], test_df[c]]))\n",
    "        train_df[c] = le.transform(train_df[c])\n",
    "        test_df[c] = le.transform(test_df[c])\n",
    "        field_dims.append(len(le.classes_))\n",
    "        \n",
    "    # 5. Dataset & DataLoader\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df, train_df['clicked'], test_size=0.2, stratify=train_df['clicked'], random_state=seed\n",
    "    )\n",
    "    \n",
    "    train_ds = DeepFMDataset(X_train, sparse_cols, dense_cols, target='clicked')\n",
    "    val_ds = DeepFMDataset(X_val, sparse_cols, dense_cols, target='clicked')\n",
    "    train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CFG.batch_size*2, shuffle=False)\n",
    "    \n",
    "    # 6. Model Train\n",
    "    model = FiBiNET(field_dims, CFG.embedding_dim, len(dense_cols)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    print(\"ğŸ”¥ Training FiBiNet...\")\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for s, d, y in train_loader:\n",
    "            s, d, y = s.to(device), d.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(s, d)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "    # 7. Predict\n",
    "    model.eval()\n",
    "    test_ds = DeepFMDataset(test_df, sparse_cols, dense_cols)\n",
    "    test_loader = DataLoader(test_ds, batch_size=CFG.batch_size*2, shuffle=False)\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for s, d in test_loader:\n",
    "            s, d = s.to(device), d.to(device)\n",
    "            p = model(s, d).cpu().numpy()\n",
    "            preds.extend(p)\n",
    "            \n",
    "    out_file = f\"05_FiBiNet_{seed}.csv\"\n",
    "    if 'ID' in test_df.columns:\n",
    "        sub = pd.DataFrame({'ID': test_df['ID'], 'clicked': preds})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'ID': range(len(preds)), 'clicked': preds})\n",
    "        \n",
    "    sub.to_csv(out_file, index=False)\n",
    "    prediction_files.append(out_file)\n",
    "    print(f\"âœ… Saved: {out_file}\")\n",
    "    \n",
    "    del model, train_df\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ All seeds processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b4b2ee",
   "metadata": {},
   "source": [
    "## ë©”ì¸ í•™ìŠµ ë£¨í”„ (3ê°œ ì‹œë“œ X 3íšŒ ë°˜ë³µ)\n",
    "\n",
    "### GBDT vs ë”¥ëŸ¬ë‹ í•™ìŠµ í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "#### 1ï¸âƒ£ **ë°ì´í„° ë¡œë“œ ë° Down-Sampling**\n",
    "- CatBoost/XGBoostì™€ ë™ì¼\n",
    "- í´ë¦­(1) vs ë¹„í´ë¦­(0) = 1:5 ë¹„ìœ¨ (ë” í° ë¹„ìœ¨)\n",
    "- ì´ìœ : ì‹ ê²½ë§ì€ GBDTë³´ë‹¤ ë” ë§ì€ ë°ì´í„° í•„ìš”\n",
    "\n",
    "#### 2ï¸âƒ£ **ì „ì²˜ë¦¬ (GBDTì™€ ë‹¤ë¦„)**\n",
    "```python\n",
    "# GaussRank ì •ê·œí™”\n",
    "train[dense_cols] = gauss_rank_transform(train[dense_cols])\n",
    "test[dense_cols] = gauss_rank_transform(test[dense_cols])\n",
    "\n",
    "# LabelEncoding\n",
    "for col in sparse_cols:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "```\n",
    "\n",
    "#### 3ï¸âƒ£ **DataLoader ìƒì„±**\n",
    "```python\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "```\n",
    "- ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì œê³µ\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n",
    "\n",
    "#### 4ï¸âƒ£ **ëª¨ë¸ í•™ìŠµ**\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()  # ì´ì§„ ë¶„ë¥˜ ì†ì‹¤\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        y_pred = model(batch)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()      # ì—­ì „íŒŒ\n",
    "        optimizer.step()     # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "```\n",
    "\n",
    "#### 5ï¸âƒ£ **ì˜ˆì¸¡ ë° ì €ì¥**\n",
    "- ê²€ì¦ ë°ì´í„°: AP, LogLoss ê³„ì‚°\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„°: í™•ë¥  ì˜ˆì¸¡ â†’ `05_FiBiNet_{seed}_predictions.csv`\n",
    "\n",
    "### ë”¥ëŸ¬ë‹ì˜ ì¥ì \n",
    "- **ìë™ íŠ¹ì„± ìƒí˜¸ì‘ìš©**: Bilinear layer\n",
    "- **ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •**: SENetìœ¼ë¡œ ì¤‘ìš” í”¼ì²˜ ê°•ì¡°\n",
    "- **ë¹„ì„ í˜•ì„±**: ReLU í™œì„±í™”ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ\n",
    "\n",
    "### ë”¥ëŸ¬ë‹ì˜ ë‹¨ì \n",
    "- **í•™ìŠµ ì‹œê°„**: GBDTë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦¼\n",
    "- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ë” ë§ì€ íŠœë‹ í•„ìš”\n",
    "- **ë¶ˆì•ˆì •ì„±**: ì´ˆê¸°ê°’ì— ë”°ë¼ ê²°ê³¼ ë³€ë™ ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18668b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ Soft Voting: ['05_FiBiNet_42.csv', '05_FiBiNet_106.csv', '05_FiBiNet_1031.csv']\n",
      "ğŸ‰ Final FiBiNet Ensemble Saved: fibinet.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nğŸ¤ Soft Voting: {prediction_files}\")\n",
    "if prediction_files:\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={'clicked': 'click_0'})\n",
    "    for i, f in enumerate(prediction_files[1:], 1):\n",
    "        d = pd.read_csv(f).rename(columns={'clicked': f'click_{i}'})\n",
    "        merged = merged.merge(d, on='ID')\n",
    "        \n",
    "    cols = [c for c in merged.columns if c.startswith('click_')]\n",
    "    merged['clicked'] = merged[cols].mean(axis=1)\n",
    "    \n",
    "    merged[['ID', 'clicked']].to_csv('05_FiBiNet_SoftVoting.csv', index=False)\n",
    "    print(\"ğŸ‰ Final FiBiNet Ensemble Saved: fibinet.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688769f",
   "metadata": {},
   "source": [
    "## Soft Voting ì•™ìƒë¸” (FiBiNet 3ê°œ ëª¨ë¸ ê²°í•©)\n",
    "\n",
    "**í”„ë¡œì„¸ìŠ¤**:\n",
    "1. 3ê°œ FiBiNet ëª¨ë¸ì˜ ì˜ˆì¸¡ íŒŒì¼ ë¡œë“œ\n",
    "2. ID ê¸°ì¤€ ë³‘í•©\n",
    "3. ê°€ì¤‘ í‰ê·  ê³„ì‚°\n",
    "4. `05_FiBiNet_SoftVoting.csv` ì €ì¥\n",
    "\n",
    "**ì•™ìƒë¸”ì˜ ê°€ì¹˜**: ì‹ ê²½ë§ì˜ ë¶ˆì•ˆì •ì„±ì„ 3ê°œ ëª¨ë¸ í‰ê· ìœ¼ë¡œ ë³´ì™„\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f13b8b1",
   "metadata": {},
   "source": [
    "## ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ê¸°ë°˜ ì„±ëŠ¥ í‰ê°€\n",
    "\n",
    "FiBiNet Soft Voting ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì‹¤ì œ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. \n",
    "í•©ì„± í‚¤(composite key)ë¥¼ í™œìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¼ë²¨ì„ ì¶”ì •í•˜ê³  ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ccd7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Train data loaded: (1071335, 119)\n",
      "âœ“ Test data loaded: (1527298, 119)\n",
      "\n",
      "=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë§¤ì¹­ ê²°ê³¼ ===\n",
      "âœ“ ë§¤ì¹­ëœ ìƒ˜í”Œ: 1,527,291 / 1,527,298 (100.00%)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'predictions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predictions'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# FiBiNet Soft Voting ì˜ˆì¸¡ ë¡œë“œ\u001b[39;00m\n\u001b[1;32m     37\u001b[0m fibinet_pred_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m05_FiBiNet_SoftVoting.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 38\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfibinet_pred_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpredictions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# ë§¤ì¹­ëœ ë¼ë²¨ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê°€\u001b[39;00m\n\u001b[1;32m     41\u001b[0m test_eval \u001b[38;5;241m=\u001b[39m test_data[test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclicked_label\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predictions'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_path = \"/home/konkukstat/python3/workspace/data/train_sample_10pct.parquet\"\n",
    "test_path = \"/home/konkukstat/python3/workspace/data/test.parquet\"\n",
    "\n",
    "train_data = pd.read_parquet(train_path)\n",
    "test_data = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"âœ“ Train data loaded: {train_data.shape}\")\n",
    "print(f\"âœ“ Test data loaded: {test_data.shape}\")\n",
    "\n",
    "# í•©ì„± í‚¤ ìƒì„± (gender + age_group + inventory_id ê¸°ë°˜)\n",
    "train_data['composite_key'] = (train_data['gender'].astype(str) + '_' + \n",
    "                               train_data['age_group'].astype(str) + '_' + \n",
    "                               train_data['inventory_id'].astype(str))\n",
    "\n",
    "test_data['composite_key'] = (test_data['gender'].astype(str) + '_' + \n",
    "                              test_data['age_group'].astype(str) + '_' + \n",
    "                              test_data['inventory_id'].astype(str))\n",
    "\n",
    "# í•™ìŠµ ë°ì´í„°ì—ì„œ clicked ë¼ë²¨ ë§¤í•‘\n",
    "train_grouped = train_data.groupby('composite_key')['clicked'].mean()\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ë¼ë²¨ ì¶”ê°€\n",
    "test_data['clicked_label'] = test_data['composite_key'].map(train_grouped)\n",
    "\n",
    "# ë¼ë²¨ ë§¤ì¹­ ê²°ê³¼ í™•ì¸\n",
    "labeled_count = test_data['clicked_label'].notna().sum()\n",
    "total_count = len(test_data)\n",
    "matching_ratio = labeled_count / total_count * 100\n",
    "\n",
    "print(f\"\\n=== í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¼ë²¨ ë§¤ì¹­ ê²°ê³¼ ===\")\n",
    "print(f\"âœ“ ë§¤ì¹­ëœ ìƒ˜í”Œ: {labeled_count:,} / {total_count:,} ({matching_ratio:.2f}%)\")\n",
    "\n",
    "# FiBiNet Soft Voting ì˜ˆì¸¡ ë¡œë“œ\n",
    "fibinet_pred_path = \"05_FiBiNet_SoftVoting.csv\"\n",
    "y_pred = pd.read_csv(fibinet_pred_path)['clicked'].values\n",
    "\n",
    "# ë§¤ì¹­ëœ ë¼ë²¨ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê°€\n",
    "test_eval = test_data[test_data['clicked_label'].notna()].copy()\n",
    "y_true = test_eval['clicked_label'].values\n",
    "\n",
    "# ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜ (0.5 ê¸°ì¤€)\n",
    "y_true_binary = (y_true >= 0.5).astype(int)\n",
    "\n",
    "# í•´ë‹¹ ìƒ˜í”Œì˜ ì˜ˆì¸¡ê°’ë§Œ ì¶”ì¶œ\n",
    "y_pred_matched = y_pred[:len(y_true)]\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "\n",
    "# Average Precision ê³„ì‚°\n",
    "ap_fibinet = average_precision_score(y_true_binary, y_pred_matched)\n",
    "\n",
    "# Log Loss ê³„ì‚° (í´ë¦­ ì—¬ë¶€ë³„ë¡œ ë¶„ë¦¬)\n",
    "eps = 1e-15\n",
    "y_pred_clipped = np.clip(y_pred_matched, eps, 1 - eps)\n",
    "\n",
    "# í´ë¦­ (1) ìƒ˜í”Œì˜ Log Loss\n",
    "clicked_mask = y_true_binary == 1\n",
    "pos_logloss = -np.log(y_pred_clipped[clicked_mask]).mean() if clicked_mask.sum() > 0 else 0\n",
    "\n",
    "# ë¹„í´ë¦­ (0) ìƒ˜í”Œì˜ Log Loss\n",
    "not_clicked_mask = y_true_binary == 0\n",
    "neg_logloss = -np.log(1 - y_pred_clipped[not_clicked_mask]).mean() if not_clicked_mask.sum() > 0 else 0\n",
    "\n",
    "# ê°€ì¤‘ Log Loss (0.5:0.5)\n",
    "weighted_logloss = 0.5 * pos_logloss + 0.5 * neg_logloss\n",
    "\n",
    "# ìµœì¢… ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "final_score_fibinet = 0.5 * ap_fibinet + 0.5 * (1 / (1 + weighted_logloss))\n",
    "\n",
    "print(f\"\\n=== ğŸ“Š FiBiNet í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„±ëŠ¥ í‰ê°€ ===\")\n",
    "print(f\"Average Precision (AP): {ap_fibinet:.4f}\")\n",
    "print(f\"Positive Log Loss: {pos_logloss:.4f}\")\n",
    "print(f\"Negative Log Loss: {neg_logloss:.4f}\")\n",
    "print(f\"Weighted Log Loss: {weighted_logloss:.4f}\")\n",
    "print(f\"\\nâœ“ FiBiNet í…ŒìŠ¤íŠ¸ ìµœì¢… ì ìˆ˜: {final_score_fibinet:.4f}\")\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì™€ ë¹„êµ\n",
    "print(f\"\\n=== ê²€ì¦ vs í…ŒìŠ¤íŠ¸ ë¹„êµ ===\")\n",
    "print(f\"ê²€ì¦ ë°ì´í„° AP: {(0.5 * val_ap if 'val_ap' in locals() else 0):.4f}\")  # validation ì •ë³´ê°€ ìˆë‹¤ë©´ ë¹„êµ\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° AP: {ap_fibinet:.4f}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°ì´í„° Weighted Log Loss: {weighted_logloss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
