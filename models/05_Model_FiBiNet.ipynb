{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0128e5e",
   "metadata": {},
   "source": [
    "\n",
    "# 05. ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì ‘ê·¼: FiBiNet êµ¬í˜„\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ DeepCTR ê³„ì—´ì˜ **FiBiNet (Feature Importance and Bilinear Interaction Network)** ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "ê¸°ì¡´ íŠ¸ë¦¬ ê¸°ë°˜ ëª¨ë¸(GBDT)ì´ í¬ì°©í•˜ê¸° ì–´ë ¤ìš´ í”¼ì²˜ ê°„ì˜ ë¯¸ì„¸í•œ ìƒí˜¸ì‘ìš©ê³¼ ì¤‘ìš”ë„ë¥¼ ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ’¡ í•µì‹¬ ì•„í‚¤í…ì²˜\n",
    "1. **SENet (Squeeze-and-Excitation Network)**: í”¼ì²˜ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ê°€ì¤‘ì¹˜ ì¡°ì •\n",
    "2. **Bilinear Interaction**: í”¼ì²˜ ì„ë² ë”© ê°„ì˜ ì™¸ì (Outer Product)ì„ í†µí•œ 2ì°¨ ìƒí˜¸ì‘ìš© í¬ì°©\n",
    "3. **GaussRank Transformation**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ì •ê·œ ë¶„í¬í™”ë¥¼ í†µí•œ í•™ìŠµ ì•ˆì •ì„± í™•ë³´\n",
    "4. **Deep Network**: ê³ ì°¨ì› ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ MLP(Multi-Layer Perceptron) êµ¬ì¡°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd435606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "from scipy.stats import rankdata, norm\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Device ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”¥ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff41bf21",
   "metadata": {},
   "source": [
    "## í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "**ëª¨ë¸5: FiBiNet (ë”¥ëŸ¬ë‹ ê¸°ë°˜)**\n",
    "\n",
    "GBDT(ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…) ëª¨ë¸ ëŒ€ë¹„ ì™„ì „íˆ ë‹¤ë¥¸ ì ‘ê·¼:\n",
    "\n",
    "- **torch, torch.nn**: ì‹ ê²½ë§ êµ¬ì¶• ë° í•™ìŠµ\n",
    "- **Dataset, DataLoader**: ë°°ì¹˜ ë‹¨ìœ„ í•™ìŠµ ë°ì´í„° ê´€ë¦¬\n",
    "- **LabelEncoder**: ë²”ì£¼í˜• â†’ ì •ìˆ˜ ë³€í™˜\n",
    "- **rankdata, norm**: GaussRank ì •ê·œí™” (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ì •ê·œ ë¶„í¬í™”)\n",
    "\n",
    "**ì¥ì **:\n",
    "- ë¹„ì„ í˜• ìƒí˜¸ì‘ìš© ìë™ í•™ìŠµ\n",
    "- í”¼ì²˜ ì„ë² ë”©ìœ¼ë¡œ í”¼ì²˜ ê°„ ìœ ì‚¬ì„± í•™ìŠµ\n",
    "- ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "026b7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = '/home/konkukstat/python3/workspace/data'\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    batch_size = 1024\n",
    "    epochs = 1  # Optimized for faster demo # ë°ëª¨ìš©, ì‹¤ì œëŠ” ë” ë§ì´\n",
    "    learning_rate = 0.001\n",
    "    embedding_dim = 16\n",
    "    \n",
    "    # ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨\n",
    "    NEG_RATIO = 5 \n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5b6d9",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "\n",
    "**GBDT vs ë”¥ëŸ¬ë‹ ì„¤ì • ë¹„êµ**\n",
    "\n",
    "| í•­ëª© | GBDT (CatBoost/XGBoost) | FiBiNet (ë”¥ëŸ¬ë‹) |\n",
    "|------|---------|---------|\n",
    "| ì…ë ¥ | ì›ë³¸ ë°ì´í„° | ì •ê·œí™”ëœ ë°ì´í„° |\n",
    "| ë°°ì¹˜ í¬ê¸° | ì „ì²´ ë˜ëŠ” ìƒ˜í”Œ | 1,024 (ë©”ëª¨ë¦¬ íš¨ìœ¨) |\n",
    "| ì—í¬í¬ | 1-100 ë°˜ë³µ | ì—¬ëŸ¬ ì—í¬í¬ |\n",
    "| í•™ìŠµë¥  | 0.01-0.1 | 0.001 (ë³´ìˆ˜ì ) |\n",
    "| ì •ê·œí™” | íŠ¸ë¦¬ ê¸°ë°˜ | ë“œë¡­ì•„ì›ƒ (0.2) |\n",
    "\n",
    "**íŠ¹ì§•**:\n",
    "- `batch_size=1,024`: ë©”ëª¨ë¦¬/ì†ë„ ê· í˜•\n",
    "- `epochs=1`: ë°ëª¨ìš© (ì‹¤ì œ: 10-50)\n",
    "- `embedding_dim=16`: ê° ë²”ì£¼ë¥¼ 16ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„\n",
    "- `learning_rate=0.001`: ì‹ ê²½ë§ì€ ë‚®ì€ í•™ìŠµë¥  ê¶Œì¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb594716",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FiBiNET(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, dense_dim, hidden_units=[256, 128], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.num_fields = len(field_dims)\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(f, embed_dim) for f in field_dims])\n",
    "        self.se_fc1 = nn.Linear(self.num_fields, self.num_fields // 2)\n",
    "        self.se_fc2 = nn.Linear(self.num_fields // 2, self.num_fields)\n",
    "        \n",
    "        # Bilinear\n",
    "        num_pairs = self.num_fields * (self.num_fields - 1) // 2\n",
    "        self.W = nn.Parameter(torch.randn(num_pairs, embed_dim, embed_dim) * 0.02)\n",
    "        \n",
    "        input_dim = num_pairs * embed_dim + dense_dim\n",
    "        layers = []\n",
    "        for h in hidden_units:\n",
    "            layers += [nn.Linear(input_dim, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            input_dim = h\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.deep = nn.Sequential(*layers)\n",
    "        \n",
    "        self.pair_idx = []\n",
    "        for i in range(self.num_fields):\n",
    "            for j in range(i+1, self.num_fields):\n",
    "                self.pair_idx.append((i, j))\n",
    "                \n",
    "    def forward(self, x_sparse, x_dense):\n",
    "        # Embedding\n",
    "        emb = [e(x_sparse[:, i]) for i, e in enumerate(self.embeddings)]\n",
    "        emb = torch.stack(emb, dim=1) # [B, F, E]\n",
    "        \n",
    "        # SENet (Simple ver)\n",
    "        z = emb.mean(dim=2)\n",
    "        w = torch.relu(self.se_fc1(z))\n",
    "        w = torch.sigmoid(self.se_fc2(w)).unsqueeze(-1)\n",
    "        emb_se = emb * w\n",
    "        \n",
    "        # Bilinear Interaction\n",
    "        p = []\n",
    "        for k, (i, j) in enumerate(self.pair_idx):\n",
    "            v_i = emb_se[:, i]\n",
    "            v_j = emb_se[:, j]\n",
    "            # Simple element-wise\n",
    "            p.append(v_i * v_j)\n",
    "            \n",
    "        p = torch.cat(p, dim=1) # [B, Pairs*E]\n",
    "        \n",
    "        # Deep\n",
    "        x = torch.cat([p, x_dense], dim=1)\n",
    "        return torch.sigmoid(self.deep(x)).squeeze()\n",
    "\n",
    "class DeepFMDataset(Dataset):\n",
    "    def __init__(self, df, sparse_cols, dense_cols, target=None):\n",
    "        self.sparse = df[sparse_cols].values.astype(np.int64)\n",
    "        self.dense = df[dense_cols].values.astype(np.float32)\n",
    "        self.target = df[target].values.astype(np.float32) if target is not None else None\n",
    "        \n",
    "    def __len__(self): return len(self.sparse)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.target is not None:\n",
    "            return self.sparse[idx], self.dense[idx], self.target[idx]\n",
    "        return self.sparse[idx], self.dense[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052b5d1",
   "metadata": {},
   "source": [
    "## FiBiNet ì‹ ê²½ë§ ì•„í‚¤í…ì²˜\n",
    "\n",
    "**êµ¬ì¡°**: Embedding â†’ SENet â†’ Bilinear Interaction â†’ MLP â†’ ì´ì§„ ë¶„ë¥˜\n",
    "\n",
    "### 1ï¸âƒ£ **Embedding Layer**\n",
    "```\n",
    "ê° ë²”ì£¼í˜• ë³€ìˆ˜ â†’ ì„ë² ë”© ì°¨ì› (16D ë²¡í„°)\n",
    "ì˜ˆ: gender='male' â†’ [0.2, -0.1, ..., 0.5]  (16ê°œ ê°’)\n",
    "```\n",
    "- ëª©ì : ë²”ì£¼ ê°„ ìœ ì‚¬ì„± í•™ìŠµ\n",
    "\n",
    "### 2ï¸âƒ£ **SENet (Squeeze-and-Excitation Network)**\n",
    "```python\n",
    "z = emb.mean(dim=2)              # í”¼ì²˜ë³„ í‰ê· \n",
    "w = sigmoid(fc1(relu(fc1(z))))   # í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "emb_se = emb * w                  # ì¤‘ìš”í•œ í”¼ì²˜ì— ë†’ì€ ê°€ì¤‘ì¹˜\n",
    "```\n",
    "- ëª©ì : í”¼ì²˜ë³„ ì¤‘ìš”ë„ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •\n",
    "\n",
    "### 3ï¸âƒ£ **Bilinear Interaction (2ì°¨ ìƒí˜¸ì‘ìš©)**\n",
    "```python\n",
    "# ëª¨ë“  í”¼ì²˜ ìŒ ê°„ì˜ ì™¸ì  ê³„ì‚°\n",
    "v_i * v_j  # í”¼ì²˜ iì™€ jì˜ ìƒí˜¸ì‘ìš©\n",
    "```\n",
    "- ëª©ì : í”¼ì²˜ ê°„ ë¯¸ì„¸í•œ ê´€ê³„ í¬ì°©\n",
    "- ì˜ˆ: ì„±ë³„ Ã— ì—°ë ¹ëŒ€ì˜ ìƒí˜¸ì‘ìš©\n",
    "\n",
    "### 4ï¸âƒ£ **Deep Network (MLP)**\n",
    "```\n",
    "[ìƒí˜¸ì‘ìš© ë²¡í„°] â†’ Dense(256, ReLU) \n",
    "              â†’ Dropout(0.2)\n",
    "              â†’ Dense(128, ReLU)\n",
    "              â†’ Dropout(0.2)\n",
    "              â†’ Dense(1, Sigmoid) â†’ í™•ë¥  ì¶œë ¥\n",
    "```\n",
    "- ëª©ì : ê³ ì°¨ì› ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ\n",
    "\n",
    "### FiBiNetì˜ ê°•ì \n",
    "- **ìë™ íŠ¹ì„± ìƒí˜¸ì‘ìš©**: ìˆ˜ë™ìœ¼ë¡œ ë§Œë“¤ í•„ìš” ì—†ìŒ\n",
    "- **ë™ì  ê°€ì¤‘ì¹˜**: SENetìœ¼ë¡œ ì¤‘ìš” í”¼ì²˜ ê°•ì¡°\n",
    "- **ë¹„ì„ í˜• í•™ìŠµ**: ë³µì¡í•œ íŒ¨í„´ í¬ì°© ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d5476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gauss_rank_transform(series):\n",
    "    rank = rankdata(series)\n",
    "    rank = (rank - 0.5) / len(series)\n",
    "    rank = 2 * rank - 1\n",
    "    rank = np.clip(rank, -0.999, 0.999)\n",
    "    return norm.ppf((rank + 1) / 2)\n",
    "\n",
    "sparse_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "# Simple dense list for demo\n",
    "dense_cols = ['seq_len', 'avg_click_prob'] # In real scenario, use more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0132d4af",
   "metadata": {},
   "source": [
    "## ë°ì´í„°ì…‹ ë° ì „ì²˜ë¦¬\n",
    "\n",
    "### 1ï¸âƒ£ **GaussRank Transform**\n",
    "```python\n",
    "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë¥¼ ì •ê·œ ë¶„í¬ë¡œ ë³€í™˜\n",
    "rank = rankdata(series)                    # ìˆœìœ„ ë³€í™˜\n",
    "rank = (rank - 0.5) / len(series)         # [0,1] ì •ê·œí™”\n",
    "rank = norm.ppf((rank + 1) / 2)           # ì—­ CDF ì ìš© â†’ ì •ê·œë¶„í¬\n",
    "```\n",
    "- ëª©ì : ì‹ ê²½ë§ í•™ìŠµ ì•ˆì •í™”\n",
    "- ì´ìœ : ì‹ ê²½ë§ì€ ì •ê·œí™”ëœ ì…ë ¥ì„ ì„ í˜¸\n",
    "\n",
    "### 2ï¸âƒ£ **ë²”ì£¼í˜• ì¸ì½”ë”©**\n",
    "```python\n",
    "# LabelEncoder: ë²”ì£¼ â†’ ì •ìˆ˜\n",
    "gender: {'male': 0, 'female': 1, 'other': 2}\n",
    "```\n",
    "\n",
    "### 3ï¸âƒ£ **Dataset í´ë˜ìŠ¤**\n",
    "```python\n",
    "class DeepFMDataset(Dataset):\n",
    "    def __getitem__(idx):\n",
    "        return sparse[idx], dense[idx], target[idx]\n",
    "```\n",
    "- ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì œê³µ\n",
    "- PyTorch DataLoaderì™€ í˜¸í™˜\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a86d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Click Map Load\n",
    "try:\n",
    "    df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    click_prob_map = dict(zip(df_click_prob['number'], df_click_prob['click_prob']))\n",
    "except:\n",
    "    click_prob_map = {}\n",
    "\n",
    "def add_simple_features(df):\n",
    "    seq_len, avg_prob = [], []\n",
    "    for s in df[\"seq\"]:\n",
    "        if isinstance(s, str) and s != \"\":\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "            seq_len.append(len(arr))\n",
    "            probs = [click_prob_map.get(num, 0) for num in arr]\n",
    "            avg_prob.append(sum(probs) / len(probs) if probs else 0)\n",
    "        else:\n",
    "            seq_len.append(0)\n",
    "            avg_prob.append(0)\n",
    "    df['seq_len'] = seq_len\n",
    "    df['avg_click_prob'] = avg_prob\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebf20e",
   "metadata": {},
   "source": [
    "## í´ë¦­ í™•ë¥  ë§µ ë¡œë“œ ë° ê°„ë‹¨ í”¼ì²˜ ìƒì„±\n",
    "\n",
    "**GBDT vs ë”¥ëŸ¬ë‹ì˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì°¨ì´**\n",
    "\n",
    "| ë°©ì‹ | GBDT | ë”¥ëŸ¬ë‹ |\n",
    "|------|------|--------|\n",
    "| í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ | ìˆ˜ë™, ìƒì„¸ | ìµœì†Œí™” (ì‹ ê²½ë§ì´ ìë™ í•™ìŠµ) |\n",
    "| ìƒí˜¸ì‘ìš© | ìˆ˜ë™ ìƒì„± | ìë™ í•™ìŠµ |\n",
    "| ì •ê·œí™” | ì„ íƒ | í•„ìˆ˜ |\n",
    "\n",
    "**ê°„ë‹¨í•œ í”¼ì²˜ ìƒì„±**:\n",
    "- `seq_len`: ì‹œí€€ìŠ¤ ê¸¸ì´\n",
    "- `avg_click_prob`: í‰ê·  í´ë¦­ í™•ë¥ \n",
    "\n",
    "ì‹ ê²½ë§ì€ ë³µì¡í•œ íŒŒìƒ ë³€ìˆ˜ë³´ë‹¤ **ì •ê·œí™”ëœ ì›ë³¸ ë°ì´í„°**ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7eaded2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 42\n",
      "==============================\n",
      "ğŸ”¥ Training FiBiNet...\n",
      "Epoch 1 Loss: 0.4658\n",
      "âœ… Saved: 05_FiBiNet_42.csv\n",
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 106\n",
      "==============================\n",
      "ğŸ”¥ Training FiBiNet...\n",
      "Epoch 1 Loss: 0.4592\n",
      "âœ… Saved: 05_FiBiNet_106.csv\n",
      "\n",
      "==============================\n",
      "ğŸš€ Processing Seed: 1031\n",
      "==============================\n",
      "ğŸ”¥ Training FiBiNet...\n",
      "Epoch 1 Loss: 0.4708\n",
      "âœ… Saved: 05_FiBiNet_1031.csv\n",
      "\n",
      "ğŸ All seeds processed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction_files = []\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*30}\\nğŸš€ Processing Seed: {seed}\\n{'='*30}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # 1. Load Data\n",
    "    train_path = os.path.join(CFG.DATA_PATH, 'train_sample_10pct.parquet')\n",
    "    train_df = pd.read_parquet(train_path)\n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, 'test.parquet'))\n",
    "    except:\n",
    "        test_df = train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(100)]\n",
    "        \n",
    "    # 2. Feature Engineering\n",
    "    train_df = add_simple_features(train_df)\n",
    "    test_df = add_simple_features(test_df)\n",
    "    \n",
    "    # 3. Downsampling\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    n_neg = int(len(clicked_1) * CFG.NEG_RATIO)\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=n_neg, random_state=seed)\n",
    "    train_df = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    # 4. Preprocessing (GaussRank + LabelEnc)\n",
    "    for c in dense_cols:\n",
    "        train_df[c] = gauss_rank_transform(train_df[c].fillna(0))\n",
    "        test_df[c] = gauss_rank_transform(test_df[c].fillna(0))\n",
    "        \n",
    "    field_dims = []\n",
    "    for c in sparse_cols:\n",
    "        le = LabelEncoder()\n",
    "        # Handle unknown\n",
    "        train_df[c] = train_df[c].astype(str)\n",
    "        test_df[c] = test_df[c].astype(str)\n",
    "        le.fit(pd.concat([train_df[c], test_df[c]]))\n",
    "        train_df[c] = le.transform(train_df[c])\n",
    "        test_df[c] = le.transform(test_df[c])\n",
    "        field_dims.append(len(le.classes_))\n",
    "        \n",
    "    # 5. Dataset & DataLoader\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df, train_df['clicked'], test_size=0.2, stratify=train_df['clicked'], random_state=seed\n",
    "    )\n",
    "    \n",
    "    train_ds = DeepFMDataset(X_train, sparse_cols, dense_cols, target='clicked')\n",
    "    val_ds = DeepFMDataset(X_val, sparse_cols, dense_cols, target='clicked')\n",
    "    train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=CFG.batch_size*2, shuffle=False)\n",
    "    \n",
    "    # 6. Model Train\n",
    "    model = FiBiNET(field_dims, CFG.embedding_dim, len(dense_cols)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG.learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    print(\"ğŸ”¥ Training FiBiNet...\")\n",
    "    for epoch in range(CFG.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for s, d, y in train_loader:\n",
    "            s, d, y = s.to(device), d.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(s, d)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        \n",
    "    # 7. Predict\n",
    "    model.eval()\n",
    "    test_ds = DeepFMDataset(test_df, sparse_cols, dense_cols)\n",
    "    test_loader = DataLoader(test_ds, batch_size=CFG.batch_size*2, shuffle=False)\n",
    "    \n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for s, d in test_loader:\n",
    "            s, d = s.to(device), d.to(device)\n",
    "            p = model(s, d).cpu().numpy()\n",
    "            preds.extend(p)\n",
    "            \n",
    "    out_file = f\"05_FiBiNet_{seed}.csv\"\n",
    "    if 'ID' in test_df.columns:\n",
    "        sub = pd.DataFrame({'ID': test_df['ID'], 'clicked': preds})\n",
    "    else:\n",
    "        sub = pd.DataFrame({'ID': range(len(preds)), 'clicked': preds})\n",
    "        \n",
    "    sub.to_csv(out_file, index=False)\n",
    "    prediction_files.append(out_file)\n",
    "    print(f\"âœ… Saved: {out_file}\")\n",
    "    \n",
    "    del model, train_df\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nğŸ All seeds processed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b4b2ee",
   "metadata": {},
   "source": [
    "## ë©”ì¸ í•™ìŠµ ë£¨í”„ (3ê°œ ì‹œë“œ X 3íšŒ ë°˜ë³µ)\n",
    "\n",
    "### GBDT vs ë”¥ëŸ¬ë‹ í•™ìŠµ í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "#### 1ï¸âƒ£ **ë°ì´í„° ë¡œë“œ ë° Down-Sampling**\n",
    "- CatBoost/XGBoostì™€ ë™ì¼\n",
    "- í´ë¦­(1) vs ë¹„í´ë¦­(0) = 1:5 ë¹„ìœ¨ (ë” í° ë¹„ìœ¨)\n",
    "- ì´ìœ : ì‹ ê²½ë§ì€ GBDTë³´ë‹¤ ë” ë§ì€ ë°ì´í„° í•„ìš”\n",
    "\n",
    "#### 2ï¸âƒ£ **ì „ì²˜ë¦¬ (GBDTì™€ ë‹¤ë¦„)**\n",
    "```python\n",
    "# GaussRank ì •ê·œí™”\n",
    "train[dense_cols] = gauss_rank_transform(train[dense_cols])\n",
    "test[dense_cols] = gauss_rank_transform(test[dense_cols])\n",
    "\n",
    "# LabelEncoding\n",
    "for col in sparse_cols:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col].astype(str))\n",
    "    test[col] = le.transform(test[col].astype(str))\n",
    "```\n",
    "\n",
    "#### 3ï¸âƒ£ **DataLoader ìƒì„±**\n",
    "```python\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "```\n",
    "- ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì œê³µ\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n",
    "\n",
    "#### 4ï¸âƒ£ **ëª¨ë¸ í•™ìŠµ**\n",
    "```python\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()  # ì´ì§„ ë¶„ë¥˜ ì†ì‹¤\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        y_pred = model(batch)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()      # ì—­ì „íŒŒ\n",
    "        optimizer.step()     # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "```\n",
    "\n",
    "#### 5ï¸âƒ£ **ì˜ˆì¸¡ ë° ì €ì¥**\n",
    "- ê²€ì¦ ë°ì´í„°: AP, LogLoss ê³„ì‚°\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„°: í™•ë¥  ì˜ˆì¸¡ â†’ `05_FiBiNet_{seed}_predictions.csv`\n",
    "\n",
    "### ë”¥ëŸ¬ë‹ì˜ ì¥ì \n",
    "- **ìë™ íŠ¹ì„± ìƒí˜¸ì‘ìš©**: Bilinear layer\n",
    "- **ë™ì  ê°€ì¤‘ì¹˜ ì¡°ì •**: SENetìœ¼ë¡œ ì¤‘ìš” í”¼ì²˜ ê°•ì¡°\n",
    "- **ë¹„ì„ í˜•ì„±**: ReLU í™œì„±í™”ë¡œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ\n",
    "\n",
    "### ë”¥ëŸ¬ë‹ì˜ ë‹¨ì \n",
    "- **í•™ìŠµ ì‹œê°„**: GBDTë³´ë‹¤ ì˜¤ë˜ ê±¸ë¦¼\n",
    "- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: ë” ë§ì€ íŠœë‹ í•„ìš”\n",
    "- **ë¶ˆì•ˆì •ì„±**: ì´ˆê¸°ê°’ì— ë”°ë¼ ê²°ê³¼ ë³€ë™ ê°€ëŠ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18668b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ Soft Voting: ['05_FiBiNet_42.csv', '05_FiBiNet_106.csv', '05_FiBiNet_1031.csv']\n",
      "ğŸ‰ Final FiBiNet Ensemble Saved: fibinet.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nğŸ¤ Soft Voting: {prediction_files}\")\n",
    "if prediction_files:\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={'clicked': 'click_0'})\n",
    "    for i, f in enumerate(prediction_files[1:], 1):\n",
    "        d = pd.read_csv(f).rename(columns={'clicked': f'click_{i}'})\n",
    "        merged = merged.merge(d, on='ID')\n",
    "        \n",
    "    cols = [c for c in merged.columns if c.startswith('click_')]\n",
    "    merged['clicked'] = merged[cols].mean(axis=1)\n",
    "    \n",
    "    merged[['ID', 'clicked']].to_csv('05_FiBiNet_SoftVoting.csv', index=False)\n",
    "    print(\"ğŸ‰ Final FiBiNet Ensemble Saved: fibinet.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8688769f",
   "metadata": {},
   "source": [
    "## Soft Voting ì•™ìƒë¸” (FiBiNet 3ê°œ ëª¨ë¸ ê²°í•©)\n",
    "\n",
    "**í”„ë¡œì„¸ìŠ¤**:\n",
    "1. 3ê°œ FiBiNet ëª¨ë¸ì˜ ì˜ˆì¸¡ íŒŒì¼ ë¡œë“œ\n",
    "2. ID ê¸°ì¤€ ë³‘í•©\n",
    "3. ê°€ì¤‘ í‰ê·  ê³„ì‚°\n",
    "4. `05_FiBiNet_SoftVoting.csv` ì €ì¥\n",
    "\n",
    "**ì•™ìƒë¸”ì˜ ê°€ì¹˜**: ì‹ ê²½ë§ì˜ ë¶ˆì•ˆì •ì„±ì„ 3ê°œ ëª¨ë¸ í‰ê· ìœ¼ë¡œ ë³´ì™„\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
