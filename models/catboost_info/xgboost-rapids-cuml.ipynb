{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ba6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============================================================================\n",
    "# XGBoost GPU íŒŒì´í”„ë¼ì¸ (cudf + cuml + xgboost GPU)\n",
    "#=============================================================================\n",
    "# ìš©ë„: train_sample_10pct.parquet (ì•½ 940MB) ì „ìš©\n",
    "# ì…€ êµ¬ë¶„: #%% ì£¼ì„ìœ¼ë¡œ ë…¸íŠ¸ë¶ ì…€ ë¶„ë¦¬\n",
    "# =============================================================================\n",
    "#\n",
    "# âš ï¸ TODO: ê³¼ì í•© í•´ê²° (Train ROC-AUC 0.89 vs Valid 0.73)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "#\n",
    "# [1] CFG.XGB_PARAMSì— ì •ê·œí™” ê°•í™” (Ridge/Lasso)\n",
    "#     CFG.XGB_PARAMS['reg_lambda'] = 5.0    # Ridge (L2), ê¸°ë³¸=1 â†’ 5~10 ì‹œë„\n",
    "#     CFG.XGB_PARAMS['reg_alpha'] = 1.0     # Lasso (L1), ê¸°ë³¸=0 â†’ 0.5~2 ì‹œë„\n",
    "#\n",
    "# [2] CFG.XGB_PARAMSì— scale_pos_weight ì¶”ê°€ (í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •)\n",
    "#     # ë‹¤ìš´ìƒ˜í”Œë§ í›„ 1:1ì´ë©´ ë¶ˆí•„ìš”í•˜ì§€ë§Œ, ì›ë³¸ ë¹„ìœ¨ ë°˜ì˜ ì‹œ:\n",
    "#     CFG.XGB_PARAMS['scale_pos_weight'] = len(train_df[train_df['clicked']==0]) / len(train_df[train_df['clicked']==1])\n",
    "#\n",
    "# [3] CFG.XGB_PARAMS íŠ¸ë¦¬ ë³µì¡ë„ ì¤„ì´ê¸°\n",
    "#     CFG.XGB_PARAMS['max_depth'] = 6       # í˜„ì¬ 8 â†’ 5~6ìœ¼ë¡œ ì¤„ì´ê¸°\n",
    "#     CFG.XGB_PARAMS['min_child_weight'] = 5 # ê¸°ë³¸=1 â†’ 5~10 (leaf ìµœì†Œ ìƒ˜í”Œ)\n",
    "#     CFG.XGB_PARAMS['gamma'] = 0.3         # ê¸°ë³¸=0 â†’ 0.1~0.5 (split ìµœì†Œ gain)\n",
    "#\n",
    "# [4] í”¼ì²˜ ìˆ˜ ì¤„ì´ê¸° (í˜„ì¬ 1437ê°œ â†’ OHE ë•Œë¬¸ì— í¼)\n",
    "#     # interaction í”¼ì²˜ ì œê±° í…ŒìŠ¤íŠ¸:\n",
    "#     # add_interaction_features_gpu() í˜¸ì¶œ ì£¼ì„ ì²˜ë¦¬ í›„ ì¬ì‹¤í–‰\n",
    "#     # ë˜ëŠ” groupby í”¼ì²˜ë§Œìœ¼ë¡œ í…ŒìŠ¤íŠ¸\n",
    "#\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2789f5e8",
   "metadata": {},
   "source": [
    "[PART 1] ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "132059b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… [Fix] Successfully loaded: /home/konkukstat/.local/lib/python3.10/site-packages/nvidia/cuda_nvrtc/lib/libnvrtc.so.12\n",
      "âœ… cuDF 25.12.00, cuML 25.12.00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ctypes\n",
    "\n",
    "# Fix for OSError: libnvrtc.so.12: cannot open shared object file\n",
    "# Portable solution: Load the library dynamically using ctypes\n",
    "try:\n",
    "    import nvidia.cuda_nvrtc\n",
    "    # Handle namespace packages where __file__ might be None\n",
    "    nvrtc_paths = nvidia.cuda_nvrtc.__path__\n",
    "    nvrtc_lib = None\n",
    "    \n",
    "    for path in nvrtc_paths:\n",
    "        candidate = os.path.join(path, 'lib', 'libnvrtc.so.12')\n",
    "        if os.path.exists(candidate):\n",
    "            nvrtc_lib = candidate\n",
    "            break\n",
    "            \n",
    "    if nvrtc_lib:\n",
    "        # RTLD_GLOBAL makes symbols available to subsequently loaded libraries (like cupy)\n",
    "        ctypes.CDLL(nvrtc_lib, mode=ctypes.RTLD_GLOBAL)\n",
    "        print(f\"âœ… [Fix] Successfully loaded: {nvrtc_lib}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ [Fix] Library file libnvrtc.so.12 not found in: {nvrtc_paths}\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ [Fix] 'nvidia.cuda_nvrtc' module not found. Please install it via pip or conda.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ [Fix] Failed to load libnvrtc.so.12: {e}\")\n",
    "\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import gc\n",
    "import datetime\n",
    "\n",
    "from cuml.model_selection import train_test_split as gpu_train_test_split\n",
    "from cuml.preprocessing import OneHotEncoder as cumlOneHotEncoder\n",
    "from cupyx.scipy.sparse import csr_matrix as cp_csr_matrix, hstack as cp_hstack\n",
    "from scipy.sparse import csr_matrix as sp_csr_matrix\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_recall_curve,\n",
    "    auc,\n",
    "    log_loss,\n",
    "    accuracy_score\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# seq ì„ë² ë”© ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append('/home/konkukstat/python3/HB/KSH/gpu_code')\n",
    "sys.path.append('/home/konkukstat/python3/HB/KSH')\n",
    "\n",
    "print(f\"âœ… cuDF {cudf.__version__}, cuML {cuml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72385aad",
   "metadata": {},
   "source": [
    "[PART 2] ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DATA_PATH = '/home/konkukstat/python3/workspace/data'\n",
    "    TRAIN_FILE = 'train_sample_10pct.parquet'\n",
    "    TEST_FILE = 'test.parquet'\n",
    "    NOW = datetime.datetime.now().strftime('%Y%m%d_%H%M') \n",
    "    OUTPUT_PREFIX = f'XGB_GPU_{NOW}'\n",
    "    OUTPUT_DIR = os.path.join('results', OUTPUT_PREFIX)\n",
    "\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    NEG_RATIO = 1\n",
    "    VALID_RATIO = 0.2\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'aucpr',\n",
    "        'learning_rate': 0.08,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'n_estimators': 500,\n",
    "        'early_stopping_rounds': 30,\n",
    "        'tree_method': 'hist',\n",
    "        'min_child_weight': 3,\n",
    "        'reg_lambda': 5,\n",
    "        'device': 'cuda',\n",
    "    }\n",
    "\n",
    "    # Feature Engineering Flags (True: ì‚¬ìš© / False: ë¯¸ì‚¬ìš©)\n",
    "    USE_SEQ_FEATURES = True\n",
    "    USE_INTERACTION_FEATURES = True\n",
    "    USE_GROUPBY_FEATURES = True\n",
    "    USE_COUNT_FEATURES = True\n",
    "\n",
    "    DEFAULT_AGG_DICT = {\n",
    "        'history_a_1': ['mean', 'std'], 'history_a_2': ['mean', 'std'],\n",
    "        'history_a_3': ['mean', 'std'], 'history_a_6': ['mean', 'std'],\n",
    "        'feat_e_2': ['mean', 'std'], 'feat_e_3': ['mean', 'std'],\n",
    "        'feat_c_8': ['mean', 'std'], 'feat_e_9': ['mean', 'std'],\n",
    "        'feat_d_4': ['mean', 'std'],\n",
    "        'l_feat_1': ['mean', 'std'], 'l_feat_2': ['mean', 'std'],\n",
    "        'l_feat_5': ['mean', 'std'], 'l_feat_7': ['mean', 'std'],\n",
    "        'l_feat_10': ['mean', 'std'], 'l_feat_15': ['mean', 'std'],\n",
    "    }\n",
    "\n",
    "    # ë²”ì£¼í˜• ë³€ìˆ˜ ëª©ë¡ (unique.db ê¸°ë°˜)\n",
    "    CATEGORICAL_COLS = [\n",
    "        'gender', 'age_group', 'inventory_id', 'day_of_week', 'hour',\n",
    "        'l_feat_1', 'l_feat_2', 'l_feat_3', 'l_feat_4', 'l_feat_8',\n",
    "        'l_feat_13', 'l_feat_16', 'l_feat_18', 'l_feat_19', 'l_feat_20',\n",
    "        'l_feat_21', 'l_feat_22', 'l_feat_23', 'l_feat_24', 'l_feat_26',\n",
    "        'l_feat_27', 'feat_e_4', 'feat_d_1', 'feat_d_2', 'feat_d_3',\n",
    "        'feat_d_5', 'feat_d_6', 'feat_a_1', 'feat_a_2', 'feat_a_3',\n",
    "        'feat_a_4', 'feat_a_6', 'feat_a_7', 'feat_a_8', 'feat_a_9',\n",
    "        'feat_a_10', 'feat_a_11', 'feat_a_12', 'feat_a_13', 'feat_a_15',\n",
    "        'feat_a_16', 'feat_a_17', 'feat_a_18'\n",
    "    ]\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def save_experiment_config(cfg):\n",
    "    \"\"\"ì„¤ì •ê°’(íŒŒë¼ë¯¸í„°, í”Œë˜ê·¸ ë“±)ì„ JSON íŒŒì¼ë¡œ ì €ì¥\"\"\"\n",
    "    import json\n",
    "    config_data = {}\n",
    "    \n",
    "    # XGB_PARAMS ì €ì¥\n",
    "    if hasattr(cfg, 'XGB_PARAMS'):\n",
    "        config_data.update(cfg.XGB_PARAMS)\n",
    "        \n",
    "    # Feature Flags ë° ê¸°íƒ€ ì„¤ì • ì €ì¥\n",
    "    for attr in dir(cfg):\n",
    "        if attr.startswith('USE_') or attr in ['VALID_RATIO', 'NEG_RATIO', 'SEEDS', 'OUTPUT_PREFIX']:\n",
    "            val = getattr(cfg, attr)\n",
    "            # serialize numpy types\n",
    "            if hasattr(val, 'item'): val = val.item()\n",
    "            config_data[attr] = val\n",
    "            \n",
    "    def default(o):\n",
    "        if hasattr(o, 'item'): return o.item()\n",
    "        return str(o)\n",
    "    \n",
    "    filename = os.path.join(cfg.OUTPUT_DIR, f\"{cfg.OUTPUT_PREFIX}_config.json\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_data, f, indent=4, ensure_ascii=False, default=default)\n",
    "        print(f\"ğŸ“„ Configuration saved: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to save configuration: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88130a11",
   "metadata": {},
   "source": [
    "[PART 3] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6899f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    CLICK_PROB_MAP = dict(zip(_df_click_prob['number'], _df_click_prob['click_prob']))\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ 'high_click_numbers.xlsx' not found.\")\n",
    "    CLICK_PROB_MAP = {}\n",
    "\n",
    "POS_LIST = frozenset({370, 528, 68, 561, 144, 227, 417, 442, 186, 395})\n",
    "NEG_LIST = frozenset({154, 222, 84, 498, 434, 511, 216, 497, 309, 446})\n",
    "\n",
    "\n",
    "def add_seq_features(df) -> None:\n",
    "    \"\"\"seq ì»¬ëŸ¼ì—ì„œ íŒŒìƒ í”¼ì²˜ 4ê°œ ìƒì„± (cudf/pandas í˜¸í™˜).\"\"\"\n",
    "    is_cudf = hasattr(df, 'to_pandas')\n",
    "    seq_series = df['seq'].to_pandas() if is_cudf else df['seq']\n",
    "\n",
    "    n = len(seq_series)\n",
    "    seq_len = np.zeros(n, dtype=np.int32)\n",
    "    avg_prob = np.zeros(n, dtype=np.float32)\n",
    "    seq_neg = np.zeros(n, dtype=np.int32)\n",
    "    seq_pos = np.zeros(n, dtype=np.int32)\n",
    "\n",
    "    for i, s in enumerate(seq_series):\n",
    "        if isinstance(s, str) and s:\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "            seq_len[i] = len(arr)\n",
    "            if CLICK_PROB_MAP:\n",
    "                probs = [CLICK_PROB_MAP.get(num, 0.0) for num in arr]\n",
    "                avg_prob[i] = np.mean(probs) if probs else 0.0\n",
    "            seq_neg[i] = sum(1 for x in arr if x in NEG_LIST)\n",
    "            seq_pos[i] = sum(1 for x in arr if x in POS_LIST)\n",
    "\n",
    "    if is_cudf:\n",
    "        df['seq_len'] = cudf.Series(seq_len)\n",
    "        df['avg_click_prob'] = cudf.Series(avg_prob)\n",
    "        df['seq_neglogcount'] = cudf.Series(seq_neg)\n",
    "        df['seq_poslogcount'] = cudf.Series(seq_pos)\n",
    "    else:\n",
    "        df['seq_len'] = seq_len\n",
    "        df['avg_click_prob'] = avg_prob\n",
    "        df['seq_neglogcount'] = seq_neg\n",
    "        df['seq_poslogcount'] = seq_pos\n",
    "\n",
    "\n",
    "def add_interaction_features_gpu(df, cols=None):\n",
    "    \"\"\"cudf DataFrameì— interaction í”¼ì²˜ ì¶”ê°€\"\"\"\n",
    "    if cols is None:\n",
    "        cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    for col1, col2 in combinations(cols, 2):\n",
    "        new_col = f'{col1}_{col2}'\n",
    "        df[new_col] = df[col1].astype(str).str.cat(df[col2].astype(str), sep='_')\n",
    "\n",
    "\n",
    "def add_groupby_features_gpu(train_df, val_df, test_df, feature_name, agg_dict=None):\n",
    "    \"\"\"cudf groupby í”¼ì²˜ (trainì—ì„œë§Œ í†µê³„ ê³„ì‚°). ë¹„ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì€ ìë™ ìŠ¤í‚µ.\"\"\"\n",
    "    if agg_dict is None:\n",
    "        agg_dict = CFG.DEFAULT_AGG_DICT\n",
    "\n",
    "    if feature_name not in train_df.columns:\n",
    "        return\n",
    "\n",
    "    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ + ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ í•„í„°ë§\n",
    "    filtered_agg = {}\n",
    "    for k, v in agg_dict.items():\n",
    "        if k in train_df.columns:\n",
    "            # cudfëŠ” dtype.kindë¡œ ì²´í¬\n",
    "            if train_df[k].dtype.kind in ['i', 'f', 'u']:  # int, float, unsigned\n",
    "                filtered_agg[k] = v\n",
    "    \n",
    "    if not filtered_agg:\n",
    "        return\n",
    "\n",
    "    agg_stats = train_df.groupby(feature_name).agg(filtered_agg)\n",
    "    new_cols = [f\"{feature_name}_{col[0]}_{col[1]}\" for col in agg_stats.columns]\n",
    "    agg_stats.columns = new_cols\n",
    "    agg_stats = agg_stats.reset_index()\n",
    "\n",
    "    for target_df in [train_df, val_df, test_df]:\n",
    "        merged = target_df.merge(agg_stats, on=feature_name, how='left')\n",
    "        for col in new_cols:\n",
    "            target_df[col] = merged[col]\n",
    "\n",
    "    for col in new_cols:\n",
    "        fill_val = float(train_df[col].mean())\n",
    "        val_df[col] = val_df[col].fillna(fill_val)\n",
    "        test_df[col] = test_df[col].fillna(fill_val)\n",
    "\n",
    "\n",
    "def add_count_features_gpu(train_df, val_df, test_df, count_cols):\n",
    "    \"\"\"cudf Count Encoding: trainì—ì„œë§Œ ë¹ˆë„ ê³„ì‚°\"\"\"\n",
    "    for col in count_cols:\n",
    "        if col not in train_df.columns:\n",
    "            continue\n",
    "        count_map = train_df[col].value_counts()\n",
    "        train_df[f\"{col}_count\"] = train_df[col].map(count_map).fillna(0).astype('int32')\n",
    "        val_df[f\"{col}_count\"] = val_df[col].map(count_map).fillna(0).astype('int32')\n",
    "        test_df[f\"{col}_count\"] = test_df[col].map(count_map).fillna(0).astype('int32')\n",
    "\n",
    "\n",
    "def gpu_hstack(matrices):\n",
    "    \"\"\"\n",
    "    GPUìš© ìë™ hstack í•¨ìˆ˜.\n",
    "    cuDF DataFrame, CuPy array, cupyx sparse ë“± ë¬´ì—‡ì´ë“  í•©ì³ì¤Œ.\n",
    "    \"\"\"\n",
    "    converted_list = []\n",
    "    for mat in matrices:\n",
    "        if hasattr(mat, 'tocsr'):\n",
    "            converted_list.append(mat.tocsr())\n",
    "        elif hasattr(mat, 'to_pandas'):  # cudf DataFrame\n",
    "            arr = mat.values\n",
    "            if hasattr(arr, '__cuda_array_interface__'):\n",
    "                arr = cp.asarray(arr)\n",
    "            if arr.ndim == 1:\n",
    "                arr = arr.reshape(-1, 1)\n",
    "            converted_list.append(cp_csr_matrix(arr))\n",
    "        elif isinstance(mat, cp.ndarray):\n",
    "            if mat.ndim == 1:\n",
    "                mat = mat.reshape(-1, 1)\n",
    "            converted_list.append(cp_csr_matrix(mat))\n",
    "        elif isinstance(mat, np.ndarray):\n",
    "            if mat.ndim == 1:\n",
    "                mat = mat.reshape(-1, 1)\n",
    "            converted_list.append(cp_csr_matrix(cp.asarray(mat)))\n",
    "        else:\n",
    "            converted_list.append(cp_csr_matrix(mat))\n",
    "    return cp_hstack(converted_list).tocsr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565809c",
   "metadata": {},
   "source": [
    "[PART 4] í‰ê°€ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7defba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_toss_score(y_true, y_pred):\n",
    "    \"\"\"Toss ëŒ€íšŒ ìŠ¤ì½”ì–´ (CPU numpy ê¸°ë°˜).\"\"\"\n",
    "    if hasattr(y_true, 'get'):\n",
    "        y_true = y_true.get()\n",
    "    if hasattr(y_pred, 'get'):\n",
    "        y_pred = y_pred.get()\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "\n",
    "    y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    N_1 = np.sum(y_true == 1)\n",
    "    N_0 = np.sum(y_true == 0)\n",
    "    if N_1 == 0 or N_0 == 0:\n",
    "        return 0.5, 0.0, 0.0\n",
    "\n",
    "    weights = np.where(y_true == 1, 0.5 / N_1, 0.5 / N_0)\n",
    "    wll = log_loss(y_true, y_pred_clipped, sample_weight=weights)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "    return score, ap, wll\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, threshold=0.5):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ â†’ DataFrame ë°˜í™˜ (GPUâ†’CPU ë³€í™˜ í¬í•¨).\"\"\"\n",
    "    results = {}\n",
    "    for name, X, y_true in [('Train', X_train, y_train), ('Valid', X_val, y_val)]:\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        y_pred_class = (y_prob >= threshold).astype(int)\n",
    "        y_true_np = np.asarray(y_true)\n",
    "        y_prob_np = np.asarray(y_prob)\n",
    "\n",
    "        roc = roc_auc_score(y_true_np, y_prob_np)\n",
    "        prec_curve, rec_curve, _ = precision_recall_curve(y_true_np, y_prob_np)\n",
    "        pr_auc = auc(rec_curve, prec_curve)\n",
    "        avg_prec = average_precision_score(y_true_np, y_prob_np)\n",
    "        ll = log_loss(y_true_np, y_prob_np)\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "            y_true_np, y_pred_class, average='binary', zero_division=0\n",
    "        )\n",
    "        acc = accuracy_score(y_true_np, y_pred_class)\n",
    "        toss, _, _ = compute_toss_score(y_true_np, y_prob_np)\n",
    "\n",
    "        results[name] = {\n",
    "            'ROC-AUC': roc, 'PR-AUC': pr_auc, 'Avg Precision': avg_prec,\n",
    "            'Precision': prec, 'Recall': rec, 'F1-Score': f1, 'Accuracy': acc,\n",
    "            'Log Loss': ll, 'Toss Score': toss\n",
    "        }\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "\n",
    "def soft_voting_ensemble(prediction_files, output_file):\n",
    "    \"\"\"ì—¬ëŸ¬ ì‹œë“œì˜ ì˜ˆì¸¡ íŒŒì¼ì„ í‰ê· í•˜ì—¬ ì•™ìƒë¸”\"\"\"\n",
    "    if not prediction_files:\n",
    "        print(\"âŒ No prediction files.\")\n",
    "        return\n",
    "\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={'clicked': 'click_0'})\n",
    "    for i, f in enumerate(prediction_files[1:], 1):\n",
    "        d = pd.read_csv(f).rename(columns={'clicked': f'click_{i}'})\n",
    "        merged = merged.merge(d, on='ID')\n",
    "\n",
    "    click_cols = [c for c in merged.columns if c.startswith('click_')]\n",
    "    merged['clicked'] = merged[click_cols].mean(axis=1)\n",
    "\n",
    "    merged[['ID', 'clicked']].to_csv(output_file, index=False)\n",
    "    print(f\"ğŸ‰ Ensemble saved: {output_file}\")\n",
    "    print(merged.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ea01f",
   "metadata": {},
   "source": [
    "[PART 5] ë©”ì¸ í•™ìŠµ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7dc0936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸš€ Processing Seed: 42\n",
      "============================================================\n",
      "ğŸ“‚ Loading (GPU): /home/konkukstat/python3/workspace/data/train_sampled_50pct.parquet\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error (failed to allocate 22536303016 bytes) at: /tmp/pip-build-env-50yo9kd_/normal/lib/python3.10/site-packages/librmm/include/rmm/mr/cuda_memory_resource.hpp:51: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m train_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG\u001b[38;5;241m.\u001b[39mDATA_PATH, CFG\u001b[38;5;241m.\u001b[39mTRAIN_FILE)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“‚ Loading (GPU): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m full_train_gdf \u001b[38;5;241m=\u001b[39m \u001b[43mcudf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Train loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_train_gdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cudf/io/parquet.py:1075\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(filepath_or_buffer, engine, columns, storage_options, filesystem, filters, row_groups, use_pandas_metadata, categorical_partitions, bytes_per_thread, dataset_kwargs, nrows, skip_rows, allow_mismatched_pq_schemas, ignore_missing_columns, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28mset\u001b[39m(v[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(filters))\n\u001b[1;32m   1071\u001b[0m         \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mset\u001b[39m(columns)\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;66;03m# Convert parquet data to a cudf.DataFrame\u001b[39;00m\n\u001b[0;32m-> 1075\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43m_parquet_to_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepaths_or_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrow_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_pandas_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_categories\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_categories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_mismatched_pq_schemas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_mismatched_pq_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_missing_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_missing_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mast_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;66;03m# Apply filters row-wise (if any are defined), and return\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ast_filter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cudf/io/parquet.py:1227\u001b[0m, in \u001b[0;36m_parquet_to_frame\u001b[0;34m(paths_or_buffers, row_groups, partition_keys, partition_categories, dataset_kwargs, nrows, skip_rows, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;129m@_performance_tracking\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_parquet_to_frame\u001b[39m(\n\u001b[1;32m   1214\u001b[0m     paths_or_buffers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;66;03m# If this is not a partitioned read, only need\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m     \u001b[38;5;66;03m# one call to `_read_parquet`\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partition_keys:\n\u001b[0;32m-> 1227\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpaths_or_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1230\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nrows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m skip_rows \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1238\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows/skip_rows is not supported when reading a partitioned parquet dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1239\u001b[0m         )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/cudf/io/parquet.py:1440\u001b[0m, in \u001b[0;36m_read_parquet\u001b[0;34m(filepaths_or_buffers, engine, columns, row_groups, use_pandas_metadata, nrows, skip_rows, allow_mismatched_pq_schemas, ignore_missing_columns, filters, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1438\u001b[0m     options\u001b[38;5;241m.\u001b[39mset_filter(filters)\n\u001b[0;32m-> 1440\u001b[0m tbl_w_meta \u001b[38;5;241m=\u001b[39m \u001b[43mplc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1441\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame\u001b[38;5;241m.\u001b[39mfrom_pylibcudf(tbl_w_meta)\n\u001b[1;32m   1442\u001b[0m df \u001b[38;5;241m=\u001b[39m _process_metadata(\n\u001b[1;32m   1443\u001b[0m     df,\n\u001b[1;32m   1444\u001b[0m     tbl_w_meta\u001b[38;5;241m.\u001b[39mcolumn_names(include_children\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1451\u001b[0m     skip_rows\u001b[38;5;241m=\u001b[39mskip_rows,\n\u001b[1;32m   1452\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pylibcudf/io/parquet.pyx:407\u001b[0m, in \u001b[0;36mpylibcudf.io.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pylibcudf/io/parquet.pyx:430\u001b[0m, in \u001b[0;36mpylibcudf.io.parquet.read_parquet\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error (failed to allocate 22536303016 bytes) at: /tmp/pip-build-env-50yo9kd_/normal/lib/python3.10/site-packages/librmm/include/rmm/mr/cuda_memory_resource.hpp:51: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "# â”€â”€ 0. ì¶œë ¥ í´ë” ìƒì„± ë° ì„¤ì • ì €ì¥ â”€â”€\n",
    "os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)\n",
    "save_experiment_config(CFG)\n",
    "\n",
    "prediction_files = []\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ğŸš€ Processing Seed: {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # â”€â”€ 1. ë°ì´í„° ë¡œë“œ (cudf) â”€â”€\n",
    "    train_path = os.path.join(CFG.DATA_PATH, CFG.TRAIN_FILE)\n",
    "    print(f\"ğŸ“‚ Loading (GPU): {train_path}\")\n",
    "    full_train_gdf = cudf.read_parquet(train_path)\n",
    "    print(f\"âœ… Train loaded: {full_train_gdf.shape}\")\n",
    "\n",
    "    try:\n",
    "        test_gdf = cudf.read_parquet(os.path.join(CFG.DATA_PATH, CFG.TEST_FILE))\n",
    "        print(f\"âœ… Test loaded: {test_gdf.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ Test not found. Creating dummy.\")\n",
    "        test_gdf = full_train_gdf.head(100).drop(columns=['clicked'])\n",
    "        test_gdf['ID'] = cudf.Series([f'TEST_{i}' for i in range(100)])\n",
    "\n",
    "    # â”€â”€ 1-1. ë²”ì£¼í˜• ë³€ìˆ˜ dtype ë³€í™˜ â”€â”€\n",
    "    print(\"ğŸ”„ Converting categorical columns to str...\")\n",
    "    for col in CFG.CATEGORICAL_COLS:\n",
    "        if col in full_train_gdf.columns:\n",
    "            full_train_gdf[col] = full_train_gdf[col].astype(str)\n",
    "        if col in test_gdf.columns:\n",
    "            test_gdf[col] = test_gdf[col].astype(str)\n",
    "    print(\"âœ… Categorical dtype conversion done\")\n",
    "\n",
    "    # â”€â”€ 2. seq íŒŒìƒ í”¼ì²˜ ìƒì„± â”€â”€\n",
    "    if CFG.USE_SEQ_FEATURES:\n",
    "        print(\"ğŸ› ï¸ Adding seq features...\")\n",
    "        add_seq_features(full_train_gdf)\n",
    "        add_seq_features(test_gdf)\n",
    "    else:\n",
    "        print(\"â­ï¸ Skipping seq features...\")\n",
    "\n",
    "    # â”€â”€ 3. seq ë³´ê´€ + ì œê±° â”€â”€\n",
    "    train_seq_raw = full_train_gdf['seq'].to_pandas() if 'seq' in full_train_gdf.columns else None\n",
    "    test_seq_raw = test_gdf['seq'].to_pandas() if 'seq' in test_gdf.columns else None\n",
    "    if 'seq' in full_train_gdf.columns:\n",
    "        full_train_gdf.drop(columns=['seq'], inplace=True)\n",
    "    if 'seq' in test_gdf.columns:\n",
    "        test_gdf.drop(columns=['seq'], inplace=True)\n",
    "\n",
    "    # â”€â”€ 4. Train / Validation Split â”€â”€\n",
    "    print(\"ğŸ“Š Splitting train/validation...\")\n",
    "    train_gdf, val_gdf = gpu_train_test_split(\n",
    "        full_train_gdf, test_size=CFG.VALID_RATIO,\n",
    "        random_state=seed, stratify=full_train_gdf['clicked']\n",
    "    )\n",
    "    \n",
    "    # seqë„ ê°™ì´ split (ì¸ë±ìŠ¤ í™œìš©)\n",
    "    if train_seq_raw is not None:\n",
    "        train_seq = train_seq_raw.loc[train_gdf.index.to_pandas()]\n",
    "        val_seq = train_seq_raw.loc[val_gdf.index.to_pandas()]\n",
    "        \n",
    "    del full_train_gdf\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "\n",
    "    # â”€â”€ 5. Train setë§Œ ë‹¤ìš´ìƒ˜í”Œë§ â”€â”€\n",
    "    print(\"ğŸ”» Down-sampling train only...\")\n",
    "    clicked_1 = train_gdf[train_gdf['clicked'] == 1]\n",
    "    clicked_0_all = train_gdf[train_gdf['clicked'] == 0]\n",
    "    n_neg = min(int(len(clicked_1) * CFG.NEG_RATIO), len(clicked_0_all))\n",
    "    clicked_0 = clicked_0_all.sample(n=n_neg, random_state=seed)\n",
    "    \n",
    "    # [FIX] ë¨¼ì € ê²°í•© ë° ì…”í”Œ (ì¸ë±ìŠ¤ ìœ ì§€)\n",
    "    downsampled_gdf = cudf.concat([clicked_1, clicked_0]).sample(frac=1, random_state=seed)\n",
    "    \n",
    "    # seq ë°ì´í„° ë™ê¸°í™” (ê°™ì€ ì¸ë±ìŠ¤ ì‚¬ìš©) ë° Series ìœ ì§€\n",
    "    if train_seq_raw is not None:\n",
    "        # train_seq ë³€ìˆ˜ëŠ” step 4ì—ì„œ ìƒì„±ë¨. ì—¬ê¸°ì„œëŠ” downsampled_gdfì˜ ì¸ë±ìŠ¤ë¡œ í•„í„°ë§í•˜ê³  ë¦¬ì…‹.\n",
    "        # ì¸ë±ìŠ¤ëŠ” ì›ë³¸ ì¸ë±ìŠ¤ë¥¼ ìœ ì§€í•˜ê³  ìˆìœ¼ë¯€ë¡œ train_seq_raw(ì „ì²´)ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ step 4ì˜ train_seqì—ì„œ ê°€ì ¸ì™€ë„ ë¨.\n",
    "        # CPU ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì²˜ë¦¬:\n",
    "        train_seq = train_seq_raw.loc[downsampled_gdf.index.to_pandas()].reset_index(drop=True)\n",
    "\n",
    "    # train_gdf ìµœì¢… í™•ì •\n",
    "    train_gdf = downsampled_gdf.reset_index(drop=True)\n",
    "    \n",
    "    del clicked_1, clicked_0, clicked_0_all\n",
    "\n",
    "    # â”€â”€ 6. Feature Engineering â”€â”€\n",
    "    print(\"ğŸ› ï¸ Feature Engineering...\")\n",
    "\n",
    "    if CFG.USE_INTERACTION_FEATURES:\n",
    "        print(\"  -> Adding interaction features...\")\n",
    "        add_interaction_features_gpu(train_gdf)\n",
    "        add_interaction_features_gpu(val_gdf)\n",
    "        add_interaction_features_gpu(test_gdf)\n",
    "    else:\n",
    "        print(\"  -> Skipping interaction features...\")\n",
    "\n",
    "    if CFG.USE_GROUPBY_FEATURES:\n",
    "        print(\"  -> Adding groupby features...\")\n",
    "        add_groupby_features_gpu(train_gdf, val_gdf, test_gdf, 'inventory_id')\n",
    "    else:\n",
    "        print(\"  -> Skipping groupby features...\")\n",
    "    \n",
    "    if CFG.USE_COUNT_FEATURES:\n",
    "        print(\"  -> Adding count features...\")\n",
    "        count_target_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "        count_target_cols = [c for c in count_target_cols if c in train_gdf.columns]\n",
    "        add_count_features_gpu(train_gdf, val_gdf, test_gdf, count_target_cols)\n",
    "    else:\n",
    "        print(\"  -> Skipping count features...\")\n",
    "\n",
    "    # â”€â”€ 7. ì»¬ëŸ¼ ë¶„ë¦¬ â”€â”€\n",
    "    exclude_cols = {'clicked', 'ID'}\n",
    "    all_cols = [c for c in train_gdf.columns if c not in exclude_cols]\n",
    "\n",
    "    # ë²”ì£¼í˜• = CFG.CATEGORICAL_COLS + object dtype (interaction í”¼ì²˜ í¬í•¨)\n",
    "    cat_cols = [\n",
    "        c for c in all_cols\n",
    "        if (c in CFG.CATEGORICAL_COLS or train_gdf[c].dtype == 'object')\n",
    "        and c in val_gdf.columns and c in test_gdf.columns\n",
    "    ]\n",
    "    # ìˆ˜ì¹˜í˜• = ë‚˜ë¨¸ì§€ (ë²”ì£¼í˜• ì œì™¸)\n",
    "    num_cols = [c for c in all_cols if c not in cat_cols\n",
    "                and c in val_gdf.columns and c in test_gdf.columns]\n",
    "\n",
    "    # â”€â”€ 8. ìˆ˜ì¹˜í˜• ì „ì²˜ë¦¬ â”€â”€\n",
    "    print(\"ğŸ“‹ Preprocessing numerical features...\")\n",
    "    # Scaling (cuml StandardScaler)\n",
    "    from cuml.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    train_gdf[num_cols] = scaler.fit_transform(train_gdf[num_cols])\n",
    "    val_gdf[num_cols] = scaler.transform(val_gdf[num_cols])\n",
    "    test_gdf[num_cols] = scaler.transform(test_gdf[num_cols])\n",
    "\n",
    "    for col in num_cols:\n",
    "        fill_val = float(train_gdf[col].mean())\n",
    "        train_gdf[col] = train_gdf[col].fillna(fill_val)\n",
    "        val_gdf[col] = val_gdf[col].fillna(fill_val)\n",
    "        test_gdf[col] = test_gdf[col].fillna(fill_val)\n",
    "\n",
    "    # â”€â”€ 9. ë²”ì£¼í˜• ì „ì²˜ë¦¬ (OneHotEncoder â†’ CSR) â”€â”€\n",
    "    print(\"ğŸ“‹ OneHot Encoding categorical features...\")\n",
    "    # cudf OneHotEncoder ë˜ëŠ” pandasë¡œ ë³€í™˜ í›„ sklearn OneHotEncoder ì‚¬ìš©\n",
    "    # pandasë¡œ ë³€í™˜í•˜ì—¬ sklearn OHE ì‚¬ìš© (ì•ˆì •ì„±)\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    train_cat_pd = train_gdf[cat_cols].to_pandas()\n",
    "    val_cat_pd = val_gdf[cat_cols].to_pandas()\n",
    "    test_cat_pd = test_gdf[cat_cols].to_pandas()\n",
    "\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    train_cat_pd = pd.DataFrame(cat_imputer.fit_transform(train_cat_pd), columns=cat_cols)\n",
    "    val_cat_pd = pd.DataFrame(cat_imputer.transform(val_cat_pd), columns=cat_cols)\n",
    "    test_cat_pd = pd.DataFrame(cat_imputer.transform(test_cat_pd), columns=cat_cols)\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=True, handle_unknown='ignore')\n",
    "    train_cat_sparse = ohe.fit_transform(train_cat_pd)\n",
    "    val_cat_sparse = ohe.transform(val_cat_pd)\n",
    "    test_cat_sparse = ohe.transform(test_cat_pd)\n",
    "    print(f\"  OHE shape: Train={train_cat_sparse.shape}\")\n",
    "\n",
    "    # â”€â”€ 10. seq ì„ë² ë”© (ì„ íƒ ì‚¬í•­ â€” ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ í•´ì œ) â”€â”€\n",
    "    ### seq ì„ë² ë”© ì‹œì‘ ###\n",
    "    # from src.models.seq_embedding_gpu import SeqEncoderGPU\n",
    "    # seq_encoder = SeqEncoderGPU(embedding_dim=16)\n",
    "    #\n",
    "    # # Train: í•™ìŠµ + ë²¡í„° ì¶”ì¶œ\n",
    "    # seq_train = seq_encoder.get_features(\n",
    "    #     train_seq, target_series=train_gdf['clicked'].to_pandas(),\n",
    "    #     train_mode=True, epochs=5\n",
    "    # )\n",
    "    # # Valid/Test: ë²¡í„°ë§Œ ì¶”ì¶œ\n",
    "    # seq_val = seq_encoder.get_features(val_seq, train_mode=False)\n",
    "    # seq_test = seq_encoder.get_features(test_seq_raw, train_mode=False)\n",
    "    ### seq ì„ë² ë”© ë â†’ ì•„ë˜ Noneì„ ì‚­ì œí•˜ê³  ìœ„ ì£¼ì„ í•´ì œ ###\n",
    "    seq_train = None\n",
    "    seq_val = None\n",
    "    seq_test = None\n",
    "\n",
    "    # â”€â”€ 11. Feature ê²°í•© â”€â”€\n",
    "    print(\"ğŸ”— Combining features...\")\n",
    "    # GPU â†’ pandas ë³€í™˜ í›„ í•©ì¹˜ê¸° (XGBoost sklearn API í˜¸í™˜)\n",
    "    from scipy.sparse import hstack as sp_hstack\n",
    "\n",
    "    train_num_pd = train_gdf[num_cols].to_pandas()\n",
    "    val_num_pd = val_gdf[num_cols].to_pandas()\n",
    "    test_num_pd = test_gdf[num_cols].to_pandas()\n",
    "\n",
    "    from scipy.sparse import csr_matrix as scipy_csr\n",
    "\n",
    "    def pandas_hstack(matrices):\n",
    "        converted = []\n",
    "        for mat in matrices:\n",
    "            if hasattr(mat, 'tocsr'):\n",
    "                converted.append(mat.tocsr())\n",
    "            elif hasattr(mat, 'values'):\n",
    "                arr = mat.values\n",
    "                if arr.ndim == 1:\n",
    "                    arr = arr.reshape(-1, 1)\n",
    "                converted.append(scipy_csr(arr))\n",
    "            elif isinstance(mat, np.ndarray):\n",
    "                if mat.ndim == 1:\n",
    "                    mat = mat.reshape(-1, 1)\n",
    "                converted.append(scipy_csr(mat))\n",
    "            else:\n",
    "                converted.append(scipy_csr(mat))\n",
    "        return sp_hstack(converted).tocsr()\n",
    "\n",
    "    hstack_train = [train_num_pd, train_cat_sparse]\n",
    "    hstack_val = [val_num_pd, val_cat_sparse]\n",
    "    hstack_test = [test_num_pd, test_cat_sparse]\n",
    "\n",
    "    if seq_train is not None:\n",
    "        hstack_train.append(seq_train)\n",
    "        hstack_val.append(seq_val)\n",
    "        hstack_test.append(seq_test)\n",
    "\n",
    "    X_train = pandas_hstack(hstack_train)\n",
    "    y_train = train_gdf['clicked'].to_pandas().values\n",
    "    X_val = pandas_hstack(hstack_val)\n",
    "    y_val = val_gdf['clicked'].to_pandas().values\n",
    "    X_test = pandas_hstack(hstack_test)\n",
    "\n",
    "    print(f\"  X_train: {X_train.shape}, X_val: {X_val.shape}, X_test: {X_test.shape}\")\n",
    "\n",
    "    # â”€â”€ 12. ëª¨ë¸ í•™ìŠµ (GPU) â”€â”€\n",
    "    print(\"ğŸ”¥ Training XGBoost (GPU)...\")\n",
    "    model = xgb.XGBClassifier(**CFG.XGB_PARAMS, random_state=seed)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=50\n",
    "    )\n",
    "\n",
    "    # â”€â”€ 13. í‰ê°€ â”€â”€\n",
    "    print(\"ğŸ“Š Evaluating...\")\n",
    "    eval_df = evaluate_model(model, X_train, y_train, X_val, y_val, threshold=0.5)\n",
    "    print(eval_df.round(4).to_string())\n",
    "\n",
    "    # â”€â”€ 13-1. í‰ê°€ ì§€í‘œ CSV ì €ì¥ â”€â”€\n",
    "    metrics_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_metrics_{seed}.csv\")\n",
    "    eval_df.to_csv(metrics_file)\n",
    "    print(f\"ğŸ“Š Metrics saved: {metrics_file}\")\n",
    "\n",
    "    # â”€â”€ 14. ì˜ˆì¸¡ ì €ì¥ â”€â”€\n",
    "    test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    out_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_{seed}.csv\")\n",
    "    test_ids = test_gdf['ID'].to_pandas().values if 'ID' in test_gdf.columns else range(len(test_pred))\n",
    "    sub = pd.DataFrame({'ID': test_ids, 'clicked': test_pred})\n",
    "    sub.to_csv(out_file, index=False)\n",
    "    prediction_files.append(out_file)\n",
    "    print(f\"ğŸ’¾ Saved: {out_file}\")\n",
    "\n",
    "    del train_gdf, val_gdf, test_gdf, X_train, X_val, X_test, model\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… All seeds processed: {prediction_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f94c17",
   "metadata": {},
   "source": [
    "[PART 6] Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe8d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Creating metrics summary...\n",
      "ğŸ“Š Summary metrics saved: XGB_GPU_metrics_summary.csv\n",
      "\n",
      "       ROC-AUC  PR-AUC  Avg Precision  Precision  Recall  F1-Score  Accuracy  Log Loss  Toss Score  Seed\n",
      "Train   0.8024  0.8112         0.8112     0.7341  0.7064    0.7200    0.7253    0.5538      0.7274    42\n",
      "Valid   0.5801  0.0283         0.0283     0.0190  0.8641    0.0371    0.1513    0.9885      0.3013    42\n",
      "Train   0.7872  0.7961         0.7961     0.7182  0.6869    0.7022    0.7087    0.5665      0.7172   106\n",
      "Valid   0.5752  0.0263         0.0264     0.0188  0.8577    0.0368    0.1508    1.0344      0.2976   106\n",
      "Train   0.8170  0.8255         0.8255     0.7496  0.7077    0.7280    0.7356    0.5411      0.7372  1031\n",
      "Valid   0.5898  0.0282         0.0282     0.0192  0.8718    0.0376    0.1549    0.9526      0.3036  1031\n",
      "ğŸ‰ Ensemble saved: XGB_GPU_SoftVoting.csv\n",
      "             ID   click_0   click_1   click_2   clicked\n",
      "0  TEST_0000000  0.425402  0.518557  0.443019  0.462326\n",
      "1  TEST_0000001  0.346332  0.449022  0.359749  0.385034\n",
      "2  TEST_0000002  0.376556  0.453064  0.443939  0.424519\n",
      "3  TEST_0000003  0.351361  0.388640  0.464481  0.401494\n",
      "4  TEST_0000004  0.442404  0.455424  0.480855  0.459561\n",
      "\n",
      "============================================================\n",
      "ğŸ ëª¨ë“  í•™ìŠµ ë° ì•™ìƒë¸” ì™„ë£Œ!\n",
      "  ì‹œë“œ: [42, 106, 1031]\n",
      "  ê°œë³„ ì˜ˆì¸¡: ['XGB_GPU_42.csv', 'XGB_GPU_106.csv', 'XGB_GPU_1031.csv']\n",
      "  ì•™ìƒë¸” ê²°ê³¼: XGB_GPU_SoftVoting.csv\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ“Š Creating metrics summary...\")\n",
    "all_metrics = []\n",
    "for seed in CFG.SEEDS:\n",
    "    metrics_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_metrics_{seed}.csv\")\n",
    "    if os.path.exists(metrics_file):\n",
    "        df = pd.read_csv(metrics_file, index_col=0)\n",
    "        df['Seed'] = seed\n",
    "        all_metrics.append(df)\n",
    "\n",
    "if all_metrics:\n",
    "    summary_metrics = pd.concat(all_metrics, ignore_index=False)\n",
    "    summary_file = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_metrics_summary.csv\")\n",
    "    summary_metrics.to_csv(summary_file)\n",
    "    print(f\"ğŸ“Š Summary metrics saved: {summary_file}\")\n",
    "    print(\"\\n\" + summary_metrics.round(4).to_string())\n",
    "\n",
    "#%% [PART 7] Soft Voting ì•™ìƒë¸”\n",
    "final_output = os.path.join(CFG.OUTPUT_DIR, f\"{CFG.OUTPUT_PREFIX}_SoftVoting.csv\")\n",
    "soft_voting_ensemble(prediction_files, final_output)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ğŸ ëª¨ë“  í•™ìŠµ ë° ì•™ìƒë¸” ì™„ë£Œ!\")\n",
    "print(f\"  ì‹œë“œ: {CFG.SEEDS}\")\n",
    "print(f\"  ê°œë³„ ì˜ˆì¸¡: {prediction_files}\")\n",
    "print(f\"  ì•™ìƒë¸” ê²°ê³¼: {final_output}\")\n",
    "print(f\"{'='*60}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
