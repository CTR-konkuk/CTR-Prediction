{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e4d51e",
   "metadata": {},
   "source": [
    "\n",
    "# 03. CatBoost ëª¨ë¸ë§ ë° ì•™ìƒë¸” (CatBoost & Soft Voting)\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ **CatBoost Classifier**ë¥¼ í™œìš©í•˜ì—¬ CTR ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ì— ê°•ì ì´ ìˆëŠ” CatBoostë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‹œë“œ(Seed) ì•™ìƒë¸”ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ› ï¸ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸\n",
    "1. **Down-Sampling**: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ 1:1 ë¹„ìœ¨ ìƒ˜í”Œë§ ì ìš©\n",
    "2. **Feature Engineering**:\n",
    "    - **Sequence Features**: ì‚¬ìš©ì í–‰ë™ ì´ë ¥(`seq`)ì„ í™œìš©í•œ ê¸¸ì´, ë¹ˆë„, íŒ¨í„´ ë³€ìˆ˜ ìƒì„±\n",
    "    - **Interaction Features**: ì£¼ìš” ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ì˜ ì¡°í•©(Interaction) íŒŒìƒ ë³€ìˆ˜\n",
    "    - **Group Statistics**: IDë³„ ìƒì„¸ í†µê³„ëŸ‰ ì§‘ê³„\n",
    "3. **Model Training**:\n",
    "    - `TossMetric` (LogLossì™€ APì˜ ê°€ì¤‘ ê²°í•©)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í•™ìŠµ ìµœì í™”\n",
    "    - 3ê°€ì§€ Random Seed(42, 106, 1031)ë¥¼ ì‚¬ìš©í•œ ë…ë¦½ì  ëª¨ë¸ í•™ìŠµ\n",
    "4. **Ensemble**: í•™ìŠµëœ ëª¨ë¸ ê°„ì˜ Soft Votingì„ í†µí•œ ìµœì¢… í™•ë¥ ê°’ ì‚°ì¶œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65183e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, log_loss, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CatBoost ì„¤ì¹˜ í™•ì¸ (í•„ìš”ì‹œ)\n",
    "try:\n",
    "    import catboost\n",
    "except ImportError:\n",
    "    !pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6c4e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: /home/konkukstat/python3/workspace/data/\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    # ë°ì´í„° ê²½ë¡œ\n",
    "    DATA_PATH = '/home/konkukstat/python3/workspace/data/'\n",
    "    \n",
    "    # ğŸ’¥ í•™ìŠµí•  ì‹œë“œ ë¦¬ìŠ¤íŠ¸ (ì•™ìƒë¸” íš¨ê³¼)\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    \n",
    "    VALID_RATIO = 0.2\n",
    "    \n",
    "    # ìƒì„±í•  íŒŒì¼ëª… Prefix\n",
    "    OUTPUT_PREFIX = '03_CatBoost'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "print(f\"Data Path: {CFG.DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c5c47",
   "metadata": {},
   "source": [
    "## í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” CatBoost ëª¨ë¸ì„ í•™ìŠµí•˜ê³  í‰ê°€í•˜ê¸° ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "\n",
    "- **pandas, numpy**: ë°ì´í„° ì¡°ì‘ ë° ìˆ˜ì¹˜ ê³„ì‚°\n",
    "- **catboost**: ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸\n",
    "- **sklearn**: ë°ì´í„° ë¶„í• , ë©”íŠ¸ë¦­ ê³„ì‚° ë“±\n",
    "- **tqdm**: ì§„í–‰ ìƒí™© í‘œì‹œ\n",
    "- **gc, random, warnings**: ë©”ëª¨ë¦¬ ê´€ë¦¬ ë° í™˜ê²½ ì„¤ì •\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd88347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. í´ë¦­ í™•ë¥  ë§µ ë¡œë“œ ---\n",
    "try:\n",
    "    df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    click_prob_map = dict(zip(df_click_prob['number'], df_click_prob['click_prob']))\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ 'high_click_numbers.xlsx' not found. Skipping click_prob features.\")\n",
    "    click_prob_map = {}\n",
    "\n",
    "# --- 2. ë¶€ì •/ê¸ì • ë¦¬ìŠ¤íŠ¸ ---\n",
    "pos_list = {370, 528, 68, 561, 144, 227, 417, 442, 186, 395}\n",
    "neg_list = {154, 222, 84, 498, 434, 511, 216, 497, 309, 446}\n",
    "\n",
    "def add_seq_features(df, name=\"dataset\"):\n",
    "    # seq_len, avg_prob, seq_neg, seq_pos ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    seq_len, avg_prob, seq_neg, seq_pos = [], [], [], []\n",
    "\n",
    "    for s in tqdm(df[\"seq\"], desc=f\"Processing {name}\"):\n",
    "        if isinstance(s, str) and s != \"\":\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "\n",
    "            # ê¸¸ì´\n",
    "            seq_len.append(len(arr))\n",
    "\n",
    "            # í´ë¦­ í™•ë¥  í‰ê· \n",
    "            probs = [click_prob_map.get(num) for num in arr if num in click_prob_map]\n",
    "            avg_prob.append(sum(probs) / len(probs) if probs else np.nan)\n",
    "\n",
    "            # neg/pos ì¹´ìš´íŠ¸\n",
    "            seq_neg.append(sum(1 for x in arr if x in neg_list))\n",
    "            seq_pos.append(sum(1 for x in arr if x in pos_list))\n",
    "        else:\n",
    "            seq_len.append(0)\n",
    "            avg_prob.append(np.nan)\n",
    "            seq_neg.append(0)\n",
    "            seq_pos.append(0)\n",
    "\n",
    "    df[\"seq_len\"] = seq_len\n",
    "    df[\"avg_click_prob\"] = avg_prob\n",
    "    df[\"seq_neglogcount\"] = seq_neg\n",
    "    df[\"seq_poslogcount\"] = seq_pos\n",
    "\n",
    "    # avg_click_prob ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "    overall_avg_prob = df[\"avg_click_prob\"].mean()\n",
    "    df[\"avg_click_prob\"].fillna(overall_avg_prob, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_groupby_features(train_df, test_df, feature_name, agg_dict):\n",
    "    print(f\"--- Creating features by grouping '{feature_name}' ---\")\n",
    "    agg_stats = train_df.groupby(feature_name).agg(agg_dict)\n",
    "    new_cols = [f\"{feature_name}_{col[0]}_{col[1]}\" for col in agg_stats.columns]\n",
    "    agg_stats.columns = new_cols\n",
    "    agg_stats.reset_index(inplace=True)\n",
    "\n",
    "    train_df = pd.merge(train_df, agg_stats, on=feature_name, how='left')\n",
    "    test_df = pd.merge(test_df, agg_stats, on=feature_name, how='left')\n",
    "\n",
    "    for col in new_cols:\n",
    "        fill_value = train_df[col].mean()\n",
    "        test_df[col].fillna(fill_value, inplace=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def add_count_features(train_df, test_df, count_cols):\n",
    "    print(\"--- Creating Count Encoding Features ---\")\n",
    "    for col in count_cols:\n",
    "        # Use concat to get global counts\n",
    "        all_data = pd.concat([train_df[col], test_df[col]], ignore_index=True)\n",
    "        count_map = all_data.value_counts().to_dict()\n",
    "        \n",
    "        train_df[f\"{col}_count\"] = train_df[col].map(count_map).fillna(0).astype(int)\n",
    "        test_df[f\"{col}_count\"] = test_df[col].map(count_map).fillna(0).astype(int)\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755c4e9",
   "metadata": {},
   "source": [
    "## í™˜ê²½ ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜\n",
    "\n",
    "**CFG í´ë˜ìŠ¤**: ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ëª¨ë“  ì„¤ì •ê°’ì„ ê´€ë¦¬\n",
    "- `DATA_PATH`: ë°ì´í„° íŒŒì¼ ì €ì¥ ìœ„ì¹˜\n",
    "- `SEEDS`: 3ê°œì˜ ë‚œìˆ˜ ì‹œë“œ (42, 106, 1031) - ì„œë¡œ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”ëœ 3ê°œ ëª¨ë¸ í•™ìŠµ\n",
    "- `VALID_RATIO`: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (20%)\n",
    "- `OUTPUT_PREFIX`: ì €ì¥ë  ì˜ˆì¸¡ íŒŒì¼ ì´ë¦„ prefix\n",
    "\n",
    "**seed_everything()**: ëª¨ë“  ë‚œìˆ˜ ìƒì„±ê¸°ë¥¼ ê³ ì •í•˜ì—¬ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ ë³´ì¥\n",
    "- Python, NumPy, ìš´ì˜ì²´ì œ ìˆ˜ì¤€ì˜ ë‚œìˆ˜ ëª¨ë‘ ì œì–´\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5c28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TossMetric:\n",
    "    def is_max_optimal(self): return True\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-approxes[0]))\n",
    "        y_true = np.array(target)\n",
    "        # Check imbalance\n",
    "        N_1, N_0 = np.sum(y_true), len(y_true) - np.sum(y_true)\n",
    "        if N_1 == 0 or N_0 == 0: return 0.5, 0\n",
    "        \n",
    "        # Weighted LogLoss\n",
    "        weights = np.where(y_true == 1, 0.5 / N_1, 0.5 / N_0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        wll = log_loss(y_true, y_pred_clipped, sample_weight=weights)\n",
    "        \n",
    "        # AP\n",
    "        ap = average_precision_score(y_true, y_pred)\n",
    "        \n",
    "        # Score Formula\n",
    "        score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "        return score, 1.0\n",
    "        \n",
    "    def get_final_error(self, scores, weight): return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b76d04",
   "metadata": {},
   "source": [
    "## í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: íŒŒìƒ ë³€ìˆ˜ ìƒì„± í•¨ìˆ˜ë“¤\n",
    "\n",
    "### 1. **high_click_numbers.xlsx ë¡œë“œ**\n",
    "- 02_EDA.ipynbì—ì„œ ìƒì„±ëœ íŒŒì¼\n",
    "- ê° ì•„ì´í…œë²ˆí˜¸ë³„ í´ë¦­ í™•ë¥  ì €ì¥ (ì˜ˆ: ì•„ì´í…œ 370ì€ 85% í´ë¦­ë¥ )\n",
    "- ì‚¬ìš©ìê°€ ë³¸ ì•„ì´í…œë“¤ì˜ í‰ê·  í´ë¦­ í™•ë¥ ì„ í”¼ì²˜ë¡œ ì¶”ì¶œí•˜ëŠ”ë° ì‚¬ìš©\n",
    "\n",
    "### 2. **pos_list, neg_list**\n",
    "- ê¸ì •/ë¶€ì • ì•„ì´í…œ: ê°•ì‚¬/ì¡°ì •íŒ€ì´ ì‚¬ì „ì— ë ˆì´ë¸”ë§í•œ ì•„ì´í…œ\n",
    "- ì‚¬ìš©ì ì‹œí€€ìŠ¤ì—ì„œ ê¸ì •/ë¶€ì • ì•„ì´í…œ ê°œìˆ˜ ì¹´ìš´íŒ…\n",
    "\n",
    "### 3. **add_seq_features()**\n",
    "ì‚¬ìš©ì ë°©ë¬¸ ì‹œí€€ìŠ¤(`seq`)ë¡œë¶€í„° 4ê°œ íŒŒìƒ ë³€ìˆ˜ ìƒì„±:\n",
    "- `seq_len`: ì‹œí€€ìŠ¤ ê¸¸ì´ (ì‚¬ìš©ìê°€ ë³¸ ê´‘ê³  ìˆ˜)\n",
    "- `avg_click_prob`: ì‹œí€€ìŠ¤ ë‚´ ì•„ì´í…œë“¤ì˜ í‰ê·  í´ë¦­ í™•ë¥ \n",
    "- `seq_neglogcount`: ë°©ë¬¸í•œ ë¶€ì • ì•„ì´í…œ ê°œìˆ˜\n",
    "- `seq_poslogcount`: ë°©ë¬¸í•œ ê¸ì • ì•„ì´í…œ ê°œìˆ˜\n",
    "\n",
    "### 4. **add_groupby_features()**\n",
    "íŠ¹ì • ì»¬ëŸ¼(ì˜ˆ: inventory_id)ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì§‘ê³„ í†µê³„ ìƒì„±:\n",
    "- ê° ì¸ë²¤í† ë¦¬ë³„ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°\n",
    "- í•™ìŠµ ë°ì´í„° í†µê³„ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì±„ìš°ê¸° (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)\n",
    "\n",
    "### 5. **add_count_features()**\n",
    "ì¹´ìš´íŠ¸ ì¸ì½”ë”©: ê° ë²”ì£¼ì˜ ë“±ì¥ ë¹ˆë„ë¥¼ ìˆ«ìë¡œ ë³€í™˜\n",
    "- ì˜ˆ: 'male'ì´ 100,000ë²ˆ ë‚˜íƒ€ë‚˜ë©´ ê°’ 100,000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d48ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ğŸš€ Processing Seed: 42\n",
      "========================================\n",
      "ğŸ“‚ Loading train data from: /home/konkukstat/python3/workspace/data/train_sample_10pct.parquet\n",
      "âœ… Train data loaded: (1071335, 119)\n",
      "âœ… Test data loaded: (1527298, 119)\n",
      "ğŸ”» Down-sampling to 1:1 ratio...\n",
      "âœ… Down-sampled train data: (40546, 119)\n",
      "ğŸ› ï¸ Adding sequence features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40546/40546 [00:03<00:00, 11794.32it/s]\n",
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1527298/1527298 [02:16<00:00, 11215.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train/Val split: (32436, 123) / (8110, 123)\n",
      "ğŸ”¥ Training CatBoost with 121 features (cat_features: ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour'])...\n",
      "ğŸ“Š Making predictions...\n",
      "âœ… Validation AP: 0.7215, LogLoss: 0.6159\n",
      "ğŸ’¾ Predictions saved: 03_CatBoost_42_predictions.csv\n",
      "\n",
      "========================================\n",
      "ğŸš€ Processing Seed: 106\n",
      "========================================\n",
      "ğŸ“‚ Loading train data from: /home/konkukstat/python3/workspace/data/train_sample_10pct.parquet\n",
      "âœ… Train data loaded: (1071335, 119)\n",
      "âœ… Test data loaded: (1527298, 119)\n",
      "ğŸ”» Down-sampling to 1:1 ratio...\n",
      "âœ… Down-sampled train data: (40546, 119)\n",
      "ğŸ› ï¸ Adding sequence features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40546/40546 [00:03<00:00, 11857.90it/s]\n",
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1527298/1527298 [02:16<00:00, 11207.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train/Val split: (32436, 123) / (8110, 123)\n",
      "ğŸ”¥ Training CatBoost with 121 features (cat_features: ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour'])...\n",
      "ğŸ“Š Making predictions...\n",
      "âœ… Validation AP: 0.7233, LogLoss: 0.6131\n",
      "ğŸ’¾ Predictions saved: 03_CatBoost_106_predictions.csv\n",
      "\n",
      "========================================\n",
      "ğŸš€ Processing Seed: 1031\n",
      "========================================\n",
      "ğŸ“‚ Loading train data from: /home/konkukstat/python3/workspace/data/train_sample_10pct.parquet\n",
      "âœ… Train data loaded: (1071335, 119)\n",
      "âœ… Test data loaded: (1527298, 119)\n",
      "ğŸ”» Down-sampling to 1:1 ratio...\n",
      "âœ… Down-sampled train data: (40546, 119)\n",
      "ğŸ› ï¸ Adding sequence features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40546/40546 [00:03<00:00, 12064.54it/s]\n",
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1527298/1527298 [02:13<00:00, 11408.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train/Val split: (32436, 123) / (8110, 123)\n",
      "ğŸ”¥ Training CatBoost with 121 features (cat_features: ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour'])...\n",
      "ğŸ“Š Making predictions...\n",
      "âœ… Validation AP: 0.7054, LogLoss: 0.6240\n",
      "ğŸ’¾ Predictions saved: 03_CatBoost_1031_predictions.csv\n",
      "\n",
      "âœ… All seeds processed: ['03_CatBoost_42_predictions.csv', '03_CatBoost_106_predictions.csv', '03_CatBoost_1031_predictions.csv']\n"
     ]
    }
   ],
   "source": [
    "# --- Main Logic Loop ---\n",
    "\n",
    "# ê²°ê³¼ íŒŒì¼ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "prediction_files = []\n",
    "\n",
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©í•  Aggregation ì •ì˜\n",
    "aggs_to_create = {\n",
    "    'history_a_1': ['mean', 'std'], 'history_a_2': ['mean', 'std'], \n",
    "    'history_a_3': ['mean', 'std'], 'history_a_6': ['mean', 'std'],\n",
    "    'feat_e_2': ['mean', 'std'], 'feat_e_3': ['mean', 'std'],\n",
    "    'feat_c_8' : ['mean', 'std'], 'feat_e_9' : ['mean', 'std'],\n",
    "    'feat_d_4' : ['mean', 'std'], \n",
    "    'l_feat_1' : ['mean', 'std'], 'l_feat_2' : ['mean', 'std'],\n",
    "    'l_feat_5' : ['mean', 'std'], 'l_feat_7' : ['mean', 'std'],\n",
    "    'l_feat_10' : ['mean', 'std'], 'l_feat_15' : ['mean', 'std']\n",
    "}\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*40}\\nğŸš€ Processing Seed: {seed}\\n{'='*40}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ (ë§¤ ë°˜ë³µë§ˆë‹¤ ì›ë³¸ ë¡œë“œ)\n",
    "    # 10% ìƒ˜í”Œë§ëœ ë°ì´í„° ì‚¬ìš© (ì „ì²´ ë°ì´í„° ì‚¬ìš© ì‹œ íŒŒì¼ëª…ì„ 'train.parquet'ìœ¼ë¡œ ë³€ê²½)\n",
    "    train_path = os.path.join(CFG.DATA_PATH, 'train_sample_10pct.parquet') \n",
    "    print(f\"ğŸ“‚ Loading train data from: {train_path}\")\n",
    "    train_df = pd.read_parquet(train_path)\n",
    "    print(f\"âœ… Train data loaded: {train_df.shape}\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, 'test.parquet'))\n",
    "        print(f\"âœ… Test data loaded: {test_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ Test data not found. Creating dummy test data for demo.\")\n",
    "        test_df = train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(len(test_df))]\n",
    "    \n",
    "    # 2. Down-sampling (1:1 ë¹„ìœ¨)\n",
    "    print(\"ğŸ”» Down-sampling to 1:1 ratio...\")\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=len(clicked_1), random_state=seed)\n",
    "    train_df = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    print(f\"âœ… Down-sampled train data: {train_df.shape}\")\n",
    "    \n",
    "    # 3. Feature Engineering\n",
    "    print(\"ğŸ› ï¸ Adding sequence features...\")\n",
    "    train_df = add_seq_features(train_df, \"train\")\n",
    "    test_df = add_seq_features(test_df, \"test\")\n",
    "    \n",
    "    # 4. Train/Validation Split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df, train_df['clicked'], test_size=CFG.VALID_RATIO, random_state=seed, stratify=train_df['clicked']\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Train/Val split: {X_train.shape} / {X_val.shape}\")\n",
    "    \n",
    "    # 5. íŠ¹ì„± ì¤€ë¹„\n",
    "    cat_cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    feature_cols = [c for c in X_train.columns if c not in ['clicked', 'ID', 'seq']]\n",
    "    \n",
    "    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° íƒ€ì… ë³€í™˜\n",
    "    for col in feature_cols:\n",
    "        if col in cat_cols:\n",
    "            # Categorical: NaNì„ 'unknown'ìœ¼ë¡œ ë³€í™˜ ë° ë¬¸ìì—´í™”\n",
    "            X_train[col] = X_train[col].fillna('unknown').astype(str)\n",
    "            X_val[col] = X_val[col].fillna('unknown').astype(str)\n",
    "            test_df[col] = test_df[col].fillna('unknown').astype(str)\n",
    "        else:\n",
    "            # Numeric: NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° ë° floatë¡œ ë³€í™˜\n",
    "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0).astype(float)\n",
    "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce').fillna(0).astype(float)\n",
    "            test_df[col] = pd.to_numeric(test_df[col], errors='coerce').fillna(0).astype(float)\n",
    "    \n",
    "    print(f\"ğŸ”¥ Training CatBoost with {len(feature_cols)} features (cat_features: {cat_cols})...\")\n",
    "    \n",
    "    model = cb.CatBoostClassifier(\n",
    "        iterations=100,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='Logloss',\n",
    "        cat_features=cat_cols,\n",
    "        verbose=50,\n",
    "        random_state=seed,\n",
    "        task_type='CPU'\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train[feature_cols], y_train,\n",
    "        eval_set=(X_val[feature_cols], y_val),\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # 6. ì˜ˆì¸¡ ë° ì €ì¥\n",
    "    print(\"ğŸ“Š Making predictions...\")\n",
    "    y_pred_val = model.predict_proba(X_val[feature_cols])[:, 1]\n",
    "    y_pred_test = model.predict_proba(test_df[feature_cols])[:, 1]\n",
    "    \n",
    "    # Validation ì„±ëŠ¥ í‰ê°€\n",
    "    val_ap = average_precision_score(y_val, y_pred_val)\n",
    "    val_logloss = log_loss(y_val, y_pred_val)\n",
    "    print(f\"âœ… Validation AP: {val_ap:.4f}, LogLoss: {val_logloss:.4f}\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "    output_file = f\"{CFG.OUTPUT_PREFIX}_{seed}_predictions.csv\"\n",
    "    result_df = pd.DataFrame({'ID': test_df['ID'].values, 'clicked': y_pred_test})\n",
    "    result_df.to_csv(output_file, index=False)\n",
    "    print(f\"ğŸ’¾ Predictions saved: {output_file}\")\n",
    "    \n",
    "    prediction_files.append(output_file)\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"\\nâœ… All seeds processed: {prediction_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8fa6c3",
   "metadata": {},
   "source": [
    "## TossMetric: ì»¤ìŠ¤í…€ í‰ê°€ ì§€í‘œ\n",
    "\n",
    "Toss ëŒ€íšŒì—ì„œ ì •ì˜í•œ ëª©í‘œ í•¨ìˆ˜:\n",
    "$$\\text{Score} = 0.5 \\times AP + 0.5 \\times \\frac{1}{1 + \\text{WeightedLogLoss}}$$\n",
    "\n",
    "- **AP (Average Precision)**: í´ë¦­í•œ ìƒí’ˆì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ëŠ¥ë ¥ ì¸¡ì •\n",
    "- **Weighted LogLoss**: í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ ì†ì‹¤ í•¨ìˆ˜\n",
    "  - í´ë¦­(1)ê³¼ ë¹„í´ë¦­(0)ì— ë™ì¼í•œ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "  - í´ë¦­ì´ ë§¤ìš° ë“œë¬¼ì–´ì„œ ë‹¨ìˆœ LogLossë§Œìœ¼ë¡œëŠ” ëª¨ë¸ì´ ëª¨ë“  ê°’ì„ 0ìœ¼ë¡œ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê²½í–¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "595ec559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ Performing Soft Voting on 3 files: ['03_CatBoost_42_predictions.csv', '03_CatBoost_106_predictions.csv', '03_CatBoost_1031_predictions.csv']\n",
      "ğŸ‰ Final CatBoost Ensemble Saved: 03_CatBoost_SoftVoting.csv\n",
      "             ID  click_42  click_106  click_1031   clicked\n",
      "0  TEST_0000000  0.424046   0.414745    0.468882  0.435891\n",
      "1  TEST_0000001  0.381192   0.371616    0.390215  0.381008\n",
      "2  TEST_0000002  0.453006   0.465260    0.443599  0.453955\n",
      "3  TEST_0000003  0.452333   0.466632    0.461494  0.460153\n",
      "4  TEST_0000004  0.411486   0.420445    0.417225  0.416385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Soft Voting Ensemble ---\n",
    "print(f\"\\nğŸ¤ Performing Soft Voting on {len(prediction_files)} files: {prediction_files}\")\n",
    "\n",
    "if len(prediction_files) > 0:\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={\"clicked\": f\"click_{CFG.SEEDS[0]}\"})\n",
    "    \n",
    "    for i, file in enumerate(prediction_files[1:], start=1):\n",
    "        df_temp = pd.read_csv(file).rename(columns={\"clicked\": f\"click_{CFG.SEEDS[i]}\"})\n",
    "        merged = merged.merge(df_temp, on=\"ID\")\n",
    "    \n",
    "    # Mean\n",
    "    click_cols = [c for c in merged.columns if c.startswith(\"click_\")]\n",
    "    merged[\"clicked\"] = merged[click_cols].mean(axis=1)\n",
    "    \n",
    "    # Save Final\n",
    "    final_output = \"03_CatBoost_SoftVoting.csv\"\n",
    "    merged[[\"ID\", \"clicked\"]].to_csv(final_output, index=False)\n",
    "    print(f\"ğŸ‰ Final CatBoost Ensemble Saved: {final_output}\")\n",
    "    print(merged.head())\n",
    "else:\n",
    "    print(\"âŒ No prediction files found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6adea8c",
   "metadata": {},
   "source": [
    "## Soft Voting ê²°ê³¼ í•´ì„\n",
    "\n",
    "### ğŸ“Š ê²°ê³¼ í…Œì´ë¸” ë¶„ì„\n",
    "\n",
    "| ID | Seed 42 | Seed 106 | Seed 1031 | **ìµœì¢… ì˜ˆì¸¡** |\n",
    "|----|---------|----------|-----------|------------|\n",
    "| TEST_0000000 | 0.4240 | 0.4147 | 0.4689 | **0.4359** |\n",
    "| TEST_0000001 | 0.3812 | 0.3716 | 0.3902 | **0.3810** |\n",
    "| TEST_0000002 | 0.4530 | 0.4653 | 0.4436 | **0.4540** |\n",
    "| TEST_0000003 | 0.4523 | 0.4666 | 0.4615 | **0.4602** |\n",
    "| TEST_0000004 | 0.4115 | 0.4204 | 0.4172 | **0.4164** |\n",
    "\n",
    "### ğŸ” ê° ì—´ì˜ ì˜ë¯¸\n",
    "\n",
    "- **click_42, click_106, click_1031**: ê°ê° ë‹¤ë¥¸ ë‚œìˆ˜ ì‹œë“œë¡œ ì´ˆê¸°í™”ëœ 3ê°œ CatBoost ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ \n",
    "- **clicked**: 3ê°œ ëª¨ë¸ ì˜ˆì¸¡ê°’ì˜ í‰ê·  (ìµœì¢… ì•™ìƒë¸” ê²°ê³¼)\n",
    "\n",
    "$$\\text{ìµœì¢… ì˜ˆì¸¡} = \\frac{\\text{click\\_42} + \\text{click\\_106} + \\text{click\\_1031}}{3}$$\n",
    "\n",
    "### ğŸ’¡ í•´ì„ í¬ì¸íŠ¸\n",
    "\n",
    "#### 1ï¸âƒ£ **ëª¨ë¸ ê°„ ì˜ˆì¸¡ í¸ì°¨ê°€ ì‘ì„ìˆ˜ë¡ ì‹ ë¢°ë„ ë†’ìŒ**\n",
    "```\n",
    "TEST_0000001:  í¸ì°¨ = 0.3716 ~ 0.3902 = 0.0186 (ì‘ìŒ âœ…)\n",
    "TEST_0000000:  í¸ì°¨ = 0.4147 ~ 0.4689 = 0.0542 (í¼ âš ï¸)\n",
    "```\n",
    "- TEST_0000001: 3ê°œ ëª¨ë¸ì´ ê±°ì˜ ê°™ì€ ì˜ˆì¸¡ â†’ ë†’ì€ ì‹ ë¢°ë„\n",
    "- TEST_0000000: 3ê°œ ëª¨ë¸ì´ ë‹¤ë¥¸ ì˜ˆì¸¡ â†’ ë‚®ì€ ì‹ ë¢°ë„ (ì¡°ì‚¬ í•„ìš”)\n",
    "\n",
    "#### 2ï¸âƒ£ **ì˜ˆì¸¡ê°’ ë²”ìœ„**\n",
    "- 0.38 ~ 0.46: ì¤‘ê°„ ì •ë„ì˜ í´ë¦­ í™•ë¥ \n",
    "- 0.5 ì´ìƒì´ë©´ í´ë¦­í•  ê°€ëŠ¥ì„± ë†’ìŒ\n",
    "- 0.5 ë¯¸ë§Œì´ë©´ í´ë¦­í•  ê°€ëŠ¥ì„± ë‚®ìŒ\n",
    "\n",
    "#### 3ï¸âƒ£ **ì•™ìƒë¸”ì˜ ì—­í• **\n",
    "- **ê°œë³„ ëª¨ë¸**: ê³¼ì í•© ìœ„í—˜, ë¶ˆì•ˆì •ì„±\n",
    "- **Soft Voting**: 3ê°œ ëª¨ë¸ì˜ ì¥ì  ê²°í•©, ì˜ˆì¸¡ ì•ˆì •í™”\n",
    "- **ê²°ê³¼**: ë” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ìµœì¢… ì˜ˆì¸¡\n",
    "\n",
    "### âœ… ì €ì¥ëœ íŒŒì¼\n",
    "\n",
    "`03_CatBoost_SoftVoting.csv`: IDì™€ ìµœì¢… ì˜ˆì¸¡ í™•ë¥ ë§Œ í¬í•¨\n",
    "- í˜•ì‹: (ID, clicked) 2ê°œ ì»¬ëŸ¼\n",
    "- ë‹¤ìŒ ë‹¨ê³„: ë‹¤ë¥¸ ëª¨ë¸(XGBoost, FiBiNet)ê³¼ ìµœì¢… ê°€ì¤‘ ì•™ìƒë¸” ì˜ˆì •\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9bb803",
   "metadata": {},
   "source": [
    "## Soft Voting ì•™ìƒë¸” (3ê°œ ëª¨ë¸ ê²°í•©)\n",
    "\n",
    "**ëª©í‘œ**: 3ê°œ ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì„ ê°€ì¤‘ í‰ê· í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ ìƒì„±\n",
    "\n",
    "### í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "1. **ì˜ˆì¸¡ íŒŒì¼ ë¡œë“œ**\n",
    "   - `03_CatBoost_42_predictions.csv`\n",
    "   - `03_CatBoost_106_predictions.csv`\n",
    "   - `03_CatBoost_1031_predictions.csv`\n",
    "\n",
    "2. **ID ê¸°ì¤€ ë³‘í•©**\n",
    "   - ëª¨ë“  ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ë™ì¼í•œ ID í–‰ìœ¼ë¡œ ì •ë ¬\n",
    "\n",
    "3. **ê°€ì¤‘ í‰ê·  ê³„ì‚°**\n",
    "   - 3ê°œ ëª¨ë¸ì´ ê°ê° ì–´ë–¤ í™•ë¥ ì„ ì˜ˆì¸¡í–ˆëŠ”ì§€ í‰ê·  ê³„ì‚°\n",
    "   - ì˜ˆ: (0.42 + 0.45 + 0.38) / 3 = 0.42\n",
    "\n",
    "4. **ìµœì¢… íŒŒì¼ ì €ì¥**\n",
    "   - `03_CatBoost_SoftVoting.csv`\n",
    "   - í˜•ì‹: ID, ì•™ìƒë¸” ì˜ˆì¸¡ í™•ë¥ \n",
    "\n",
    "### ì•™ìƒë¸”ì˜ ì´ì \n",
    "- **ì•ˆì •ì„±**: 3ê°œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ë‹¤ë¥´ë©´ ê³¼ì í•© ìœ„í—˜ ê°ì†Œ\n",
    "- **ì¼ë°˜í™”**: ì—¬ëŸ¬ ê°€ì¤‘ ì´ˆê¸°í™”ë¥¼ í†µí•œ ë‹¤ì–‘ì„± í™•ë³´\n",
    "- **ì„±ëŠ¥**: ë³´í†µ ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ ë‹¬ì„±\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed351bd",
   "metadata": {},
   "source": [
    "## ë©”ì¸ í•™ìŠµ ë£¨í”„ (3ê°œ ì‹œë“œ X 3íšŒ ë°˜ë³µ)\n",
    "\n",
    "**ëª©í‘œ**: ì„œë¡œ ë‹¤ë¥¸ ì´ˆê¸°ê°’ìœ¼ë¡œ 3ê°œì˜ ë…ë¦½ì ì¸ CatBoost ëª¨ë¸ í•™ìŠµ â†’ ì•™ìƒë¸”\n",
    "\n",
    "### íŒŒì´í”„ë¼ì¸ ë‹¨ê³„\n",
    "\n",
    "#### 1ï¸âƒ£ **ë°ì´í„° ë¡œë“œ**\n",
    "- í•™ìŠµ ë°ì´í„°: `train_sample_10pct.parquet` (ì•½ 107ë§Œ í–‰, 119ê°œ ì»¬ëŸ¼)\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„°: `test.parquet` (ì•½ 153ë§Œ í–‰)\n",
    "\n",
    "#### 2ï¸âƒ£ **Down-Sampling (í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°)**\n",
    "- ì›ë³¸: ë¹„í´ë¦­(98%) vs í´ë¦­(2%) â†’ **1:50 ë¶ˆê· í˜•**\n",
    "- ì²˜ë¦¬: í´ë¦­ ìƒ˜í”Œ ìˆ˜ë§Œí¼ ë¹„í´ë¦­ ìƒ˜í”Œ ëœë¤ ì¶”ì¶œ â†’ **1:1 ê· í˜•**\n",
    "- ê²°ê³¼: ì•½ 40K í–‰ìœ¼ë¡œ ì¶•ì†Œ (í•™ìŠµ ì†ë„ í–¥ìƒ)\n",
    "\n",
    "#### 3ï¸âƒ£ **Feature Engineering**\n",
    "- `add_seq_features()`: ì‚¬ìš©ì ì‹œí€€ìŠ¤ â†’ 4ê°œ íŒŒìƒ ë³€ìˆ˜\n",
    "- ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° íƒ€ì… ë³€í™˜\n",
    "\n",
    "#### 4ï¸âƒ£ **Train/Validation ë¶„í• **\n",
    "- 8:2 ë¹„ìœ¨ë¡œ ë¶„í•  (80% í•™ìŠµ, 20% ê²€ì¦)\n",
    "- Stratified ë¶„í• : í´ë¦­/ë¹„í´ë¦­ ë¹„ìœ¨ ìœ ì§€\n",
    "\n",
    "#### 5ï¸âƒ£ **íŠ¹ì„± ì¤€ë¹„**\n",
    "- Categorical: `['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']`\n",
    "- Numeric: ë‚˜ë¨¸ì§€ ëª¨ë“  ì»¬ëŸ¼\n",
    "- NaN ì²˜ë¦¬: ë²”ì£¼í˜•ì€ 'unknown', ìˆ˜ì¹˜í˜•ì€ 0ìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "\n",
    "#### 6ï¸âƒ£ **CatBoost ëª¨ë¸ í•™ìŠµ**\n",
    "- **iterations**: 100 (ë¶€ìŠ¤íŒ… ë°˜ë³µ íšŸìˆ˜)\n",
    "- **learning_rate**: 0.05 (ê° ë¶€ìŠ¤íŒ… ìŠ¤í…ì˜ ì˜í–¥ë ¥)\n",
    "- **depth**: 6 (íŠ¸ë¦¬ ê¹Šì´ = í”¼ì²˜ ìƒí˜¸ì‘ìš© ë³µì¡ë„)\n",
    "- **early_stopping_rounds**: 10 (ê²€ì¦ ì†ì‹¤ì´ 10ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ë©ˆì¶¤)\n",
    "\n",
    "#### 7ï¸âƒ£ **ì˜ˆì¸¡ ë° í‰ê°€**\n",
    "- ê²€ì¦ ë°ì´í„°ë¡œ `AP`ì™€ `LogLoss` ê³„ì‚°\n",
    "- í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ì €ì¥ â†’ `03_CatBoost_{seed}_predictions.csv`\n",
    "\n",
    "#### 8ï¸âƒ£ **ë©”ëª¨ë¦¬ ê´€ë¦¬**\n",
    "- ê° ë°˜ë³µ í›„ `gc.collect()`: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
