{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5e4d51e",
   "metadata": {},
   "source": [
    "\n",
    "# 03. CatBoost ëª¨ë¸ë§ ë° ì•™ìƒë¸” (CatBoost & Soft Voting)\n",
    "\n",
    "ë³¸ ë…¸íŠ¸ë¶ì€ **CatBoost Classifier**ë¥¼ í™œìš©í•˜ì—¬ CTR ì˜ˆì¸¡ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.\n",
    "ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ì— ê°•ì ì´ ìˆëŠ” CatBoostë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‹œë“œ(Seed) ì•™ìƒë¸”ì„ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ› ï¸ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸\n",
    "1. **Down-Sampling**: í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•œ 1:1 ë¹„ìœ¨ ìƒ˜í”Œë§ ì ìš©\n",
    "2. **Feature Engineering**:\n",
    "    - **Sequence Features**: ì‚¬ìš©ì í–‰ë™ ì´ë ¥(`seq`)ì„ í™œìš©í•œ ê¸¸ì´, ë¹ˆë„, íŒ¨í„´ ë³€ìˆ˜ ìƒì„±\n",
    "    - **Interaction Features**: ì£¼ìš” ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ì˜ ì¡°í•©(Interaction) íŒŒìƒ ë³€ìˆ˜\n",
    "    - **Group Statistics**: IDë³„ ìƒì„¸ í†µê³„ëŸ‰ ì§‘ê³„\n",
    "3. **Model Training**:\n",
    "    - `TossMetric` (LogLossì™€ APì˜ ê°€ì¤‘ ê²°í•©)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ í•™ìŠµ ìµœì í™”\n",
    "    - 3ê°€ì§€ Random Seed(42, 106, 1031)ë¥¼ ì‚¬ìš©í•œ ë…ë¦½ì  ëª¨ë¸ í•™ìŠµ\n",
    "4. **Ensemble**: í•™ìŠµëœ ëª¨ë¸ ê°„ì˜ Soft Votingì„ í†µí•œ ìµœì¢… í™•ë¥ ê°’ ì‚°ì¶œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65183e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import catboost as cb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, log_loss, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CatBoost ì„¤ì¹˜ í™•ì¸ (í•„ìš”ì‹œ)\n",
    "try:\n",
    "    import catboost\n",
    "except ImportError:\n",
    "    !pip install catboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a6c4e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Path: ../rawdata\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CFG:\n",
    "    # ë°ì´í„° ê²½ë¡œ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
    "    # í˜„ì¬ ìœ„ì¹˜: /models/ ì´ë¯€ë¡œ ë°ì´í„°ëŠ” ../data/ ì— ì¡´ì¬\n",
    "    DATA_PATH = '../rawdata'\n",
    "    \n",
    "    # ğŸ’¥ í•™ìŠµí•  ì‹œë“œ ë¦¬ìŠ¤íŠ¸ (ì•™ìƒë¸” íš¨ê³¼)\n",
    "    SEEDS = [42, 106, 1031]\n",
    "    \n",
    "    VALID_RATIO = 0.2\n",
    "    \n",
    "    # ìƒì„±í•  íŒŒì¼ëª… Prefix\n",
    "    OUTPUT_PREFIX = '03_CatBoost'\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "print(f\"Data Path: {CFG.DATA_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd88347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. í´ë¦­ í™•ë¥  ë§µ ë¡œë“œ ---\n",
    "try:\n",
    "    df_click_prob = pd.read_excel(os.path.join(CFG.DATA_PATH, 'high_click_numbers.xlsx'))\n",
    "    click_prob_map = dict(zip(df_click_prob['number'], df_click_prob['click_prob']))\n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸ 'high_click_numbers.xlsx' not found. Skipping click_prob features.\")\n",
    "    click_prob_map = {}\n",
    "\n",
    "# --- 2. ë¶€ì •/ê¸ì • ë¦¬ìŠ¤íŠ¸ ---\n",
    "pos_list = {370, 528, 68, 561, 144, 227, 417, 442, 186, 395}\n",
    "neg_list = {154, 222, 84, 498, 434, 511, 216, 497, 309, 446}\n",
    "\n",
    "def add_seq_features(df, name=\"dataset\"):\n",
    "    # seq_len, avg_prob, seq_neg, seq_pos ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "    seq_len, avg_prob, seq_neg, seq_pos = [], [], [], []\n",
    "\n",
    "    for s in tqdm(df[\"seq\"], desc=f\"Processing {name}\"):\n",
    "        if isinstance(s, str) and s != \"\":\n",
    "            arr = [int(x) for x in s.split(\",\") if x]\n",
    "\n",
    "            # ê¸¸ì´\n",
    "            seq_len.append(len(arr))\n",
    "\n",
    "            # í´ë¦­ í™•ë¥  í‰ê· \n",
    "            probs = [click_prob_map.get(num) for num in arr if num in click_prob_map]\n",
    "            avg_prob.append(sum(probs) / len(probs) if probs else np.nan)\n",
    "\n",
    "            # neg/pos ì¹´ìš´íŠ¸\n",
    "            seq_neg.append(sum(1 for x in arr if x in neg_list))\n",
    "            seq_pos.append(sum(1 for x in arr if x in pos_list))\n",
    "        else:\n",
    "            seq_len.append(0)\n",
    "            avg_prob.append(np.nan)\n",
    "            seq_neg.append(0)\n",
    "            seq_pos.append(0)\n",
    "\n",
    "    df[\"seq_len\"] = seq_len\n",
    "    df[\"avg_click_prob\"] = avg_prob\n",
    "    df[\"seq_neglogcount\"] = seq_neg\n",
    "    df[\"seq_poslogcount\"] = seq_pos\n",
    "\n",
    "    # avg_click_prob ê²°ì¸¡ì¹˜ ì²˜ë¦¬\n",
    "    overall_avg_prob = df[\"avg_click_prob\"].mean()\n",
    "    df[\"avg_click_prob\"].fillna(overall_avg_prob, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_groupby_features(train_df, test_df, feature_name, agg_dict):\n",
    "    print(f\"--- Creating features by grouping '{feature_name}' ---\")\n",
    "    agg_stats = train_df.groupby(feature_name).agg(agg_dict)\n",
    "    new_cols = [f\"{feature_name}_{col[0]}_{col[1]}\" for col in agg_stats.columns]\n",
    "    agg_stats.columns = new_cols\n",
    "    agg_stats.reset_index(inplace=True)\n",
    "\n",
    "    train_df = pd.merge(train_df, agg_stats, on=feature_name, how='left')\n",
    "    test_df = pd.merge(test_df, agg_stats, on=feature_name, how='left')\n",
    "\n",
    "    for col in new_cols:\n",
    "        fill_value = train_df[col].mean()\n",
    "        test_df[col].fillna(fill_value, inplace=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def add_count_features(train_df, test_df, count_cols):\n",
    "    print(\"--- Creating Count Encoding Features ---\")\n",
    "    for col in count_cols:\n",
    "        # Use concat to get global counts\n",
    "        all_data = pd.concat([train_df[col], test_df[col]], ignore_index=True)\n",
    "        count_map = all_data.value_counts().to_dict()\n",
    "        \n",
    "        train_df[f\"{col}_count\"] = train_df[col].map(count_map).fillna(0).astype(int)\n",
    "        test_df[f\"{col}_count\"] = test_df[col].map(count_map).fillna(0).astype(int)\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5c28d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TossMetric:\n",
    "    def is_max_optimal(self): return True\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        y_pred = 1.0 / (1.0 + np.exp(-approxes[0]))\n",
    "        y_true = np.array(target)\n",
    "        # Check imbalance\n",
    "        N_1, N_0 = np.sum(y_true), len(y_true) - np.sum(y_true)\n",
    "        if N_1 == 0 or N_0 == 0: return 0.5, 0\n",
    "        \n",
    "        # Weighted LogLoss\n",
    "        weights = np.where(y_true == 1, 0.5 / N_1, 0.5 / N_0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        wll = log_loss(y_true, y_pred_clipped, sample_weight=weights)\n",
    "        \n",
    "        # AP\n",
    "        ap = average_precision_score(y_true, y_pred)\n",
    "        \n",
    "        # Score Formula\n",
    "        score = 0.5 * ap + 0.5 * (1 / (1 + wll))\n",
    "        return score, 1.0\n",
    "        \n",
    "    def get_final_error(self, scores, weight): return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d48ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "ğŸš€ Processing Seed: 42\n",
      "========================================\n",
      "ğŸ”» Down-sampling...\n",
      "   Train shape: (40546, 119)\n",
      "ğŸ› ï¸ Feature Engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40546/40546 [00:04<00:00, 9649.71it/s] \n",
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1527298/1527298 [02:58<00:00, 8551.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating features by grouping 'inventory_id' ---\n",
      "--- Creating Count Encoding Features ---\n",
      "ğŸ”¥ Training CatBoost...\n",
      "0:\tlearn: 0.6414476\ttest: 0.6361165\tbest: 0.6361165 (0)\ttotal: 736ms\tremaining: 12m 15s\n",
      "100:\tlearn: 0.6798209\ttest: 0.6654646\tbest: 0.6654646 (100)\ttotal: 14.3s\tremaining: 2m 7s\n",
      "200:\tlearn: 0.6940942\ttest: 0.6699310\tbest: 0.6699310 (200)\ttotal: 28.1s\tremaining: 1m 51s\n",
      "300:\tlearn: 0.7049440\ttest: 0.6723240\tbest: 0.6723240 (300)\ttotal: 42.3s\tremaining: 1m 38s\n",
      "400:\tlearn: 0.7140569\ttest: 0.6738242\tbest: 0.6738552 (398)\ttotal: 55.9s\tremaining: 1m 23s\n",
      "500:\tlearn: 0.7228176\ttest: 0.6747322\tbest: 0.6747341 (498)\ttotal: 1m 9s\tremaining: 1m 8s\n",
      "600:\tlearn: 0.7323062\ttest: 0.6753737\tbest: 0.6754183 (579)\ttotal: 1m 22s\tremaining: 54.5s\n",
      "700:\tlearn: 0.7427383\ttest: 0.6760500\tbest: 0.6760764 (692)\ttotal: 1m 35s\tremaining: 40.6s\n",
      "800:\tlearn: 0.7517399\ttest: 0.6762834\tbest: 0.6762834 (800)\ttotal: 1m 48s\tremaining: 27s\n",
      "900:\tlearn: 0.7596509\ttest: 0.6767309\tbest: 0.6767940 (881)\ttotal: 2m 1s\tremaining: 13.4s\n",
      "999:\tlearn: 0.7673442\ttest: 0.6769479\tbest: 0.6770683 (976)\ttotal: 2m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.677068253\n",
      "bestIteration = 976\n",
      "\n",
      "Shrink model to first 977 iterations.\n",
      "ğŸ”® Predicting...\n",
      "âœ… Saved result to 03_CatBoost_42.csv\n",
      "\n",
      "========================================\n",
      "ğŸš€ Processing Seed: 106\n",
      "========================================\n",
      "ğŸ”» Down-sampling...\n",
      "   Train shape: (40546, 119)\n",
      "ğŸ› ï¸ Feature Engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40546/40546 [00:04<00:00, 8515.87it/s]\n",
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1527298/1527298 [02:54<00:00, 8759.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating features by grouping 'inventory_id' ---\n",
      "--- Creating Count Encoding Features ---\n",
      "ğŸ”¥ Training CatBoost...\n",
      "0:\tlearn: 0.6255548\ttest: 0.6242829\tbest: 0.6242829 (0)\ttotal: 189ms\tremaining: 3m 8s\n",
      "100:\tlearn: 0.6796333\ttest: 0.6689177\tbest: 0.6689177 (100)\ttotal: 12.7s\tremaining: 1m 53s\n",
      "200:\tlearn: 0.6936070\ttest: 0.6735137\tbest: 0.6735137 (200)\ttotal: 25.2s\tremaining: 1m 40s\n",
      "300:\tlearn: 0.7038193\ttest: 0.6755912\tbest: 0.6755928 (299)\ttotal: 37.8s\tremaining: 1m 27s\n",
      "400:\tlearn: 0.7123894\ttest: 0.6769260\tbest: 0.6769294 (399)\ttotal: 50.2s\tremaining: 1m 14s\n",
      "500:\tlearn: 0.7209331\ttest: 0.6779858\tbest: 0.6779858 (500)\ttotal: 1m 2s\tremaining: 1m 2s\n",
      "600:\tlearn: 0.7300700\ttest: 0.6785162\tbest: 0.6785982 (589)\ttotal: 1m 15s\tremaining: 50s\n",
      "700:\tlearn: 0.7393660\ttest: 0.6790900\tbest: 0.6791115 (691)\ttotal: 1m 27s\tremaining: 37.4s\n",
      "800:\tlearn: 0.7485119\ttest: 0.6789778\tbest: 0.6792279 (777)\ttotal: 1m 40s\tremaining: 24.9s\n",
      "900:\tlearn: 0.7560808\ttest: 0.6792540\tbest: 0.6792614 (899)\ttotal: 1m 52s\tremaining: 12.4s\n",
      "999:\tlearn: 0.7639102\ttest: 0.6795240\tbest: 0.6796332 (948)\ttotal: 2m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.679633201\n",
      "bestIteration = 948\n",
      "\n",
      "Shrink model to first 949 iterations.\n",
      "ğŸ”® Predicting...\n",
      "âœ… Saved result to 03_CatBoost_106.csv\n",
      "\n",
      "========================================\n",
      "ğŸš€ Processing Seed: 1031\n",
      "========================================\n",
      "ğŸ”» Down-sampling...\n",
      "   Train shape: (40546, 119)\n",
      "ğŸ› ï¸ Feature Engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40546/40546 [00:04<00:00, 9748.43it/s] \n",
      "Processing test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1527298/1527298 [02:53<00:00, 8813.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Creating features by grouping 'inventory_id' ---\n",
      "--- Creating Count Encoding Features ---\n",
      "ğŸ”¥ Training CatBoost...\n",
      "0:\tlearn: 0.6373777\ttest: 0.6296628\tbest: 0.6296628 (0)\ttotal: 158ms\tremaining: 2m 37s\n",
      "100:\tlearn: 0.6818855\ttest: 0.6625761\tbest: 0.6625761 (100)\ttotal: 12.7s\tremaining: 1m 52s\n",
      "200:\tlearn: 0.6956936\ttest: 0.6661410\tbest: 0.6661410 (200)\ttotal: 25.5s\tremaining: 1m 41s\n",
      "300:\tlearn: 0.7057635\ttest: 0.6677409\tbest: 0.6677409 (300)\ttotal: 38.3s\tremaining: 1m 28s\n",
      "400:\tlearn: 0.7145724\ttest: 0.6690983\tbest: 0.6690983 (400)\ttotal: 51.2s\tremaining: 1m 16s\n",
      "500:\tlearn: 0.7225827\ttest: 0.6697614\tbest: 0.6697933 (497)\ttotal: 1m 3s\tremaining: 1m 3s\n",
      "600:\tlearn: 0.7313299\ttest: 0.6701476\tbest: 0.6701727 (575)\ttotal: 1m 16s\tremaining: 50.7s\n",
      "700:\tlearn: 0.7406551\ttest: 0.6707380\tbest: 0.6707380 (700)\ttotal: 1m 28s\tremaining: 37.9s\n",
      "800:\tlearn: 0.7490444\ttest: 0.6710650\tbest: 0.6710650 (800)\ttotal: 1m 41s\tremaining: 25.2s\n",
      "900:\tlearn: 0.7567595\ttest: 0.6712584\tbest: 0.6712898 (899)\ttotal: 1m 54s\tremaining: 12.6s\n",
      "999:\tlearn: 0.7642888\ttest: 0.6712146\tbest: 0.6714917 (913)\ttotal: 2m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.6714917105\n",
      "bestIteration = 913\n",
      "\n",
      "Shrink model to first 914 iterations.\n",
      "ğŸ”® Predicting...\n",
      "âœ… Saved result to 03_CatBoost_1031.csv\n",
      "\n",
      "ğŸ All seeds processed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main Logic Loop ---\n",
    "\n",
    "# ê²°ê³¼ íŒŒì¼ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "prediction_files = []\n",
    "\n",
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©í•  Aggregation ì •ì˜\n",
    "# í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì— ì‚¬ìš©í•  Aggregation ì •ì˜ (Original Logic ì¶”ê°€)\n",
    "aggs_to_create = {\n",
    "    'history_a_1': ['mean', 'std'], 'history_a_2': ['mean', 'std'], \n",
    "    'history_a_3': ['mean', 'std'], 'history_a_6': ['mean', 'std'],\n",
    "    'feat_e_2': ['mean', 'std'], 'feat_e_3': ['mean', 'std'],\n",
    "    'feat_c_8' : ['mean', 'std'], 'feat_e_9' : ['mean', 'std'],\n",
    "    'feat_d_4' : ['mean', 'std'], \n",
    "    'l_feat_1' : ['mean', 'std'], 'l_feat_2' : ['mean', 'std'],\n",
    "    'l_feat_5' : ['mean', 'std'], 'l_feat_7' : ['mean', 'std'],\n",
    "    'l_feat_10' : ['mean', 'std'], 'l_feat_15' : ['mean', 'std']\n",
    "}\n",
    "\n",
    "for seed in CFG.SEEDS:\n",
    "    print(f\"\\n{'='*40}\\nğŸš€ Processing Seed: {seed}\\n{'='*40}\")\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ (ë§¤ ë°˜ë³µë§ˆë‹¤ ì›ë³¸ ë¡œë“œ)\n",
    "    # 10% ìƒ˜í”Œë§ëœ ë°ì´í„° ì‚¬ìš© (ì „ì²´ ë°ì´í„° ì‚¬ìš© ì‹œ íŒŒì¼ëª…ì„ 'train.parquet'ìœ¼ë¡œ ë³€ê²½)\n",
    "    train_path = os.path.join(CFG.DATA_PATH, 'D:/folder\\ëŒ€í•™ì›\\CTR/rawdata/train_sample_10pct.parquet') \n",
    "    train_df = pd.read_parquet(train_path)\n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ (ì—†ìœ¼ë©´ ë”ë¯¸ ìƒì„± ë¡œì§ í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ìˆë‹¤ê³  ê°€ì •)\n",
    "    try:\n",
    "        test_df = pd.read_parquet(os.path.join(CFG.DATA_PATH, 'test.parquet'))\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ Test data not found. Creating dummy test data for demo.\")\n",
    "        # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì œê±°)\n",
    "        test_df = train_df.head(100).drop(columns=['clicked'])\n",
    "        test_df['ID'] = [f'TEST_{i}' for i in range(100)] \n",
    "    \n",
    "    # 2. Down-Sampling (1:1 Ratio)\n",
    "    print(\"ğŸ”» Down-sampling...\")\n",
    "    clicked_1 = train_df[train_df['clicked'] == 1]\n",
    "    clicked_0 = train_df[train_df['clicked'] == 0].sample(n=int(len(clicked_1)), random_state=seed)\n",
    "    train_df = pd.concat([clicked_1, clicked_0], axis=0).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    print(f\"   Train shape: {train_df.shape}\")\n",
    "    \n",
    "    # 3. Feature Engineering\n",
    "    print(\"ğŸ› ï¸ Feature Engineering...\")\n",
    "    \n",
    "    # Seq Features\n",
    "    train_df = add_seq_features(train_df, \"train\")\n",
    "    test_df = add_seq_features(test_df, \"test\")\n",
    "    \n",
    "    # Interaction Features\n",
    "    cols = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour']\n",
    "    for col1, col2 in combinations(cols, 2):\n",
    "        new_col = f'{col1}_{col2}'\n",
    "        train_df[new_col] = train_df[col1].astype(str) + '_' + train_df[col2].astype(str)\n",
    "        test_df[new_col] = test_df[col1].astype(str) + '_' + test_df[col2].astype(str)\n",
    "        train_df[new_col] = train_df[new_col].astype('category')\n",
    "        test_df[new_col] = test_df[new_col].astype('category')\n",
    "        \n",
    "    # Groupby Features\n",
    "    train_df, test_df = add_groupby_features(train_df, test_df, 'inventory_id', aggs_to_create)\n",
    "    \n",
    "    \n",
    "    # Count Encoding\n",
    "    count_cols = ['age_group', 'inventory_id', 'day_of_week'] # ì£¼ìš” ë³€ìˆ˜ ì¹´ìš´íŠ¸\n",
    "    # 'age_group_inventory_id', 'inventory_id_hour' ë“± ì¡°í•© ë³€ìˆ˜ê°€ ìˆë‹¤ë©´ ì¶”ê°€\n",
    "    train_df, test_df = add_count_features(train_df, test_df, count_cols)\n",
    "\n",
    "    # Fill NA & Clean\n",
    "    categorical_features = ['gender', 'age_group', 'inventory_id', 'day_of_week', 'hour', 'gender_age_group',\n",
    "                            'gender_inventory_id', 'gender_day_of_week', 'gender_hour', 'age_group_inventory_id',\n",
    "                            'age_group_day_of_week', 'age_group_hour', 'inventory_id_day_of_week', 'inventory_id_hour',\n",
    "                            'day_of_week_hour']\n",
    "                            \n",
    "    for col in categorical_features:\n",
    "        if col in train_df.columns:\n",
    "            train_df[col] = train_df[col].astype(str).fillna('NaN').astype('category')\n",
    "            test_df[col] = test_df[col].astype(str).fillna('NaN').astype('category')\n",
    "            \n",
    "    for df in [train_df, test_df]:\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col] = df[col].fillna(0)\n",
    "    \n",
    "    # 4. Training\n",
    "    print(\"ğŸ”¥ Training CatBoost...\")\n",
    "    target_col = \"clicked\"\n",
    "    exclude_cols = {target_col, \"seq\", \"ID\"}\n",
    "    features = [c for c in train_df.columns if c not in exclude_cols]\n",
    "    cat_features = [c for c in features if c in categorical_features] # Only use valid cat features\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_df[features], train_df[target_col], \n",
    "        test_size=CFG.VALID_RATIO, random_state=seed, stratify=train_df[target_col]\n",
    "    )\n",
    "    \n",
    "    model = cb.CatBoostClassifier(\n",
    "        iterations=1000, # ë°ëª¨ìš© ì¤„ì„, ì‹¤ì œëŠ” 10000\n",
    "        learning_rate=0.02,\n",
    "        depth=8,\n",
    "        random_seed=seed,\n",
    "        thread_count=-1,\n",
    "        eval_metric=TossMetric(),\n",
    "        early_stopping_rounds=100,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], cat_features=cat_features)\n",
    "    \n",
    "    # 5. Prediction\n",
    "    print(\"ğŸ”® Predicting...\")\n",
    "    # predict_proba for class 1\n",
    "    preds = model.predict_proba(test_df[features])[:, 1]\n",
    "    \n",
    "    # 6. Save Individual Result\n",
    "    output_filename = f\"{CFG.OUTPUT_PREFIX}_{seed}.csv\"\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ID ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì • (ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ ì‚¬ìš©)\n",
    "    if 'ID' in test_df.columns:\n",
    "        sub = pd.DataFrame({'ID': test_df['ID'], 'clicked': preds})\n",
    "    else:\n",
    "        # ì‹¤ì œ submission íŒŒì¼ ë¡œë“œí•˜ì—¬ ID ë§¤í•‘ í•„ìš” (ìƒëµì‹œ ì¸ë±ìŠ¤)\n",
    "        sub = pd.DataFrame({'ID': range(len(preds)), 'clicked': preds})\n",
    "        \n",
    "    sub.to_csv(output_filename, index=False)\n",
    "    prediction_files.append(output_filename)\n",
    "    print(f\"âœ… Saved result to {output_filename}\")\n",
    "    \n",
    "    # ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "    del train_df, X_train, X_val\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\nğŸ All seeds processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b460e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "595ec559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤ Performing Soft Voting on 3 files: ['03_CatBoost_42.csv', '03_CatBoost_106.csv', '03_CatBoost_1031.csv']\n",
      "ğŸ‰ Final CatBoost Ensemble Saved: 03_CatBoost_SoftVoting.csv\n",
      "             ID  click_42  click_106  click_1031   clicked\n",
      "0  TEST_0000000  0.465993   0.371046    0.403368  0.413469\n",
      "1  TEST_0000001  0.342321   0.350810    0.346905  0.346678\n",
      "2  TEST_0000002  0.410186   0.440518    0.443540  0.431415\n",
      "3  TEST_0000003  0.480882   0.466825    0.488084  0.478597\n",
      "4  TEST_0000004  0.369274   0.378560    0.423092  0.390308\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Soft Voting Ensemble ---\n",
    "print(f\"\\nğŸ¤ Performing Soft Voting on {len(prediction_files)} files: {prediction_files}\")\n",
    "\n",
    "if len(prediction_files) > 0:\n",
    "    merged = pd.read_csv(prediction_files[0]).rename(columns={\"clicked\": f\"click_{CFG.SEEDS[0]}\"})\n",
    "    \n",
    "    for i, file in enumerate(prediction_files[1:], start=1):\n",
    "        df_temp = pd.read_csv(file).rename(columns={\"clicked\": f\"click_{CFG.SEEDS[i]}\"})\n",
    "        merged = merged.merge(df_temp, on=\"ID\")\n",
    "    \n",
    "    # Mean\n",
    "    click_cols = [c for c in merged.columns if c.startswith(\"click_\")]\n",
    "    merged[\"clicked\"] = merged[click_cols].mean(axis=1)\n",
    "    \n",
    "    # Save Final\n",
    "    final_output = \"03_CatBoost_SoftVoting.csv\"\n",
    "    merged[[\"ID\", \"clicked\"]].to_csv(final_output, index=False)\n",
    "    print(f\"ğŸ‰ Final CatBoost Ensemble Saved: {final_output}\")\n",
    "    print(merged.head())\n",
    "else:\n",
    "    print(\"âŒ No prediction files found.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
